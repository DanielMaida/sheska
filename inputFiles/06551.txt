Reinforcement Learning in POMDPs with Memoryless Options and
Option-Observation Initiation Sets
Denis Steckelmacher
Diederik M. Roijers
Anna Harutyunyan
Peter Vrancx
Ann Nowe?
Artificial Intelligence Lab, Vrije Universiteit Brussel, Belgium
Abstract
Most real-world reinforcement learning problems have a hi-
erarchical nature, and often exhibit some degree of partial
observability. While hierarchy and partial observability are
usually tackled separately, for instance by combining recur-
rent neural networks and options, we show that addressing
both problems simultaneously is simpler and more efficient
in many cases. More specifically, we make the initiation
set of options conditional on the previously-executed option,
and show that options with such Option-Observation Initi-
ation Sets (OOIs) are at least as expressive as Finite State
Controllers (FSCs), a state-of-the-art approach for learning
in POMDPs. In contrast to other hierarchical methods in
partially observable environments, OOIs are easy to design
based on an intuitive description of the task, lead to ex-
plainable policies and keep the top-level and option policies
memoryless. Our experiments show that OOIs allow agents
to learn optimal policies in challenging POMDPs, outper-
forming an human-provided policy in our robotic experiment,
while learning much faster than a recurrent neural network
over options.
1 Introduction
Real-world applications of reinforcement learning (RL) face
two main challenges: complex long-running tasks and par-
tial observability. Hierarchical RL, of which options are the
particular instance we focus on, addresses the first challenge
by factoring a complex task into simpler sub-tasks (Barto
and Mahadevan 2003; Roy et al. 2006; Tessler et al. 2016).
Instead of learning which action to perform depending on
an observation, the agent learns a top-level policy that re-
peatedly selects options, that in turn execute a sequence of
actions before returning (Sutton et al. 1999). The second
challenge, partial observability, is addressed by maintaining
a belief of what the agent thinks the full state is (Cassandra
et al. 1994), reasoning about possible future observations
(Littman et al. 2001; Boots et al. 2011), storing information
in an external memory for later reuse (Peshkin et al. 2001;
Zaremba and Sutskever 2015; Graves et al. 2016), or using
recurrent neural networks (RNNs) to allow information to
flow between time-steps (Bakker 2001; Mnih et al. 2016).
Combined solutions to the above two challenges have al-
ready been proposed, but are not always ideal. HQ-Learning
decomposes a task into a list of fully-observable subtasks to
be executed in sequence (Wiering and Schmidhuber 1997),
which precludes cyclic tasks with an unbounded number
of repetitions from being solved. Using recurrent neural
networks in options and for the top-level policy (Sridha-
ran et al. 2010) addresses both challenges, but brings in
the design complexity of RNNs (Jo?zefowicz et al. 2015;
Angeline et al. 1994; Mikolov et al. 2014). RNNs also have
limitations regarding long time horizons, as their memory
decays over time (Hochreiter and Schmidhuber 1997), and
provide no explanation on which past observations affect the
current choice of action.
In her PhD thesis, Precup (2000, page 126) suggests that
options may already be close to addressing partial observ-
ability, thus removing the need for more complicated solu-
tions. In this paper, we prove this intuition correct by:
1. Showing that options do not suffice in POMDPs;
2. Introducing Option-Observation Initiation Sets (OOIs),
that make the initiation sets of options conditional on the
previously-executed option;
3. Proving that OOIs make options at least as expressive as
Finite State Controllers (Section 3.2), state-of-the-art in
POMDPs.
Designing OOIs is as easy as describing a task in natu-
ral language (Section 3.1). In contrast to existing HRL
algorithms for POMDPs (Wiering and Schmidhuber 1997;
Theocharous 2002; Sridharan et al. 2010), OOIs handle
repetitive tasks, do not restrict the action set available to sub-
tasks, and keep the option policies and policy over options
memoryless. Experimental results in Section 4 confirm that
OOIs allow partially observable tasks to be solved optimally,
while allowing learned (Section 4.4) or human-provided op-
tions to be used (Sections 4.3 and 4.5). Our experiments
also show that OOIs are much more sample efficient than a
recurrent neural network over options, and lead to a policy
significantly outperforming an expert policy in Section 4.3.
1.1 Motivating Example
OOIs are designed to solve complex partially observable
tasks that can be decomposed into a set of fully-observable
sub-tasks. For instance, a robot with first-person sensors
may be able to avoid obstacles, open doors or manipulate
objects even if its precise location in a building is not ob-
served. We now introduce such an environment, on which
our robotic experiments of Section 4.3 are based.
ar
X
iv
:1
70
8.
06
55
1v
1 
 [
cs
.A
I]
  2
2 
A
ug
 2
01
7
a) b)
Blue Green
Red (root)
Figure 1: Industrial object gathering task. a) Khepera III, the
two-wheeled robot used in the experiments. b) The robot has
to gather objects from two terminals separated by a wall, and
to bring them to the root.
A Khepera III1robot has to gather objects from a green
and a blue terminal separated by a wall, and to bring them to
a central root location. Objects have to be gathered one by
one from a terminal until it becomes empty, which requires
many journeys between the root and a terminal. When a
terminal is emptied, the other one is automatically refilled.
The robot therefore has to alternatively gather objects from
both terminals, until the end of the episode. The root is col-
ored in red and marked by a paper QR-code displaying 1.
Each terminal has a screen displaying its color and a 1 QR-
code when full, 2 when empty. Because the robot cannot
read QR-codes from far away, the state of a terminal can-
not be observed from the root. This makes the environment
partially observable, and requires that the robot goes to a ter-
minal, observes that it is empty, goes back to the root, and
remembers to now go to the other terminal.
The robot is able to control the speed of its two wheels.
A wireless camera mounted on top of the robot allows it to
identify where the largest red, green or blue color blobs are
in its field of view, and can read nearby QR-codes. Such
low-level actions and observations, combined with a com-
plicated task, motivate the use of hierarchical reinforcement
learning. Fixed options allow the robot to move towards the
largest red, green or blue blob in its field of view. The op-
tions terminate as soon as a QR-code is in front of the cam-
era and close enough to be read. This allows the robot to
move towards colored objects, then to make a decision based
on the QR-code displayed on that object. The limited range
of the QR-code reader and its inability to read more than
one QR-code at once prevent the agent from observing the
complete state of the environment. The robot has to learn a
policy over options that solves the task.
The robot may have to gather a large number of objects,
alternating between terminals several times. The repetitive
nature of this task is incompatible with HQ-Learning (Wier-
ing and Schmidhuber 1997). Options with standard initia-
tion sets are not able to solve this task, as the top-level pol-
icy is memoryless (Sutton et al. 1999) and cannot remem-
ber from which terminal the robot arrives at the root, and
whether that terminal was full or empty. Because the termi-
1http://www.k-team.com/mobile-robotics-
products/old-products/khepera-iii
a) b)
Figure 2: Observations of the Khepera robot. a) Color im-
age from the camera b) Color blobs detected by the vision
system, as observed by the robot. QR-codes can only be de-
tected when the robot is a couple of inches away from them.
nals are far from the root, almost a hundred primitive actions
have to be executed to complete any root/terminal journey.
Without options, this represents a time horizon much larger
than usually handled by recurrent neural networks (Bakker
2001) or finite history windows (Lin and Mitchell 1993).
OOIs allow each option to be selected conditionally on
the previously executed one (Section 3.1), which is much
simpler than combining options and recurrent neural net-
works (Sridharan et al. 2010). The ability of OOIs to solve
POMDPs builds on the time abstraction capabilities and ex-
pressiveness of options. Section 4.3 shows that OOIs al-
low the optimal policy for our robotic task to be represented
and learned above expert level. Section 4.4 demonstrates on
a simulated task that both the top-level and option policies
can be learned by the agent. Finally, Section 4.5 shows that
OOIs lead to substantial gains over standard initiation sets
even if the option set is reduced or unsuited to the task.
2 Background
This section formally introduces Markov Decision Pro-
cesses (MDPs), Options, Partially Observable MDPs
(POMDPs) and Finite State Controllers, before presenting
our main contribution in Section 3.
2.1 Markov Decision Processes
A discrete-time Markov Decision Process (MDP)
?S,A,R, T, ?? with discrete actions is defined by a
possibly-infinite set S of states, a finite set A of actions,
a reward function R(st, at, st+1) ? R, that provides
a scalar reward rt for each state transition, a transition
function T (st, at, st+1) ? [0, 1], that outputs a probability
distribution over new states st+1 given a (st, at) state-action
pair, and 0 ? ? < 1 the discount factor, that defines how
sensitive the agent should be to future rewards.
A stochastic memoryless policy ?(st, at) ? [0, 1] maps a
state to a probability distribution over actions. The goal of
the agent is to find a policy ?? that maximizes the expected
cumulative discounted reward E?? [
?
t ?
trt] obtainable by
following that policy.
2.2 Options
The options framework, defined in MDPs by Sutton et al.,
consists of a set of options O where each option ? ? O is a
tuple ???, I?, ???, with ??(st, at) ? [0, 1] the memoryless
option policy, ??(st) ? [0, 1] the termination function that
gives the probability for the option ? to terminate in state st,
and I? ? S the initiation set that defines in which states ?
can be started (Sutton et al. 1999).
The memoryless policy over options µ(st, ?t) ? [0, 1]
maps states to a distribution over options and allows to
choose which option to start in a given state. When an op-
tion ? is started, it executes until termination (due to ??), at
which point µ selects a new option based on the now current
state.
2.3 Partially Observable MDPs
Most real-world problems are not MDPs and present at least
some degree of partial observability. A Partially Observable
MDP (POMDP) ??, S,A,R, T,O, ?? is a MDP extended
with two components: the possibly infinite set ? of observa-
tions, and theO : S ? ? function that produces observations
x based on the hidden state s of the process. Two different
states, requiring two different optimal actions, may produce
the same observation. This makes POMDPs remarkably
challenging for reinforcement learning algorithms, as mem-
oryless policies, that select actions or options based only on
the current observation, typically no longer suffice.
2.4 Finite State Controllers
Finite State Controllers (FSCs) are state of the art in rep-
resenting policies that work well in POMDPs. An FSC
?N , ?, ?, ?0? is defined by a finite set N of nodes, an
action function ?(nt, at) ? [0, 1] that maps nodes to a
probability distribution over actions, a successor function
?(nt?1, xt, nt) ? [0, 1] that maps nodes and observations
to a probability distribution over next nodes, and an initial
function ?0(x1, n1) ? [0, 1] that maps initial observations to
nodes (Meuleau et al. 1999).
At the first time-step, the agent observes x1 and activates a
node n1 by sampling from ?0(x1, ·). An action is performed
by sampling from ?(n1, ·). At each time-step t, a node nt
is sampled from ?(nt?1, xt, ·), then an action at is sampled
from ?(nt, ·). FSCs allow the agent to select actions ac-
cording to the entire history of past observations (Meuleau
et al. 1999), which has been shown to be one of the best ap-
proaches for POMDPs (Lin and Mitchell 1992). OOIs, our
main contribution, make options at least as expressive and as
relevant to POMDPs as FSCs, while being able to leverage
the hierarchical structure of the problem.
3 Option-Observation Initiation Sets
Our main contribution, Option-Observation Initiation Sets
(OOIs), make the initiation sets of options conditional on
the option that has just terminated. We prove that OOIs
make options at least as expressive as FSCs (thus suited to
POMDPs, see Section 3.2), even if the top-level and option
policies are memoryless, while options without OOIs are
strictly less expressive than FSCs (see Section 3.3). In Sec-
tion 4, we show on one robotic and two simulated tasks that
OOIs allow challenging POMDPs to be solved optimally.
3.1 Conditioning on Previous Option
Descriptions of partially observable tasks in natural lan-
guage often contain allusions at sub-tasks that must be se-
quenced or cycled through, possibly with branches. This is
easily mapped to a policy over options (learned by the agent)
and sets of options that may or may not follow each other.
A good memory-based policy for our motivating exam-
ple, where the agent has to bring objects from two terminals
to the root, can be described as “go to the green terminal,
then go to the root, then go back to the green terminal if it
was full, to the blue terminal otherwise”, and symmetrically
so for the blue terminal. This sequence of sub-tasks, that
contains a condition, is easily translated to a set of options.
Two options, sharing a single policy, go from the green ter-
minal to the root (using low-level motor actions). One is ex-
ecuted when the terminal is full, the other when it is empty.
At the root, the option that goes back to the green terminal
can only follow the option that goes to the root after having
observed that the green terminal was full. This forces the
agent to switch to the blue terminal when the green one is
empty. In Section 4.3, we show that a slightly larger set of
options avoids encoding parts of the solution in the option
set, which allows the agent to discover a stochastic policy
that outperforms our good policy.
We now formally define our main contribution, Option-
Observation Initiation Sets (OOIs), that allow to describe
which options may follow which ones. We define the ini-
tiation set I? of option ? so that the set Ot of options avail-
able at time t depends on the observation xt and previously-
executed option ?t?1:
I? ? ?× (O ? {?})
Ot ? {? ? O : (xt, ?t?1) ? I?}
with ?0 = ?, ? the set of observations and O the set of op-
tions. Ot allows the agent to condition the option selected
at time t on the one that has just terminated, even if the
top-level policy does not observe ?t?1. The option poli-
cies and top-level policy remain memoryless. Not having to
observe ?t?1 keeps the observation space of the top-level
policy small, instead of extending it to ? × O, without im-
pairing the representational power of OOIs, as shown in the
next sub-section.
3.2 OOIs Make Options as Expressive as FSCs
Finite State Controllers are state-of-the-art in policies appli-
cable to POMDPs (Meuleau et al. 1999). By proving that
options with OOIs are as expressive as FSCs, we provide a
lower bound on the expressiveness of OOIs and ensure that
they are applicable to a wide range of POMDPs.
Theorem 1. OOIs allow options to represent any policy that
can be expressed using a Finite State Controller.
Proof. The reduction from any FSC to options requires one
option ?n?t?1, nt? per ordered pair of node in the FSC, and
one option ??, n1? per node in the FSC. Assuming that n0 =
? and ?(?, x1, ·) = ?0(x1, ·), the options are defined by:
??n?t?1,nt?(xt) = 1 (1)
??n?t?1,nt?(xt, at) = ?(nt, at) (2)
µ(xt, ?n?t?1, nt?) = ?(n?t?1, xt, nt) (3)
I??,n1? = ?× {?}
I?n?t?1,nt? = ?× {?n
?
t?2, nt?1? : n?t?1 = nt?1}
Each option corresponds to an edge of the FSC. Equa-
tion 1 ensures that every option stops after having emitted
a single action, as the FSC takes one transition every time-
step. Equation 2 maps the current option to the action emit-
ted by the destination node of its corresponding FSC edge.
We show that µ and I?n?t?1,nt? implement ?(nt?1, xt, nt)
by:
µ(xt, ?n?t?1, nt? | ?t?1 = ?n?t?2, nt?1?) =?????????
0
?n?t?2, nt?1? /? I?n?t?1,nt?
? n?t?1 6= nt?1
?(nt?1, xt, nt)
?n?t?2, nt?1? ? I?n?t?1,nt?
? n?t?1 = nt?1
Because ? maps nodes to nodes and µ selects options
representing pairs of nodes, µ is extremely sparse and re-
turns a value different from zero, ?(nt?1, xt, nt), only when
?n?t?2, nt?1? and ?n?t?1, nt? agree on nt?1.
Equation 2 ignores the observation, which leads to a large
amount of options to compensate. In practice, we expect
to be able to express policies for real-world POMDPs with
much less options than the number of states an FSC would
require, as shown in our simulated (Section 4.4, 2 options)
and robotic experiments (Section 4.3, 12 options). In ad-
dition to being sufficient, the next sub-section proves that
OOIs are necessary for options to be as expressive as FSCs.
3.3 Original Options are not as Expressive as
FSCs
While options with regular initiation sets may be able to ex-
press some memory-based policies (Sutton et al. 1999, page
7), the tiny but valid Finite State Controller presented in Fig-
ure 3 cannot be mapped to a set of options and a policy over
options (without OOIs). This proves that options without
OOIs are strictly less expressive than FSCs.
Theorem 2. Options without OOIs are not as expressive as
Finite State Controllers.
Proof. Figure 3 shows a Finite State Controller that emits
a sequence of alternating A’s and B’s, based on a constant
uninformative observation x?. This task requires memory
because the observation does not provide any information
about what was the last letter to be emitted, or which one
must now be emitted. Options having memoryless policies,
options executing for multiple time-steps are unable to rep-
resent the FSC exactly. A combination of options that exe-
cute for a single time-step cannot represent the FSC either,
as the options framework is unable to represent memory-
based policies with single-time-step options (Sutton et al.
1999).
A B
Figure 3: Two-nodes Finite State Controller that emits an
infinite sequence ABAB... based on an uninformative ob-
servation x?. This FSC cannot be expressed using options
without OOIs.
4 Experiments
The experiments in this section illustrate how OOIs allow
agents to perform optimally in environments where options
without OOIs fail. Section 4.3 shows on our motivating ex-
ample (Section 1.1) that OOIs allow the agent to find a pol-
icy outperforming an expert-provided one, which motivates
the use of reinforcement learning in industrial POMDPs
where the currently deployed policies may still be improved
upon. Section 4.4 shows that the top-level and option poli-
cies required by a repetitive task can be learned, which is
useful when no predefined options are available. In Section
4.5, we progressively reduce the amount of options avail-
able to the agent, and demonstrate how OOIs still allow
good memory-based policies to emerge when a sub-optimal
amount of options are used. Our experiments demonstrate
how OOIs allow to reach optimal policies in real-world
POMDPs with minimal engineering effort.
All our results are averaged across 20 runs, with standard
deviation represented by the light regions in the figures.
4.1 Experimental Setup
All our agents learn their policy over options and option
policies (if not fixed) using a single feed-forward neural net-
work, with one hidden layer of 100 neurons, trained using
Policy Gradient (Sutton et al. 2000) and the Adam opti-
mizer (Kingma and Ba 2014). Our neural network ? takes
three inputs and produces one output. The inputs are ob-
servation features x = ?(xt), the one-hot encoded current
option ? (? = 0 when executing the top-level policy), and
a mask, mask. The output is a distribution y over extended
options:
h1 = tanh(W1[x
T?T ]T + b1),
y? = ?(W2h1 + b2) ?mask,
y =
y?
1T y?
,
with Wi and bi the trainable weights and biases of layer i, ?
the sigmoid function, and ? the element-wise product of two
vectors. The fraction ensures that a valid probability dis-
tribution is produced by the network2. The set of extended
options O? is the union of the set A of actions and the set O
of options, multiplied by 2 as the agent is able to select an
action while terminating (or not) the current option3:
2A Softmax over allowed options did not provide results as
good as this implementation
3??t(xt) =
?
a ?(xt, ?t, a+ terminate)
O? ? (O ?A)× {end, continue}
The initiation sets of options are implemented using the
mask input of the neural network, a vector of |O?| integers.
When ? = 0, the mask forces the probability of primi-
tive actions to zero, preserves option ?i according to I?i ,
and prevents the top-level policy from terminating. When
? 6= 0, the mask only allows primitive actions to be ex-
ecuted. For instance, if there are two options and three
actions, mask = endcont (
0 0 1 1 1
0 0 1 1 1 ) when executing any of
the options. When executing the top-level policy, mask =
end
cont (
0 0 0 0 0
a b 0 0 0 ), with a = 1 if and only if the option that
has just finished is in the initiation set of the first option, and
b = 1 according to the same rule but for the second option.
The neural network ? is trained using Policy Gradient, with
the following loss:
L(?) = ?
T?
t=0
(Rt ? V (xt, ?t)) log(?(xt, ?t, at))
with at ? ?(xt, ?t, ·) the action executed at time t. The re-
turnRt =
?T
?=t ?
?r? , with r? = R(s? , a? , s?+1), is a sim-
ple discounted sum of future rewards, and ignores changes
of current option. This gives the agent information about the
complete outcome of an action or option, by directly eval-
uating its flattened policy. A baseline V (xt, ?t) is used to
reduce the variance of the L estimate (Sutton et al. 2000).
V (xt, ?t) predicts the expected cumulative reward obtain-
able from xt in option ?t using a separate neural network,
trained on the monte-carlo return obtained from xt in ?t.
4.2 Comparison with LSTM over Options
In order to provide a complete evaluation of OOIs, a vari-
ant of the ? network of Section 4.1, where the hidden layer
is replaced with a layer of 100 LSTM units (Hochreiter and
Schmidhuber 1997; Sridharan et al. 2010), is also evaluated
on every task. In every experiment, the agent based on the
LSTM network manages to reach the optimal policy, but it
requires hundreds of thousands of episodes to do so, an or-
der of magnitude more than options with OOIs. We explain
this result by the continuous nature of LSTM recurrent con-
nections, and the fact that the LSTM agents are unable to ex-
ploit the high-level structure of the task, encoded by OOIs.
Their policy spaces are therefore larger, and more difficult
to search in. To keep our graphs readable, we do not show
the full length of the learning curves of the LSTM agents.
4.3 Object Gathering
The first experiment illustrates how OOIs can be applied
to a complex real-world robotic partially observable task,
leading to a stochastic top-level policy significantly outper-
forming an expert-provided policy. This result is particu-
larly important as, combined with the simplicity of OOIs,
it illustrates the benefits of using reinforcement learning for
industrial tasks, even when existing policies already seem
satisfactory.
The experiment takes place in the environment described
in Section 1.1. A robot has to gather objects one by one
 0
 1
 2
 3
 4
 5
 6
 7
 8
 9
0K 2K 4K 6K 8K 10K 12K 14K
C
um
ul
at
iv
e 
re
w
ar
d 
pe
r e
pi
so
de
Episode
LSTM + Options
OOIs + Options
Options
Expert Policy
Figure 4: Cumulative reward per episode obtained on our
object gathering task, with OOIs, without OOIs and using an
LSTM over options. OOIs, combined with Policy Gradient,
allow to find a policy outperforming an expert policy and
LSTM over options.
from two terminals, green and blue, and bring them back to
a root location. When the robot reaches one of the terminals,
it either picks up an object and receives a reward of +2, or
receives a reward of -2 when the terminal is empty. Whether
a terminal is full or empty is observed by the agent only
when it is at the terminal. At the beginning of the episode,
each terminal contains 2 or 3 objects, this amount being se-
lected randomly for each terminal. When the agent goes to
an empty terminal, the other one is re-filled with 2 or 3 ob-
jects. The episode ends after 2 or 3 emptyings (combined
across both terminals). As detailed in Section 1.1, the agent
only has a first-person view of the world, cannot observe
terminals from far away, and therefore has to remember in-
formation acquired at terminals in order to make good deci-
sions at the root. When at the root, 10 feet away from any
terminal, the agent has no way of knowing which terminal is
empty, if any.
The agent has access to 12 memoryless options that go
to red (?R1..R4), green (?G1..G4) or blue objects (?B1..B4),
and terminate when the agent is close enough to them to
read a QR-code displayed on them. The initiation set of
?R1,R2 is ?G1..G4, of ?R3,R4 is ?B1..B4, and of ?Gi,Bi is
?Ri ?i = 1..4. This description of the options and their
OOIs is purposefully uninformative, and illustrates how lit-
tle information the agent has about the task. The option set
used in this experiment is also richer than the simple exam-
ple of Section 3.1, to better illustrate the generality of OOIs.
Agents with and without OOIs learn top-level policies
over these options. We compare them to a fixed agent,
using an expert top-level policy that interprets the op-
tions as follows: ?R1..R4 go to the root from a full/empty
green/blue terminal (and are selected accordingly at the ter-
minals depending on the QR-code displayed on them), while
?G1..G4,B1..B4 go to the green/blue terminal from the root
when the previous terminal was full/empty and green/blue.
At the root, the expert top-level policy outputs a uniform
probability distribution over go to green after a full green,
go to green after an empty blue, go to blue after a full blue
and go to blue after an empty green. OOIs ensure that only a
single of these options is selected, according to what the last
 0
 5
 10
 15
 20
0K 10K 20K 30K 40K 50K
C
um
ul
at
iv
e 
re
w
ar
d 
pe
r e
pi
so
de
Episode
LSTM + Options
OOIs + Options
Options
Optimal Policy
Figure 5: Cumulative reward per episode obtained on mod-
ified DuplicatedInput, with OOIs, without OOIs and using
an LSTM over options. OOIs allow the task to be solved to
optimality. An LSTM over options learns the task in more
than 100K episodes.
terminal was and whether it was full or empty. The agent
goes to a terminal until it is empty, then goes back to the
root and starts emptying the other terminal, which leads to
an average reward of 7.5.4
When the top-level policy is not fixed, OOIs allow the
task to be learned, as shown in Figure 4. Because experi-
ments on a robot are slow, we developed a small simulator
for this task, and used it to produce Figure 4 after having
successfully asserted its accuracy using two 1000-episodes
runs on the actual robot. The agent learns to properly se-
lect ?R1..R4 at the terminals, depending on the QR-code,
and to output a proper distribution over options at the root.
Interestingly, the agent converges to a stochastic policy (as
permitted by Policy Gradient, see Section 4.1). Instead of
always emptying a terminal before switching to the other
one, the agent switches terminal early with a small proba-
bility, which reduces its chances of observing an empty ter-
minal before having to switch anyway. This policy leads to
an average reward of 7.825, significantly higher than the 7.5
average obtained by our expert policy (p < 10?10, com-
puted from 2000 episodes). This demonstrates how rein-
forcement learning, options and OOIs can be applied to a
complex partially observable task, fine-tuning probabilities
more precisely than a human could do, to lead to a policy
achieving better results than one produced by a human ex-
pert, even if the task seems easy to solve at first glance.
Because fixed option policies are not always available, we
now show that OOIs allow them to be learned at the same
time as the top-level policy.
4.4 Modified DuplicatedInput
In some cases, a hierarchical reinforcement learning agent
may not have been provided policies for several or all its op-
tions. In this case, OOIs allow the agent to learn its top-level
policy, the option policies and their termination functions.
In this experiment, the agent has to learn its top-level and
option policies to copy characters from an input tape to an
42.5×(?2+2.5×2), 2 or 3 emptyings of terminals that contain
2 or 3 objects. Average confirmed experimentally from 32 runs on
the actual robot, p > 0.49.
x1
g0
 g1
g2
g3
g4
g5
g6
g7
Figure 6: TreeMaze environment. The agent starts at x1
and must go to one of the leaves. The leaf to be reached is
indicated by 3 bits observed at time-steps 1, 2 and 3.
output tape, removing duplicate B’s and D’s (mapping AB-
BCCEDD to ABCCED for instance; B’s and D’s always ap-
pear in pairs). The agent only observes a single input char-
acter at a time, and can write at most one character to the
output tape per time-step.
The input tape is a sequence of N symbols x ? ?, with
? = {A,B,C,D,E} and N a random number between 20
and 30. The agent observes a single symbol xt ? ?, read
from the i-th position in the input sequence, and does not
observe i. When t = 1, i = 0. The action set A contains
20 actions, each of them representing a symbol, whether it
must be pushed onto the output tape, and whether i should
be incremented or decremented. A reward of 1 is given for
each correct symbol written to the output tape. The episode
finishes with a reward of -0.5 when an error is made.
The agent has access to two options, ?1 and ?2. OOIs
are designed so that ?2 cannot follow itself, with no such re-
striction on ?1. No reward shaping or hint about what each
option should do is provided. The agent automatically dis-
covers that ?1 must copy the current character to the output,
and that ?2 must skip the character without copying it. It
also learns the top-level policy, that selects ?2 (skip) when
observing B or D and ?2 is allowed, ?1 otherwise (copy).
Figure 5 shows that an agent with two options and OOIs
learns an optimal policy for this task, while an agent with
two options and only standard initiation sets (I? = ? ??)
fails to do so. The agent without OOIs only learns to copy
characters and never skips any (having two options does not
help it). This shows that OOIs are necessary for learning
this task, and allow to learn a policy over options and option
policies suited to our repetitive partially observable task.
The first two experiments use a set of options tailored to
the task to solve. That set may not always be known, and the
last experiment shows that OOIs provide large benefits over
standard initiation sets and LSTM over options even when a
sub-optimal amount of options are available.
4.5 TreeMaze
Defining an extensive set of options and their OOIs may
sometimes require more knowledge about the task and the
environment than available. This experiment shows that a
sub-optimal set of options, arising from a mis-specification
of the environment or normal trial-and-error in design phase,
does not prevent agents with OOIs from learning reasonably
-2
 0
 2
 4
 6
 8
0K 2K 4K 6K 8K 10K 12K 14K
C
um
ul
at
iv
e 
re
w
ar
d 
pe
r e
pi
so
de
Episode
LSTM (14)
With OOIs (14)
With OOIs (8)
With OOIs (4)
Without OOIs (14)
Optimal Policy
Figure 7: Cumulative reward per episode obtained on
TreeMaze, using 14, 8 or 4 options. Even with an insuffi-
cient amount of options (8 or 4), OOIs lead to better perfor-
mance (5.90 and 2.67 after 40K episodes) than no OOIs but
14 options (1.92 after 40K episodes). LSTM over options
needs more than 50K episodes before learning anything.
good policies.
TreeMaze is our generalization of the T-maze environ-
ment (Bakker 2001) to arbitrary heights. The agent starts
at the root of the tree-like maze depicted in Figure 6, and
has to reach the extremity of one of the 8 leaves. The leaf to
be reached (the goal) is chosen uniformly randomly before
each episode, and is indicated to the agent using 3 bits, ob-
served one at a time during the first 3 time-steps. The agent
receives no bit afterwards, and has to remember them in or-
der to navigate to the goal. The agent observes its position in
the current corridor (0 to 4) and the number of T junctions it
has already crossed (0 to 3). A reward of -0.1 is given each
time-step, +10 when reaching the goal. The episode finishes
when the agent reaches any of the leaves. The optimal re-
ward is 8.2.
We consider 14 options with predefined memoryless poli-
cies, several of them sharing the same policy, but encod-
ing distinct states (among 14) of a 3-bit memory where
some bits may be unknown. 6 partial-knowledge options
?0??, ?1??, ?00?, ..., ?11? go right then terminate. 8
full-knowledge options ?000, ?001, ..., ?111 go to their cor-
responding leaf. OOIs are defined so that any option may
only be followed by itself, and every option that represents
a memory state where a single 0 or - has been flipped to 1.
Three agents have to learn their top-level policy, which re-
quires them to discover how to use the available options to
represent and update their implicit memory. The agents do
not know the name or meaning of the options, and do not
even know that an implicit memory is required for this task.
The first agent has access to all 14 options. The second agent
only has access to complete-knowledge options, and there-
fore cannot disambiguate unknown and 0 bits. The third
agent is restricted to options ?000, ?010, ?100 and ?110 and
therefore cannot reach odd-numbered goals. The options of
the second and third agents terminate in the first two cells of
the first corridor, to allow the top-level policy to observe the
second and third bits.
Figure 7 shows that OOIs allow the agent with 14 options
to consistently learn the optimal policy for this task. When
the number of options is reduced, the quality of the result-
ing policies decreases, while still remaining above the agent
without OOIs. Even the agent with 4 options, that cannot
reach half the goals, performs better than the agent with-
out OOIs. This experiment demonstrates that OOIs provide
measurable benefits over standard initiation sets, even if the
option set is largely reduced.
Combined, our three experiments demonstrate that OOIs
lead to optimal policies in challenging POMDPs, consis-
tently outperform LSTM over options (and a carefully-
designed expert policy in Section 4.3), allow the option poli-
cies to be learned, and are robust to insufficient amounts of
options.
5 Conclusion and Future Work
This paper proposes OOIs, an extension of the initiation sets
of options so that they restrict which options are allowed
to be executed after one terminates. This makes options as
expressive as Finite State Controllers. Experimental results
confirm that challenging partially observable tasks, simu-
lated or on physical robots, one of them requiring exact in-
formation storage for hundreds of time-steps, can now be
solved using options. On our robotic task, options with OOIs
lead to a stochastic policy better than our expert one. The hi-
erarchical nature of options and simplicity of OOIs enable us
to explain the reasoning of the agent, potentially allowing it
to be applied to new tasks.
Options with OOIs also perform surprisingly well com-
pared to an LSTM network over options. While LSTM over
options does not require the design of OOIs, efficient re-
current neural networks have several hyperparameters to be
tuned, and their ability to learn without any a-priori knowl-
edge comes at the cost of sample efficiency and explainabil-
ity. OOIs therefore provide a compelling alternative, appli-
cable to a wide range of problems.
Finally, the compatibility between OOIs and a large vari-
ety of reinforcement learning algorithms leads to many fu-
ture research opportunities. For instance, we have obtained
very encouraging results in continuous action spaces, using
CACLA (Van Hasselt and Wiering 2007) to implement para-
metric options, that take continuous arguments when exe-
cuted, in continuous-action hierarchical POMDPs.
Acknowledgments
The first author is “Aspirant” with the Science Foundation
of Flanders (FWO, Belgium), grant number 1129317N. The
second author is “Postdoctoral Fellow” with the FWO, grant
number 12J0617N.
Thanks to Finn Lattimore, who gave a computer to the
first author, so that he could finish this paper while attending
the UAI 2017 conference in Sydney, after his own computer
unexpectedly fried.
References
[Angeline et al. 1994] Peter J. Angeline, Gregory M. Saun-
ders, and Jordan B. Pollack. An evolutionary algorithm that
constructs recurrent neural networks. IEEE Trans. Neural
Networks, 5(1):54–65, 1994.
[Bakker 2001] Bram Bakker. Reinforcement learning with
long short-term memory. Advances in Neural Information
Processing Systems, 2001.
[Barto and Mahadevan 2003] Andrew G Barto and Sridhar
Mahadevan. Recent advances in hierarchical reinforcement
learning. Discrete Event Dynamic Systems: Theory and Ap-
plications, 13(12):341–379, 2003.
[Boots et al. 2011] Byron Boots, Sajid M. Siddiqi, and Ge-
offrey J. Gordon. Closing the learning-planning loop with
predictive state representations. The International Journal
of Robotics Research, 30(7):954–966, 2011.
[Cassandra et al. 1994] A Cassandra, L Kaelbling, and
M Littman. Acting optimaly in partially observable stochas-
tic domains. AAAI, (April), 1994.
[Graves et al. 2016] Alex Graves, Greg Wayne, Malcolm
Reynolds, Tim Harley, Ivo Danihelka, Agnieszka Grabska-
Barwin?ska, Sergio Go?mez Colmenarejo, Edward Grefen-
stette, Tiago Ramalho, John Agapiou, Adria? Puigdome?nech
Badia, Karl Moritz Hermann, Yori Zwols, Georg Ostrovski,
Adam Cain, Helen King, Christopher Summerfield, Phil
Blunsom, Koray Kavukcuoglu, and Demis Hassabis. Hy-
brid computing using a neural network with dynamic exter-
nal memory. Nature, 538(7626):471–476, 2016.
[Hochreiter and Schmidhuber 1997] Sepp Hochreiter and
Jurgen Ju?rgen Schmidhuber. Long short-term memory. Neu-
ral Computation, 9(8):1–32, 1997.
[Jo?zefowicz et al. 2015] Rafal Jo?zefowicz, Wojciech
Zaremba, and Ilya Sutskever. An empirical exploration of
recurrent network architectures. In Proceedings of the 32nd
International Conference on Machine Learning (ICML),
pages 2342–2350, 2015.
[Kingma and Ba 2014] Diederik Kingma and Jimmy Ba.
Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
[Lin and Mitchell 1992] Long-Ji Lin and Tom M Mitchell.
Memory approaches to reinforcement learning in non-
Markovian domains. Carnegie-Mellon University. Depart-
ment of Computer Science, 1992.
[Lin and Mitchell 1993] Long-Ji Lin and Tom M Mitchell.
Reinforcement learning with hidden states. From animals
to animats, 2:271–280, 1993.
[Littman et al. 2001] Michael L: Littman, Richard S. Sutton,
and Satinder Singh. Predictive Representations of State.
Neural Information Processing Systems (NIPS), 14:1555–
1561, 2001.
[Meuleau et al. 1999] Nicolas Meuleau, Leonid Peshkin,
Kee-eung Kim, and Leslie Pack Kaelbling. Learning Finite-
State Controllers for partially observable environments. Pro-
ceedings of the fifteenth conference on uncertainty in artifi-
cial intelligence, pages 427–436, 1999.
[Mikolov et al. 2014] Tomas Mikolov, Armand Joulin,
Sumit Chopra, Michae?l Mathieu, and Marc’Aurelio Ran-
zato. Learning longer memory in recurrent neural networks.
CoRR, abs/1412.7753, 2014.
[Mnih et al. 2016] Volodymyr Mnih, Adria? Puigdome?nech
Badia, Mehdi Mirza, Alex Graves, Timothy P Lillicrap,
Tim Harley, David Silver, and Koray Kavukcuoglu. Asyn-
chronous Methods for Deep Reinforcement Learning. In
International Conference on Machine Learning, page 10,
2016.
[Peshkin et al. 2001] Leonid Peshkin, Nicolas Meuleau, and
Leslie Kaelbling. Learning Policies with External Memory.
Sixteenth International Conference on Machine Learning,
page 8, 2001.
[Precup 2000] Doina Precup. Temporal Abstraction in Re-
inforcement Learning. PhD thesis, University of Mas-
sachusetts, 2000.
[Roy et al. 2006] Nicholas Roy, Geoffrey Gordon, and Se-
bastian Thrun. Planning under uncertainty for reliable
health care robotics. Springer Tracts in Advanced Robotics,
24:417–426, 2006.
[Sridharan et al. 2010] Mohan Sridharan, Jeremy Wyatt, and
Richard Dearden. Planning to see: A hierarchical approach
to planning visual actions on a robot using POMDPs. Artifi-
cial Intelligence, 174(11):704–725, 2010.
[Sutton et al. 1999] Richard Sutton, Doina Precup, and
Satinder Singh. Between MDPs and semi-MDPs: A frame-
work for temporal abstraction in reinforcement learning. Ar-
tificial intelligence, 1999.
[Sutton et al. 2000] Richard Sutton, David McAllester,
Satinder Singh, and Yishay Mansour. Policy gradient meth-
ods for reinforcement learning with function approximation.
Advances in Neural Information Processing Systems, 2000.
[Tessler et al. 2016] Chen Tessler, Shahar Givony, Tom Za-
havy, Daniel J Mankowitz, and Shie Mannor. A Deep Hi-
erarchical Approach to Lifelong Learning in Minecraft. In
13th European Workshop on Reinforcement Learning, 2016.
[Theocharous 2002] Georgios Theocharous. Hierarchical
learning and planning in partially observable Markov deci-
sion processes. PhD thesis, Michigan State University, 2002.
[Van Hasselt and Wiering 2007] Hado Van Hasselt and
Marco A. Wiering. Reinforcement learning in continuous
action spaces. Proceedings of the 2007 IEEE Symposium
on Approximate Dynamic Programming and Reinforcement
Learning, pages 272–279, 2007.
[Wiering and Schmidhuber 1997] Marco Wiering and Ju?rgen
Schmidhuber. HQ-Learning. Adaptive Behavior, 6(2):219–
246, 1997.
[Zaremba and Sutskever 2015] Wojciech Zaremba and Ilya
Sutskever. Reinforcement Learning Neural Turing Ma-
chines. Arxiv, 2015.
