Anytime Neural Networks via
Joint Optimization of Auxiliary Losses
Hanzhang Hu1? Debadeepta Dey2 J. Andrew Bagnell1 Martial Hebert1
Abstract
We address the problem of anytime prediction in neural networks. An anytime
predictor automatically adjusts to and utilizes available test-time budget: it pro-
duces a crude initial result quickly and continuously refines the result afterwards.
Traditional feed-forward networks achieve state-of-the-art performance on many
machine learning tasks, but cannot produce anytime predictions during their typi-
cally expensive computation. In this work, we propose to add auxiliary predictions
in a residual network to generate anytime predictions, and optimize these predic-
tions simultaneously. We solve this multi-objective optimization by minimizing
a carefully constructed weighted sum of losses. We also oscillate weightings of
the losses in each iteration to avoid spurious solutions that are optimal for the sum
but not for each individual loss. The proposed approach produces competitive re-
sults if computation is interrupted early, and the same level of performance as the
original network once computation is finished. Observing that the relative perfor-
mance gap between the optimal and our proposed anytime network shrinks as the
network is near completion, we propose a method to combine anytime networks to
achieve more accurate anytime predictions with a constant fraction of additional
cost. We evaluate the proposed methods on real-world visual recognition data-sets
to demonstrate their anytime performance.
1 Introduction
Anytime predictors, defined by Grass & Zilberstein (1996), are predictors that can be interrupted
at any time during testing and still produce valid results, and the more computation before the
interruption the better the results are. There are two main types of applications that require anytime
predictions. First, some applications need to adjust to varying test-time budgets. For instance,
a moving vehicle needs different frequencies of obstacle detection based on its velocity and the
complexity of its surrounding. Second, some applications require real-time responses but allow the
prediction to be continuously improved, e.g., planners on robots need immediate responses to avoid
nearby obstacles, but the planners also want thorough scene analyses later for long term planning.
While the above-mentioned examples would benefit from anytime predictions that can achieve state-
of-the-art performance at each time-budget, it is non-trivial to transform the current state-of-the-
art predictors to competitive anytime predictors. In particular, the recent quick advance in image
recognition tasks are mostly brought by convolutional neural networks (CNN) that have become
increasingly wide and deep, starting from AlexNet(Krizhevsky et al., 2012) and VGG(Simonyan
& Zisserman, 2015), to Residual Network(He et al., 2016), Inception(Christian Szegedy & Alemi,
2017), and DenseNet(Huang et al., 2017). Unfortunately, such complex and accurate predictors are
inherently unfriendly to existing anytime algorithms that form anytime predictions by assembling
multiple predictors, because whenever we aim for the final accurate results of CNN, we have to wait
for the entire network to finish and have no intermediate responses. Such delays can be unacceptable
in robotics, autonomous vehicles, and mobile devices that have limited battery and computation.
?1: Carnegie Mellon Univeristy, 2: Microsoft
Submitted to 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.
ar
X
iv
:1
70
8.
06
83
2v
1 
 [
cs
.L
G
] 
 2
2 
A
ug
 2
01
7
This work proposes augmentations to existing networks to achieve competitive anytime predictions.
Specifically, we desire to train networks capable of outputting competitive intermediate results fre-
quently. In particular, the number of intermediate predictions should grow linearly with the run-time
complexity of the network. Moreover, after spending cost B, we expect the anytime results to be
competitive against those of a network trained specifically for a cost similar to B (Grubb & Bag-
nell, 2012). These requirements above unfortunately rule out forming an ensemble of networks of
depth one, two, three and so on, because the ensemble produces only O(i) anytime predictions after
computing O(i2) layers, and the best network in the ensemble has only complexity O(i). Hence
the prediction frequency shrinks to zero as the ensemble grows, and given the known networks, the
performance of the ensemble is far worse than a single network of O(i2) complexity. One can trade
off between predictive power and frequency by adjusting the growth of the depths of individual
networks, but we cannot achieve the desired competency and frequency at the same time.
To achieve frequent and competitive anytime predictions, this work proposes to form anytime neural
networks (ANNs) by adding uniformly spaced auxiliary predictions and losses to the residual bot-
tleneck units of Resnets (He et al., 2016). We train all the losses simultaneously in a weighted sum,
and we experimentally show that a carefully structured weighting on losses can induce competi-
tive intermediate predictions while keeping the final prediction at the optimal level of performance,
where the optimal at the ith prediction is from training the ANN only for this prediction. We ex-
perimentally justify our proposed weight assignments by studying how some of the more intuitive
ones fail. In addition, we note that optimizing only the fixed weighted sum may result in solutions
that are optimal for the sum but not for individual losses, and we propose to oscillate the weights
in each iteration of the stochastic gradient descent optimization. We show experimentally that such
alternating optimization leads to improvement of anytime predictions. Finally, based on our experi-
mental observation that the performance gap between the optimal and an ANN shrinks as the ANN
approaches its final layers, we propose to form a sequence of ANNs whose depths grow exponen-
tially. If we formalize the observation and assume each ANN is near-optimal in a late fraction of its
layers, we can prove that such a sequence produces frequent predictions that are also near-optimal
at any budget B, where the optimal costs a constant fraction of B.
2 Related Work
Anytime predictions (Grass & Zilberstein, 1996) have been studied from the perspectives of opti-
mization with functional boosting (Grubb & Bagnell, 2012; Hu et al., 2016), and cascades design
(Viola & Jones, 2001; Lefakis & Fleuret, 2010; Xu et al., 2014; Cai et al., 2015). Both approaches
are meta-algorithms that learn sequences of weak learners to produce anytime predictions. Complex
but accurate learners such as CNN are difficult for these meta algorithms to utilize, however, because
of the dilemma of choosing between accurate versus frequent predictions. This work instead pro-
poses to modify feed-forward neural networks to generate intermediate predictions. We also show
experimentally that it is sub-optimal to train the anytime predictions within a network one by one
like in boosting and cascade design. Hence the proposed ANN is orthogonal to the previous anytime
meta-algorithms.
ANNs generate anytime predictions via auxiliary predictions, constructs that are previously applied
for various purposes. Lee et al. (2015) add auxiliary predictions and losses as a form regularization.
In particular, a weighted sum of all auxiliary losses is used as the training loss and the weight of
the intermediate losses shrink as the training proceeds. While this procedure improves the final pre-
diction and speeds up the training of the original network, it does not necessarily yield competitive
anytime predictions in the final model because the weights on auxiliary losses are annealed away.
Some deep and complex architectures (Christian Szegedy & Alemi, 2017; Zhao et al., 2017) add
one single auxiliary loss at a certain layer to combat the problem of vanishing gradients in training
deep neural networks. It is unclear whether the training procedure that optimizes very few auxiliary
losses can be directly applied to an ANN that has many auxiliary losses. Curriculum learning (Ben-
gio et al., 2009; Zamir et al., 2017) also leverages auxiliary losses so that the early predictions of a
model fit easy samples and crude classes, and late predictions fit hard samples and refined classes.
In particular, Zamir et al. (2017) also care about the quality of the anytime predictions, and our novel
training techniques can be applied to such curriculum learning networks that utilize auxiliary losses.
Our proposed alternating optimization procedure for ANN is inspired by both practitioners that task
a single network for multiple purposes (Ren et al., 2015; Misra et al., 2016) and theorists that use
online learning to automatically choose data samples of interest to optimize model parameters with
(Shalev-Shwartz & Wexler, 2016).
2
(a) Anytime Neural Network (b) Ensemble of exponentially deepening
anytime neural network (EANN)
Figure 1: (a): Anytime neural networks contain auxiliary predictions and losses, y?i and `i, for intermediate
feature unit fi. Each fi can be a residual bottleneck unit. (b): Networks in Exponential ANN (EANN) are
computed in order of depth; anytime results are reported if the current depth in the current ANN is higher than
the depth of the previous network (light blue region and arrows).
3 Training Anytime Neural Networks
3.1 Anytime Augmentation a Feed-forward Network
Given a sample (x, y) ? D, the input layer f0 of a feed-forward network is set to x, and the
subsequent layers can be partitioned into a sequence of operations fi for i = 1, ..., L. Each fi
for i ? 1 takes input from the previous result fi?1 and the local parameter ?i of fi to generate
the mid feature fi(fi?1; ?i), e.g., each fi could be a residual bottleneck unit in Resnet (He et al.,
2016). The final prediction y?L(fL;wL) is produced by a prediction unit on the final feature fL
with parameter wL. In Resnet, the prediction unit first global average pools the input feature map,
and then applies a fully connected layer to generate the logits, i.e., the log of predicted conditional
probability of each label given the feature x. The prediction is evaluated by some loss function `.
In Resnet, ` is the softmax cross entropy loss. As illustrated in Fig. 1a, to add auxiliary predictions
to a feed-forward network to form an Anytime Neural Network (ANN), we simply apply the same
type of prediction units to the desired mid features to form prediction y?i(fi;wi) for i = 1, ..., L.
Each prediction y?i then incurs a loss `(y, y?i). We define `i as the expected loss from the prediction
y?i, i.e., `i := E(x,y)?D[`(y, y?i)]. During test-time, ANN computes feature layers sequentially. At
interruption, if a final linear product is allowed, ANN extracts the latest available fi and computes
y?i. If otherwise, it computes y?i once fi becomes available, and reports the latest y?i at interruption.
3.2 Multi-objective Optimization
Let the parameters of the full ANN be ? = (?1, w1, ..., ?L, wL). Let `?i := min? `i(?) be the
minimum expected loss of the ith prediction. The goal of training an ANN is to find a single ?? such
that `i(??) = `?i for all i. We characterize this multi-objective optimization problem as follows:
Seek ?? such that ?? ? arg min?`i(?), ?i = 1, 2, ..., L (1)
While such an ideal ?? may not exist in general multi-objective optimizations, when the network is
highly over-parameterized, it may be reasonable to assume that the sub neural networks of the full
network can encode extra information to enable competitive early predictions, and there may exist
? such that `i(?) ? `?i for any i.
A common approach to multi-objective optimization is to optimize a weighted sum of the losses:
min
?
L?
i=1
Bi`i(?), (2)
where Bi is the weight of the loss `i. This approach is used by almost all feed-forward architec-
tures that have auxiliary losses, such as Inception (Christian Szegedy & Alemi, 2017) and Deeply-
supervised Networks (Lee et al., 2015). We call the various choices of the Bi as weight schemes and
examine them in Sec. 4.3. With observations on some intuitive schemes there, we propose the sieve
3
scheme, where for i = 1, ..., L ? 1, Bi is proportional to one plus the 2-adic valuation2 of L ? i,
1+?2(L?i), andBL is set to be the sum of other weights,
?L?1
i=1 Bi, so that the final prediction has
half of the total weights. Fig. 2f illustrates sieve weights for L = 15. We show experimentally that
the sieve scheme leads to competitive anytime predictions and a near-optimal final prediction, where
the optimal at depth i is from training the ANN only for depth i. Two key observations lead us to this
proposed non-monotone and non-uniform weight scheme. First, and interestingly, the optimization
Eq. 2 may not be solved by a simple end-to-end training. For instance, it is intuitive to set eachBi to
be 1L in order to minimize of the average error rate of y?i, an anytime performance metric equivalent
to the timeliness metric proposed by Karayev et al. (2012). However, this constant weight scheme
does not achieve near-optimal final predictions, because it does not compensate for the fact that the
final losses are typically much smaller than early ones. It also does not focus early losses enough to
guarantee optimal predictions in early layers. Second, increasing the weight Bi can improve both
layer i and its neighbors, possibly because neighbors share most of their models. For instance, if we
put half of the total weights in the final layer BL and split the other half evenly among others, the
error rates at L? 2, L? 1 drop even if their loss weights are cut by half.
Unfortunately, regardless of the scheme, optimizing of Eq. 2 alone is insufficient to find the true
solution ??, because it can get stuck on spurious solutions where only the sum of the gradients?L
i=1Bi?`i(?) is zero and the individual gradients ?`i(?) are non-zero. We propose to solve this
problem by changing the objective each iteration to amplify a different loss: in each iteration, we
can choose a loss `i, and increase temporarily its weight in objective Eq. 2, Bi, by a constant ?
of the total weights, ?
?L
i=1Bi. We choose layer i with the probability proportional to the weight
Bi in the proposed sieve weight scheme. We call this sampling strategy alternating ANN (AANN),
and will show experimentally that such sampling strategy improves anytime predictions, especially
those from mid and early layers. We also experiment with learning the sampling strategy via no-
regret online learning, but this requires more hyper-parameters and has almost no performance gains
in our experiments. We defer these more complex strategies to the appendix.
3.3 Sequence of Exponentially Deepening Anytime Neural Networks
We observe in our experiments in Sec. 4.5 that the relative performance gap between the optimal and
an ANN shrinks as the ANN computation approaches its final layer, due to the large loss weighting
of the final layer. Although the early optimal performance is limited in the network, we still wish
to shrink the relative gap as early in the network computation as possible. We propose to utilize the
much more competitive deeper layers, and form a sequence of ANNs to eliminate their individual
weakness in early layers. Formally, we assume there exists a constant b > 1 such that for any L,
ANNs of depth L are competitive after Lb layers. We learn a sequence of ANNs of exponentially
increasing depths: 1, b1, b2, b3, ... (rounded up), where each ANN is trained independently. In test-
time, we compute the ANN in the sequence in the order of their depths. There are two cases for
prediction. Let the interruption happens on depth i in the jth network, which has depth bj?1. If i
is deeper than the previous network, i.e., i > bj?2, then we report the anytime result at depth i of
the jth ANN. If otherwise, we report the final result of the (j ? 1)th ANN. We call this sequence as
Exponential ANN (EANN) and illustrate it in Fig. 1b. The following proposition proves that EANN
is competitive at any budget with a constant fraction of additional computational complexity.
Proposition 3.1. Let b > 1. Assume for any L, any ANN of depth L has competitive anytime
prediction at depth i > Lb against the optimal of depth i. Then afterB layers of computation, EANN
produces anytime predictions that are competitive against the optimal of depth BC for some C > 1
such that supB C = 2 +
1
b?1 , and C has expectation EB?uniform(1,L)[C] ? 1?
1
2b +
1+ln(b)
b?1 .
With better ANNs, b increases, and the cost inflation rate C shrinks: supB C and E[C] shrink to 2
and to 1. This proposition also implies that if we pay an extra constant fraction of cost, we can build
a strong anytime predictor from weak anytime models that each only needs to predict after 1b of its
total layers are computed. One may wonder what happens if we simply form the sequence using
regular networks instead of ANNs. First, we can no longer produce ?(B) number of predictions
with a cost of B, because the cost of new predictions will grow exponentially. Second, we will have
a larger cost inflation rate C, such that supB C ? 4 and E[C] ? 1.5 +
?
2. We defer the proofs of
these inequalities and the proposition to the appendix.
2For n ? Z+, 2-adic valuation of n, ?2(n), is defined as the highest integer t such that n is divisible by 2t.
4
metric constant expb0.5 expb2 half-end linear midfeat no-grad optimal sieve
AE 19.07 21.19 36.43 22.26 22.63 41.81 24.10 14.89 21.99
1/4 27.43 24.29 64.36 34.98 37.96 62.10 27.89 21.13 32.38
1/2 14.44 19.26 32.15 17.18 20.23 46.86 23.83 12.19 14.97
3/4 10.78 17.99 10.83 10.85 12.18 25.22 20.20 8.90 10.32
C
IF
A
R
10
1 9.19 16.91 7.14 7.30 7.35 7.14 18.87 7.14 7.34
AE 68.97 59.30 68.59 56.24 59.51 - - 46.53 55.51
1/4 82.28 64.23 97.24 74.52 78.69 - - 61.06 73.37
1/2 67.77 54.02 74.92 54.73 63.48 - - 41.78 50.64
3/4 57.37 55.23 42.71 40.60 46.39 - - 33.29 39.20
C
IF
A
R
10
0
1 50.33 55.97 30.66 30.31 30.34 - - 30.32 30.48
Table 1: Performance of Various Anytime Weighting Schemes on CIFAR10 and CIFAR100. The best perfor-
mance of each metric on each data-set that is not the optimal is in bold.
4 Experiment
4.1 Data-sets, Validation Set-up, Evaluation Metrics and the Optimal
We experiment with ANN on CIFAR10, CIFAR100 (Krizhevsky, 2009), and SVHN (Netzer et al.,
2011), where deep convolutional networks are the state-of-the-art predictors 3. We extend the stan-
dard Resnets to form ANNs. Each network starts with a 3x3 convolution that converts the image to
an initial channel size c, followed by three blocks of residual bottleneck units. Each block consists
of n bottleneck units that are designed by Han et al. (2017) and each has two convolutions. Between
a pair of adjacent blocks, there is a 3x3 convolution that doubles the channel size and sub-samples
with a stride of two. Hence the number of convolution layers in a network is 6n+3, and the run-time
of each bottleneck unit stays constant in a network.
For selecting hyper parameters such as weight schemes through validation, we use the last 5000
samples in CIFAR10 and CIFAR100 training sets as validation sets, and choose n = 5, c = 16 for
the validation networks. We apply the selected hyper parameters to the other model configurations.
For all experiments, we use stochastic gradient descent with a momentum of 0.9. The learning
rate schedule is the same as in the original Resnet. Inspired by Karayev et al. (2012), we compute
a metric that estimates the overall anytime performance with the average of top-1 errors (AE).
Following Zamir et al. (2017), we also report the top-1 error at four milestone budget cut-offs: 14 ,
1
2 ,
3
4 and 1 of the total cost. These individual losses can provide insights on how the top-1 error
decreases with the depth of the network. We compare these metrics against those of the optimal
(OPT), where the optimal at depth i is from training an ANN specifically to predict y?i.
4.2 The Importance of Joint Training of Anytime Predictors in Neural Networks
We first show on CIFAR10 the necessity to train anytime predictions in networks jointly: we ex-
amine two intuitive ways to train anytime predictions one by one and show how they fail. In the
first case, we train the regular network targeting the final prediction first, and then train each fixed
mid feature to generate its associated anytime prediction. In Fig. 2a, 2b and Table 1, we call this
approach “midfeat”, and observe that it is significantly worse than the optimal for almost all in-
termediate layers, and only becomes competitive at the last two units. This suggests that the mid
features are not readily suited for predictions. In the second case, we train the ANN one bottleneck
unit at a time: in each training stage, we fix the previous trained anytime predictors and features,
and train the next anytime predictor to convergence. This approach resembles cascade-correlation
networks (Fahlman & Lebiere, 1990). Based on adaptive submodularity (Golovin & Krause, 2011),
it is possible to argue such greedy approach can lead to near-optimal anytime predictions among
ensembles of shallow networks. However, shown as “no-grad” in Fig. 2a, 2b and the top half of
Table 1, this method achieves only competitive results from the first few layers, and cannot produce
competitive predictions afterwards. This negative result suggests that a deep network is fundamen-
tally different from a linear ensemble of shallow networks. Hence it appears necessary to train
anytime predictions jointly.
3All three data-sets consist of 32x32 colored real-world images. CIFAR10 and CIFAR100 have 10 and
100 classes, and each have 50000 training and 10000 testing images. We follow during training the standard
augmentation of CIFAR from (Lee et al., 2015; He et al., 2016): we pad each side by four zeros and randomly
crop a 32x32 image; the image is also randomly flipped left to right. SVHN contains around 600000 training
and around 25000 testing images of numeric digits from the Google Street Views. We apply to SVHN images
the same pad-and-crop augmentation as to CIFAR.
5
(a) CIFAR10 Top-1 Error vs. Number
of Bottlenecks
(b) Zoom-in at the Last Four Bot-
tlenecks of (a)
(c) CIFAR100 Error vs. Cost
(d) CIFAR100 training loss vs. cost (e) CIFAR100 EANN vs. OPT+ (f) Weights in Sieve Scheme
Figure 2: (a) and (b) show curves of top-1 error versus number of bottlenecks units with various weighting
schemes on CIFAR10, using ANN of 15 residual bottlenecks. (b) zooms in at the final four bottlenecks of (a).
(c) plots the error versus number of bottlenecks on CIFAR100. (d) plots the training softmax cross entropy
loss versus the number of bottlenecks on CIFAR100. In (e), “EANN” represents the true performance, and
“EANN(divided by E[C])” divides each cost of “EANN” by E[C] to compare EANN against OPT+ that has
1
E[C]
of the cost, as suggested by Proposition 3.1.
4.3 Weight Schemes of Anytime Losses
This section studies the effects of weight schemes on optimizing Eq. 2 for anytime predictions on
CIFAR10 and CIFAR100 validation sets. The most intuitive scheme is to weigh all anytime outputs
equally, which we call “constant”. In fact, this objective is equivalent to the average top-1 error
(AE). If we ignore the cost at the block transitions so that anytime predictions are produced at a
constant frequency, then minimizing AE is equivalent to maximizing timeliness (Karayev et al.,
2012), a measure of anytime performance. In a simple data-set like CIFAR10, this scheme achieves
the AE that is the closest to the optimal (Fig. 2a and Table 1). However, there is a clear gap between
the optimal and the anytime prediction even at the final layers (Fig. 2b). This gap is magnified on
the more difficult CIFAR100, where the constant scheme achieves the worst performance on almost
all layers as shown in Fig. 2c and Table 1. The final gap exists partially because the final losses
are typically much smaller than the early ones, so that the constant scheme may favor achieving
near-optimal early losses. Another key disadvantage of the constant scheme is that only 1L of the
total weights are on each layer, and the fraction decreases with L. If we believe that for a layer to be
competitive, its loss weight needs to be at least a fraction of the total weight, then we can apply this
belief recursively either from final layer to the first layer or vice versa, and the loss weights should
be exponentially increasing or decreasing with the depth. We test on CIFAR10 and CIFAR100 the
schemes “expb2” and “expb0.5”, which have their loss weights at layer i, Bi, to be proportional to
2i and 0.5i respectively. We found that expb2 achieves near-optimal in the final layers and fail to be
comparable in the early layers, and expb0.5 achieves the opposite. This observation agrees with the
intuition that the quality of the prediction y?i is positively related to its loss weight Bi, and suggests
that exponential increase/decrease in weights is too drastic for achieving competitive overall anytime
performance. We also tested “linear” scheme, whereBi grows linearly with i, as suggested by Zamir
et al. (2017). Similar to expb2 on CIFAR100 as shown in Fig. 2c and the bottom half of Table 1,
linear also achieves near-optimal final performance but sub-optimal intermediate ones.
To find better weight schemes we first try to manually increase loss weights of under-performing
layers, e.g., in “half-end” we set the final weight BL to be half of the total weight, and evenly dis-
tribute the other half among other Bi. Interestingly, this scheme not only achieves the near-optimal
performance in the final layer as designed, but also improves multiple intermediate predictions even
6
Method 1/4 1/2 3/4 1 AE
Milstone 69.44 47.22 50.00 15.28 30.56
AANN 66.67 67.95 76.92 55.13 65.38
(a) Compare Sampling Strategies Against ANN
Method 1/4 1/2 3/4 1
ANN 52.72 28.05 14.36 2.57
AANN 47.77 22.70 7.99 1.66
(b) Relative Increases in Error Rate from OPT
Table 2: (a) Percentages of experiments that the sampling methods are better than ANN. (b) The averages of
relative increases in error rates of ANN and AANN from OPT in percentages.
though they each has only about half of the loss weight as in constant scheme (Fig. 2a, Fig. 2c and
Table 1). Furthermore, the improvement is more drastic near the final layer. As we manually add
weights at 1/4, 1/2, and 3/4 of the total costs, we observe similar improvements of neighborhoods.
A plausible explanation is that because the neighboring predictions are highly correlated, a good
solution for one loss is also competitive for its neighboring losses. Hence it could be both easier
and better to focus on a few spaced out losses than to optimize all losses evenly. Based on these
observations, we develop the heuristic “sieve” weight scheme as described in Sec. 3. On CIFAR100
validation network, sieve scheme achieves the best average error among listed weight schemes and
near-optimal final error. The advantage of sieve over half-end is demonstrated in the plot of training
soft-max cross entropy loss versus depth in Fig. 2d, where the more concentrated weights in sieve
reduce the loss of almost all layers, including those whose relative weights decreased. We use sieve
in all remaining experiments.
4.4 Evaluate Alternating Anytime Neural Networks (AANN)
As explained in Sec. 3, AANN samples layer i with the probability proportional to Bi in each it-
eration and increase the weight Bi temporarily by ?
?L
i=1Bi. We choose ? = 0.5 from the set
{0.25, 0.5, 1.0, 2.0} by experiments on the validation network. Since we evaluate anytime per-
formance at milestone costs, 1/4, 1/2, 3/4 and 1 of the total cost, we also consider the sampling
strategy, called milestone, where half of the samples are split evenly among the milestones, and the
other half are split evenly among the others. Table 3 showcases some performances of AANN on
networks of various complexity, and a more comprehensive table is in the appendix. In total we
experimented these methods on 13 network models and on all three data-sets. Using these 39 exper-
iments, we compare these sampling strategy against the vanilla ANN by computing the percentages
of experiments in which each strategy is better than ANN in each of the five metrics. We list these
percentages in Table 2a. We apply hypothesis testing to better understand these percentages. For
each entry in Table 2a, let X be the probability listed in the entry and p ? [0, 1] be the probability
that ANN outperforms the associated method on the associated metric. Let the null hypothesis H0
be “p ? 0.5” and the alternative hypothesis H1 be “p > 0.5”. Using Hoeffding’s inequality4, we
have Pr(X|H0) ? 0.05 for X ? 0.0598, and Pr(X|H0) ? 0.001 for X ? 0.65. Thus, we accept
that AANN improves over ANN at the first three milestone costs and the average top-1 error. On
the other hand, though the “milestone” strategy focuses directly on the four milestones, it generally
degrades the overall performance, an observation similar to the previous observations on constant
scheme. It would be interesting for future work to further understand why end-to-end training is
not ideal for learning anytime predictions in neural networks. The average of the relative increases
in error rate from OPT in the 39 experiments are shown in Table 2b, where we observe the relative
performance gaps between OPT and ANNs drop exponentially from the early 50% extra mistakes to
almost none (2%) at the final layer. AANN also has a smaller relative performance gap everywhere
than ANN has, an evidence of the uniform improvement of AANN over ANN.
4.5 Evaluation on Multiple Models
In this section, we experiment with ANNs of various model complexity. An ANN is specified by the
number of units in each of its three residual blocks, n, and the initial channel size, c. An ANN can
also decide the prediction period s, i.e., the number of residual bottleneck units between consecutive
predictions. We defer a detailed study on the prediction frequency to the appendix, and set s = 1
unless specified otherwise. Table 3 lists performances of some of the 13 models we experiment on.
A full table is in the appendix. In all experiments, ANN and AANN are competitive against OPT
in the intermediate predictions, and improve these predictions to near-optimal at the final layer. As
expected, as the model becomes deeper and wider, the performance generally improves. However,
4Using Hoeffding’s inequality on the 39 experiments, we know Pr(X|H0) ? Pr(X ? p( + 1)) ?
exp(?782). Under H0, for each probability X Table 2a,  = Xp ? 1 ?
X
2
? 1.
7
SVHN top-1 error % CIFAR10 top-1 error % CIFAR100 top-1 error %
Method 1/4 1/2 3/4 1 AE 1/4 1/2 3/4 1 AE 1/4 1/2 3/4 1 AE
OPT(n=17,c=16) 2.83 1.82 1.83 1.81 13.12 7.42 5.98 5.71 49.73 33.78 28.54 26.66
ANN(n=17,c=16) 5.23 2.64 2.07 1.98 7.78 26.56 10.62 7.61 6.12 17.17 70.80 39.83 32.20 26.59 49.49
AANN(n=17,c=16) 5.32 2.89 2.01 1.87 8.61 24.50 9.20 6.71 6.23 16.19 67.85 42.36 30.58 26.74 49.27
OPT(n=17,c=32) 2.55 1.76 1.81 1.71 9.47 5.63 5.11 4.73 37.57 27.03 23.93 22.80
ANN(n=17,c=32) 4.56 2.61 2.04 1.84 5.88 18.04 7.23 5.98 5.33 12.41 47.53 32.03 27.70 23.92 38.89
AANN(n=17,c=32) 3.80 2.32 1.89 1.80 6.23 14.43 6.77 5.34 5.00 11.50 53.33 31.17 24.00 22.29 39.95
OPT(n=13,c=16) 3.61 2.01 1.97 1.84 14.15 8.33 6.40 6.17 51.16 35.05 29.99 27.04
ANN(n=13,c=16) 6.42 2.88 2.48 1.92 9.35 25.11 11.76 7.86 6.30 18.18 69.97 41.99 31.98 26.94 49.42
AANN(n=13,c=16) 5.72 3.15 2.16 2.01 8.62 25.38 10.11 7.55 6.40 17.64 67.84 44.35 31.41 26.84 49.72
OPT(n=13,c=32) 2.85 1.86 1.68 1.67 9.75 6.27 5.23 4.96 38.65 28.11 24.01 23.29
ANN(n=13,c=32) 4.28 2.75 2.06 1.82 6.79 15.47 7.92 6.23 5.40 12.92 50.62 34.82 26.56 23.60 39.49
AANN(n=13,c=32) 4.05 2.43 1.93 1.77 7.45 15.77 7.01 5.57 5.37 12.70 53.95 31.61 25.19 23.03 40.38
OPT(n=13,c=64) 2.37 1.72 1.71 1.76 7.59 5.28 4.58 4.71 31.80 23.55 21.94 21.13
ANN(n=13,c=64) 3.28 2.06 1.79 1.71 5.40 12.33 6.68 6.03 5.12 11.11 42.84 27.47 23.14 20.95 34.70
AANN(n=13,c=64) 3.28 2.19 1.67 1.61 5.45 11.43 6.19 5.09 4.78 10.31 41.07 25.94 21.98 20.58 33.42
OPT(n=9,c=16) 4.09 2.27 2.09 1.85 15.95 9.35 7.35 6.13 53.16 36.55 31.35 28.15
ANN(n=9,c=16) 6.66 3.42 2.40 1.94 10.26 27.90 11.36 7.73 6.70 18.77 70.05 45.69 33.24 28.08 51.53
AANN(n=9,c=16) 6.96 3.44 2.34 2.04 9.40 27.45 11.95 8.08 7.24 18.51 69.79 46.35 33.65 28.35 51.14
OPT(n=9,c=32) 3.22 1.92 1.87 1.71 11.36 6.76 5.78 5.44 40.09 29.62 25.47 23.63
ANN(n=9,c=32) 4.43 2.89 1.96 1.72 6.94 17.02 8.40 6.24 5.73 13.17 58.55 34.40 27.61 24.05 42.11
AANN(n=9,c=32) 4.28 2.65 1.99 1.83 7.27 18.30 8.20 6.15 5.45 13.72 54.20 33.86 26.14 23.48 41.10
OPT(n=9,c=64) 2.38 1.84 1.84 1.81 8.76 5.51 5.00 5.41 33.23 24.64 22.25 21.84
ANN(n=9,c=64) 3.80 2.33 2.00 1.80 6.43 13.35 7.02 6.10 4.97 11.64 43.02 30.58 26.18 22.58 35.65
AANN(n=9,c=64) 3.61 2.04 1.68 1.67 6.13 11.44 6.89 5.16 5.04 9.93 43.06 27.15 22.50 21.11 34.84
OPT(n=9,c=128) 2.38 1.92 1.79 1.70 7.06 5.32 4.84 5.33 27.71 22.17 21.14 22.07
ANN(n=9,c=128) 3.11 2.05 1.76 1.73 5.26 9.10 5.50 4.77 4.53 8.57 34.97 26.14 23.74 20.25 31.99
AANN(n=9,c=128) 3.11 2.30 1.76 1.73 5.25 9.97 5.53 4.59 4.41 8.57 34.18 24.55 21.70 20.45 30.00
OPT+ (n=25,c=32) 1.79 1.68 1.66 1.66 5.82 5.23 4.73 4.73 25.81 23.63 22.66 22.66
OPT(n=25,c=32) 2.31 1.69 1.70 1.77 8.24 5.42 5.00 5.17 36.28 25.93 22.66 22.93
ANN(n=25,c=32,s=3) 3.70 2.11 1.81 1.77 3.41 14.44 8.05 6.77 5.33 11.23 51.59 31.16 25.81 22.96 37.76
AANN(n=25,c=32,s=3) 3.55 1.94 1.84 1.71 3.26 13.73 6.31 4.93 4.65 9.84 48.72 28.05 23.46 22.00 35.96
EANN(b=2, c=32) 1.93 1.77 1.76 1.71 2.21 5.53 5.31 5.31 4.65 6.18 24.72 23.03 23.01 22.00 26.48
Table 3: Error rates of ANN and AANN on various network models. The last rows compare EANN against
OPT+ that has around 1
E[C]
consumed cost. AEs for OPT are absent because they each requires 3n networks.
The best non-OPT performance of each metric is in bold.
in the more complex models such as (n = 9, c = 128) we found the optimal (OPT) has difficulty
to converge well, so that the final performance may not even be better than the optimal at 3/4 of the
total cost. Anytime neural networks have no trouble converging in these cases and can sometimes
outperform OPT, possibly due to the regularizing effect suggested by Lee et al. (2015). We also
observe that the relative performance difference between ANN and OPT shrinks when we hold n
fixed and increase the initial channel size from 16 to up to 128. We somewhat expected this result,
because a network that is more over-parameterized has more parameters to spare for improving
anytime predictions. This result is also encouraging for ANN, because it means when the base
Resnet is wide and complex so that anytime predictions are more appreciated, ANN is closer to the
optimal. In practice, wide networks are advocated by Zagoruyko & Komodakis (2016) and are used
in some state-of-the-art semantic segmentation networks (Zhao et al., 2017).
4.6 Demonstration of EANN
In this section, we experimentally analyze sequences of exponentially deepening ANNs (EANN) as
described in Sec. 3.3. We train individually AANN models that have c = 32 and n = 1, 3, 7, 13, 25
in order to form an EANN with exponential base b ? 2. By proposition 3.1, E[C] ? 2.44, and
the EANN should compete against the optimal with n = (1 + 3 + 7 + 13 + 25)/2.44 ? 20. In
our comparison, we instead compete against the optimal from n = 25, a more complex and more
competitive model. To set an even more challenging target performance, called OPT+, we also
collect all previously computed optimal performances with c = 32, and the error of OPT+ at depth
i is set to be the minimum error among the available OPT of depth no greater than i. Fig. 2e and
last rows of Table 3 illustrate the performance of EANN against OPT+. Unlike individual AANNs,
EANN, with its constant fraction of additional computational cost, has almost no performance gap
from the OPT+ at every milestone. The performance of EANN can potentially be further improved,
if we train the contained ANNs so that they only predict after 1b of their layers are computed.
5 Conclusion
In this work, we propose weighting techniques to achieve competitive anytime predictions in deep
neural networks without degrading the final performance. We further improve the anytime per-
formance to be near-optimal at any test-time budget using an ensemble technique with a constant
fraction of additional computational cost.
8
References
Auer, Peter, Cesa-Bianchi, Nicolo, Freund, Yoav, and Schapire, Robert E. The nonstochastic multi-
armed bandit problem. In SIAM Journal on Computing, 2002.
Bengio, Y., Louradour, J., Collobert, R., and Weston, J. Curriculum learning. In In Proceedings of
the 26th annual international conference on machine learning, 2009.
Cai, Zhaowei, Saberian, Mohammad J., and Vasconcelos, Nuno. Learning Complexity-Aware Cas-
cades for Deep Pedestrian Detection. In International Conference on Computer Vision (ICCV),
2015.
Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke and Alemi, Alex. Inception-v4, inception-
resnet and the impact of residual connections on learning. In Association for the Advancement of
Artificial Intelligence (AAAI), 2017.
Fahlman, Scott E. and Lebiere, Christian. The cascade-correlation learning architecture. In Ad-
vances in neural information processing systems, 1990.
Golovin, Daniel and Krause, Andreas. Adaptive submodularity: Theory and applications in active
learning and stochastic optimization. In Journal of Artificial Intelligence Research (JAIR), 2011.
Grass, Joshua and Zilberstein, Shlomo. Anytime Algorithm Development Tools. SIGART Bulletin,
1996.
Grubb, Alexander and Bagnell, J. Andrew. SpeedBoost: Anytime Prediction with Uniform Near-
Optimality. In the 15th International Conference on Artificial Intelligence and Statistics (AIS-
TATS), 2012.
Han, Dongyoon, Kim, Jiwhan, and Kim, Junmo. Deep pyramidal residual networks. In Computer
Vision and Pattern Recognition (CVPR), 2017.
He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In Computer
Vision and Pattern Recognition (CVPR), 2016.
Hu, Hanzhang, Grubb, Alexander, Hebert, Martial, and Bagnell, J. Andrew. Efficient feature group
sequencing for anytime linear prediction. In UAI, 2016.
Huang, Gao, Liu, Zhuang, Weinberger, Kilian Q., and van der Maaten, Laurens. Densely connected
convolutional networks. In Computer Vision and Pattern Recognition (CVPR), 2017.
Karayev, Sergey, Baumgartner, Tobias, Fritz, Mario, and Darrell, Trevor. Timely Object Recogni-
tion. In Conference and Workshop on Neural Information Processing Systems (NIPS), 2012.
Krizhevsky, Alex. Learning multiple layers of features from tiny images. Technical report, 2009.
Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. Imagenet classification with deep con-
volutional neural networks. In Advances in Neural Information Processing Systems 25, pp. 1097–
1105, 2012.
Lee, Chen-Yu, Xie, Saining, Gallagher, Patrick W., Zhang, Zhengyou, and Tu, Zhuowen. Deeply-
supervised nets. In International Conference on Artificial Intelligence and Statistics (AISTATS),
2015.
Lefakis, Leonidas and Fleuret, Francois. Joint Cascade Optimization Using a Product of Boosted
Classifiers. In Advances in Neural Information Processing Systems (NIPS), 2010.
Littlestone, N. and Warmuth, M.K. The weighted majority algorithm. Information and Computation,
108(2):212 – 261, 1994.
Misra, Ishan, Shrivastava, Abhinav, Gupta, Abhinav, and Hebert, Martial. Cross-stitch networks for
multi-task learning. In Computer Vision and Pattern Recognition (CVPR), 2016.
Netzer, Yuval, Wang, Tao, Coates, Adam, Bissacco, Alessandro, Wu, Bo, and Ng, Andrew Y. Read-
ing digits in natural images with unsupervised feature learning. In NIPS Workshop on Deep
Learning and Unsupervised Feature Learning 2011, 2011.
9
Ren, Shaoqing, He, Kaiming, Girshick, Ross B., and Sun, Jian. Faster r-cnn: Towards real-time
object detection with region proposal networks. In Advances in Neural Information Processing
Systems (NIPS), 2015.
Shalev-Shwartz, Shai and Wexler, Yonatan. Minimizing the maximal loss: How and why. In Inter-
national Conference on Machine Learning (ICML), 2016.
Shi, Xingjian, Chen, Zhourong, Wang, Hao, Yeung, Dit-Yan, kin Wong, Wai, and chun Woo, Wang.
Convolutional lstm network: A machine learning approach for precipitation nowcasting. In Inter-
national Conference on Neural Information Processing Systems Pages, 2015.
Simonyan, Karen and Zisserman, Andrew. Very deep convolutional networks for large-scale image
recognition. In International Conference on Learning Representations (ICLR), 2015.
Viola, Paul A. and Jones, Michael J. Rapid Object Detection using a Boosted Cascade of Simple
Features. In Computer Vision and Pattern Recognition (CVPR), 2001.
Xu, Z., Kusner, M. J., Weinberger, K. Q., Chen, M., and Chapelle, O. Classifier cascades and trees
for minimizing feature evaluation cost. Journal of Machine Learning Research, 15(1):2113–2144,
2014.
Zagoruyko, Sergey and Komodakis, Nikos. Wide residual networks. In British Machine Vision
Conference (BMVC), 2016.
Zamir, Amir R., Wu, Te-Lin, Sun, Lin, Shen, William, Malik, Jitendra, and Savarese, Silvio. Feed-
back networks. In Computer Vision and Pattern Recognition (CVPR), 2017.
Zhao, Hengshuang, Shi, Jianping, Qi, Xiaojuan, Wang, Xiaogang, and Jia, Jiaya. Pyramid scene
parsing network. In Computer Vision and Pattern Recognition (CVPR), 2017.
10
Supplementary Material for “Anytime Neural Network via Joint Optimization of Auxiliary
Losses”
A Sketch of Proof of Proposition 3.1
Proof. For each budget consumed x, we compute the cost x? of the optimal that EANN is competi-
tive against. The goal is then to analyze the ratio C = xx? . The first ANN in EANN has depth 1. The
optimal and the result of EANN are the same. Now assume EANN is on depth z of ANN number
n+ 1 for n ? 0, which has depth bn.
(Case 1) For z ? bn?1, EANN reuse the result from the end of ANN number n. The cost spent is
x = z +
?n?1
i=0 b
i = z + b
n?1
b?1 . The optimal we compete has cost of the last ANN, which is b
n?1
The ratio satisfies:
C = x/x? =
z
bn?1
+ 1 +
1
b? 1
? 1
bn?1(b? 1)
? 2 + 1
b? 1
+
1
bn?1(b? 1)
n??????? 2 + 1
b? 1
.
Furthermore, since C increases with z,
Ez?Uniform(0,bn?1)[C] ? b1?n
? bn?1
0
zb1?n + 1 +
1
b? 1
dz = 1.5 +
1
b? 1
.
(Case 2) For bn?1 < z ? bn, EANN outputs anytime results from ANN number n + 1 at depth z.
The cost is still x = z + b
n?1
b?1 . The optimal competitor has cost x
? = z. Hence the ratio is
C = x/x? = 1 +
bn ? 1
z(b? 1)
? 2 + 1
b? 1
+
1
bn?1(b? 1)
n??????? 2 + 1
b? 1
.
Furthermore, since C decreases with z,
Ez?Uniform(bn?1,bn)[C] ? (b? 1)?1b1?n
[
(2 +
1
b? 1
) +
? bn
bn?1
2 +
1
b? 1
+
bn ? 1
z(b? 1)
dz
]
n??????? 1 + b ln b
(b? 1)2
Finally, since case 1 and case 2 happen with probability 1b and (1?
1
b ), we have
sup
B
C = 2 +
1
b? 1
and EB?Uniform(0,L)[C] ? 1?
1
2b
+
1
b? 1
+
ln b
b? 1
.
We also note that with large b, supB C ? 2 and E[C]? 1 from above.
If we form a sequence of regular networks that grow exponentially in depth instead of ANN, then the
worst case happen right before a new prediction is produced. Hence the ratio between the consumed
budget and the cost of the optimal that the current anytime prediction can compete, C, right before
the number n+ 1 network is completed, is?n
i=1 b
i
bn?1
n??????? b
2
b? 1
= 2 + (b? 1) + 1
b? 1
.
Note that (b? 1) + 1b?1 ? 4 and the inequality is tight at b = 2. Hence we know supB C is at least
4. Furthermore, the expected value of C, assume B is uniformly sampled such that the interruption
happens on the (n+ 1)th network, is:
E[C] =
1
bn
? bn
0
x+ b
n?1
b?1
bn?1
dx
n??????? 1.5 + b? 1
2
+
1
b? 1
? 1.5 +
?
2 ? 2.91.
The inequality is tight at b = 1+
?
2. With large n, since almost all budgets are consumed by the last
few networks, we know the overall expectation EB?Uniform(0,L)[C] approaches 1.5 + b?12 +
1
b?1 ,
which is at least 1.5 +
?
2.
11
(a) n=7 (b) n=9 (c) n=13
Figure 3: Test error versus cost curves with various periods on CIFAR100. Steps drop at new predictions.
Metric s1 s2 s3 s5
AE 49.30 50.77 44.72 56.86
1/4 68.58 66.48 72.35 74.29
1/2 49.94 51.81 45.48 70.51
3/4 35.78 38.33 35.32 33.80
1 28.86 29.31 29.28 29.30
(a) Anytime error rates for n=7
Metric s1 s2 s3 s5 s7
AE 49.53 50.62 45.77 51.24 42.08
1/4 74.03 73.86 68.97 79.06 67.70
1/2 46.02 46.00 50.26 46.55 39.59
3/4 35.79 35.21 40.12 39.37 32.50
1 28.44 28.36 28.74 28.23 28.55
(b) Anytime error rates for n=9
Metric s1 s2 s3 s4 s5 s8 s10
AE 48.62 49.44 45.39 46.24 45.75 40.80 39.30
1/4 69.92 68.25 68.27 64.45 65.63 61.12 62.65
1/2 42.87 42.70 42.74 44.04 38.18 48.36 37.19
3/4 33.28 33.46 34.17 36.59 29.95 37.10 30.23
1 26.96 26.93 27.27 27.19 27.29 27.20 27.11
(c) Anytime error rates for n=13
Table 4: Tables of anytime performance error rates for various prediction period s on various models (n = 7, 9
and 13) on CIFAR100. The best of each metric (each row) is in bold.
B Frequency of Anytime Prediction
We note that not every residual bottleneck unit is required to compute an anytime output, and it is
possible that allowing more layers to be computed without interruption can lead to better predictions.
This section studies the relationship between frequency and performance of anytime predictions. We
expect that a model with more frequent anytime predictions to perform worse, because optimizing
the extra auxiliary losses limits the freedom of the network. Additionally, with more anytime out-
puts, the average weight of anytime losses decreases, assuming the total loss weight is a constant.
Then according to our previous observations in Sec. 4.3, the less relative weight a layer has, the less
optimized the layer will be. Following (Zamir et al., 2017), we call an ANN that makes an anytime
prediction every i units of feature transformations (residual bottlenecks) as stack-i (i.e., the predic-
tion period is i). The layers that predict bottlenecks whose index can be written as L ? ci, where c
is a non-negative integer, so that the last layer always predict. Fig. 3 and Table 4 demonstrate the
performances of ANNs on CIFAR100 with different stack-i on networks with n = 7, 9, 13. In all
cases all anytime predictions are able to achieve near-optimal final predictions due to the final large
weights from the sieve scheme. From the slightly larger network n = 9 and n = 13, we observe
the general trend of improvement of anytime performances in all metrics as the period s increases.
However, such increase is not without a limit. For instance, when n = 7, if have a relatively large
period, s = 5, then it is possible that a milestone cost ( 12 in this case) is right before a new prediction,
so that the anytime performance at such a milestone suffers. In applications, we should choose large
period s as long as it does not cause much waste of budgets right before milestones.
C In Relation to the Evaluation of Feedback Networks
Feedback networks uses recurrent neural network structures to generate early predictions that can
be used for anytime predictions. Recent work by Zamir et al. (2017) shows it is possible to use
convLSTM (Shi et al., 2015) to train feedback networks for recognition tasks, and show that such
network can produce competitive anytime predictions against the optimal. However, each cell or
12
depth of convLSTM, requires four gates, and each gate contains two convolutions layers. Given
that Feedback Networks uses the same channel size as the original Resnet, each depth of Feedback
Network costs as much as eight convolutional layers, or four residual units. Furthermore, because
each convLSTM cell outputs one single anytime prediction, and Zamir et al. (2017) advise to output
every two cells, feedback network effectively produces an anytime prediction every 16 convolutional
layers, or equivalently every eight residual units. In this work, the cost of network is determined by
the computational costs, e.g., FLOPS, instead of the depth of the computational dependencies, which
is used by Zamir et al. (2017). If our anytime networks are allowed to have four times the cost of
the optimal that we compete against, then we suggest applying EANN, which has been shown in
Sec. 4.6 to be able to achieve competitive performance with an expectation cost inflation E[C],
which is less than four if the exponential base b is no smaller than 1.4.
D No-regret Online learners for Choosing Layers to Amplify
D.1 Learn to Choose Layers via Min-max Optimization
This section consider to replace the heuristic and static strategy in AANN with strategies that are
learned during training. Inspired by Shalev-Shwartz & Wexler (2016), we choose Bi to amplify in
each iteration by applying no-regret online learner on the maximization of the following min-max
optimization:
min
?
max
v??L
L?
i=1
viLi(?), (3)
where ?L is the L-dimensional probability simplex, and Li(?) is a heuristic loss objective of choos-
ing layer i at parameter ?. Note that L may not be the same as the loss of layer i, `i. Intuitively,
this min-max optimization can be considered as a two player zero-sum game, where the max player
chooses layer i using v to generate the maximal loss, and the min player updates ? to reduce the cho-
sen loss. As suggested by Shalev-Shwartz & Wexler (2016), applying no-regret online learner on the
maximization w.r.t. v leads to a no-regret strategy of choosing which layers against any static strate-
gies. There are multiple options for no-regret online learners. For instance, EXP3(Auer et al., 2002)
and Random Weighted Majority (RWM)(Littlestone & Warmuth, 1994) both exponential gradient
ascend on variable v to find layers that consistently results in high losses. Ideally, these no-regret
online learners will discover high loss layer for optimization to focus on, so that eventually all loss
Li will be equal. We also consider another heuristic sampling strategy called AVGL: we sample
layer i with probability proportional to the exponential average of Li.
D.2 Maximization Objective
Now we explain our choice of heuristic loss objective Li so that minimization of Eq. 3 leads to
low loss `i. Ideally, we would like to set Li = max( `i?`
?
i
`?i
, 0) to measure the relative difference
of `i from its optimal `?i . In fact Lee et al. (2015) apply intermediate loss of this very form, and
set the `?i to be a hyper parameter to tune. However, in general tuning this parameter is difficult,
and we cannot compute `?i efficiently without training a model specifically for each i. To avoid the
dependency on `?i , we consider the relative loss difference between neighboring layers, and use the
following min-max optimization to select losses `i to optimize,
min
?
max
v??L
L?1?
i=1
vi
`i(?)? `i+1(?)
`i(?)
+ ?vL max
i=1,...,L?1
`i(?)? `i+1(?)
`i(?)
, (4)
where ? is a constant chosen by cross-validation. Intuitively, for i < L, Li(?) = `i(?)?`i+1(?)`i(?)
represents the relative reduction in loss from layer i to i + 1, and if this value is high, layer i is
performing much worse than its successor while their structures are relative the same. This suggests
layer i could be improved significantly with more optimization focus. The final loss objective is set
to be LL = ?maxi=1,...,L?1 `i(?)?`i+1(?)`i(?) , a constant fraction of the maximal Li of the previous L?
1 layers, because the final layer does not have a successor and we desire a relative high probability
of choosing the final layer according to experiments in Sec. 4.3.
13
(a) EXP3 strategy over epochs (b) RWM strategy over epochs
(c) AVGL strategy over epochs (d) AANN strategy over epochs
Figure 4: Probability of each layer being chosen versus number of iterations using sampling strategy (a) EXP3,
(b) RWM, (c) AVGL, and (d) AANN. (The stack plots are more visible with colors)
D.3 No-regret Online Learning Algorithms
As suggested by Shalev-Shwartz & Wexler (2016), we let the max player use EXP3, a no-regret
online learning algorithm, to update v in the min-max optimization in Eq. 3 as follows. In each
iteration, we first sample a loss `i using EXP3 according to the probability distribution pi = (1 ?
?)vi +
?
L , where ? is a hyper parameter of EXP3 that represents the probability that the algorithm
should explore a loss at random. Then the chosen `i is added to a fixed sum objective
?L
j=1Bj`j
to form the total loss `total =
?L
j=1Bj`j + ?
?L
j=1Bj`i, where ? is a hyper parameter. The
extra weight of `i is ?
?L
j=1Bj , a fraction of the total weight of the sum objective, so that the
chosen `i has significant influence on the total loss. After computing `total, we next update network
parameter ? gradient of `total. Finally, we apply EXP3 update rules to update vi to be viexp(
?Li(?)
Lpi
)
and normalize vector v onto the probability simplex ?L. If we use Random Weighted Majority
(RWM) (Littlestone & Warmuth, 1994) instead of EXP3, the procedure is almost identical: the only
difference is that in each iteration we update all vj , j = 1, ..., L, by exponential gradient descent
using the signal Lj .
D.4 Hyper Parameter Set-up
There are three hyper parameters for applying EXP3 or RWM to alternating optimization: a proba-
bility ? for the online learner to explore uniformly at random, a ratio ? between the reward for the
final layer and the maximum reward among previous layers, and the additional weight of the chosen
layer as ?
?L
i=1Bi. We run grid search on CIFAR10 and CIFAR100 validation network with ?
from {0.1, 0.2, 0.3, 0.4}, ? from {0.7, 0.8, 0.9, 1.0}, and ? from {2, 1, 0.5, 0.25}, and then sort the
settings by average error and final error. We choose the setting that first appears on both ranking:
? = 0.3, ? = 0.8, and ? = 0.5. A true optimal parameter setting may be hard to determine due
14
Method 1/4 1/2 3/4 1 AE
EXP3 31.58 48.68 71.05 47.37 55.26
Milstone 69.44 47.22 50.00 15.28 30.56
RWM 38.24 47.06 61.76 52.94 58.82
AVGL 39.47 53.95 55.26 27.63 65.79
AANN 66.67 67.95 76.92 55.13 65.38
Table 5: Percentages of the total experiments that the listed methods are better than the vanilla ANN.
to the random nature of SGD and the randomized strategy itself, but luckily we found on validation
set that the performance is not overly sensitive to parameters. We set the momentum constant in the
exponential gradient average of the AVGL strategy to be 0.9
D.5 Experimental Results of Various Sampling Strategies
In Fig. 4, we plot the probability that each bottleneck unit i is chosen by various strategies on model
with n = 7, and c = 32. The overall behavior of the probabilities are similar across models, even
the performance ranking are not the same. We plot the stack graph versus the training epochs, so
that we can view how the strategy evolve over time. We found that both no-regret algorithms, EXP3
and RWM, learned to pick the final layer with more than half of the probability. The remaining
probability are roughly split evenly among layers. AVGL learns to put more weight on the final
layer than the other layers as well, but since AVGL update its probabilities with gradient descent
instead of exponentiated gradient descent in EXP3 and RWM, AVGL does not select the final layer
with too high a probability. Moreover, AVGL learns to focus layers around layer 7 and 14, which
are around the transition layers that subsample the feature maps and double the channel sizes via
1x1 convolutions. AANN based on sieve weight scheme naturally focus half of the weights on the
final layer, and it does not spread the remaining weights evenly. Instead, we see intermittent large
weights and small weights, so that every layer is somewhat focused on due to its correlation with its
neighbors, and no neighborhood other than the final layer is overly focused on.
Table 5 lists for each sampling strategy and each anytime evaluation metric the percentage of the
39 experiments in Table 6 such that the corresponding strategy is better than the vanilla ANN in the
corresponding metric. Missing experiments are ignored. We see that RWM degrades performance
in every metric. EXP3 is only able to improve at the 3/4 milestone, and degrades performance at all
other milestones. AVGL is able to somewhat improve performance at 1/2 and 3/4 milestones, and
improves the average performance measured in AE. However, this is at the cost of the final predic-
tion performance, even though we note the difference is usually small. In contrast, AANN is able
to improve performances in every metric, except in the final prediction, which cannot be improved
because ANN is already at the optimal level in this metric. Overall, all of dynamic strategies, EXP3,
RWM and AVGL are not as effective as the proposed simple AANN strategy that samples layers
according to the normalized static weights, Bi. Since each of these dynamic strategies has extra pa-
rameters to tune, and the chosen heuristic loss objective L is probably sub-optimal, our experiments
cannot fully prove that they are not helpful for improving performance of ANN. However, we can
conclude that even if they can be helpful, they require extensive parameter tuning.
E Full Table for Evaluating Sampling Strategies
Table 6 lists the 13 different models that we experiment with sampling strategies on. The ta-
ble also contains the performance of the sampling strategy milestone, which we show to be in-
ferior to the vanilla ANN. These models are specified by pairs (n, c), which are their number of
bottleneck units n per residual block and their initial channel size c. The 13 models we tried
are: (7, 16), (7, 32), (9, 16), (9, 32), (9, 64), (9, 128), (13, 16), (13, 32), (13, 64), (17, 16), (17, 32),
(25, 16), (25, 32). Every model produces anytime predictions on every bottleneck unit, except
n = 25, where we set the model to produce an anytime output every three units, because having
75 anytime predictions seems excessive and slow down the training due to the extra GPU memory
and computation for storing gradients for the additional auxiliary losses. We examine more in detail
about the frequency of the anytime prediction in the next appendix section Sec. B, where we show
that we can trade off between frequency and anytime prediction performance.
15
SVHN CIFAR10 CIFAR100
Method 1/4 1/2 3/4 1 AE 1/4 1/2 3/4 1 AE 1/4 1/2 3/4 1 AE
OPT(n=25,c=16) 3.00 1.82 1.82 1.68 12.63 6.79 5.70 5.80 47.36 33.46 27.31 25.51
ANN(n=25,c=16) 5.98 2.79 2.06 1.87 6.11 22.13 8.46 6.66 6.28 14.58 64.03 38.17 29.00 25.30 45.33
EXP3(n=25,c=16) 5.89 2.77 2.17 1.91 4.54 21.81 9.07 7.22 6.36 15.79 62.96 37.96 28.68 26.07 45.39
Milstone(n=25,c=16) 5.20 2.62 2.13 1.94 6.70 19.42 8.80 6.52 6.04 14.27 63.60 40.36 30.51 27.29 47.64
RWM(n=25,c=16) 5.92 2.74 2.29 2.02 5.23 19.42 8.47 6.52 6.08 14.52 63.49 36.18 28.20 25.94 45.11
AVGL(n=25,c=16) 5.55 2.72 2.25 2.11 5.48 64.88 37.06 29.46 25.97 45.62
AANN(n=25,c=16) 5.10 2.34 2.02 1.92 5.13 21.48 8.12 6.66 6.18 14.29 66.08 36.71 28.32 25.46 45.15
OPT(n=25,c=32) 2.31 1.66 1.70 1.77 8.24 5.42 5.00 5.17 36.28 25.93 22.66 22.93
ANN(n=25,c=32) 3.70 2.11 1.81 1.77 3.41 14.44 8.05 6.77 5.33 11.23 51.59 31.16 25.81 22.96 37.76
EXP3(n=25,c=32) 3.64 1.85 1.71 1.66 2.87 44.85 28.21 23.94 22.40 35.71
Milstone(n=25,c=32) 3.35 2.04 1.71 1.70 3.31 14.04 8.12 6.00 5.70 11.12 53.63 33.40 25.92 23.97 40.04
RWM(n=25,c=32) 3.65 2.14 1.93 1.83 3.11 11.05 6.52 5.34 5.23 9.75 46.22 27.97 23.55 21.98 35.89
AVGL(n=25,c=32) 4.19 2.39 1.95 1.78 3.55 13.26 7.80 5.86 5.51 10.74 53.76 31.70 24.50 22.84 38.72
AANN(n=25,c=32) 3.55 1.94 1.84 1.71 3.26 13.73 6.31 4.93 4.65 9.84 48.72 28.05 23.46 22.00 35.96
OPT(n=17,c=16) 2.83 1.82 1.83 1.81 13.12 7.42 5.98 5.71 49.73 33.78 28.54 26.66
ANN(n=17,c=16) 5.23 2.64 2.07 1.98 7.78 26.56 10.62 7.61 6.12 17.17 70.80 39.83 32.20 26.59 49.49
EXP3(n=17,c=16) 7.52 3.20 2.26 2.06 8.49 22.05 9.78 6.98 6.24 16.11 72.54 41.30 30.75 26.20 49.99
Milstone(n=17,c=16) 6.13 2.88 2.20 2.06 8.62 25.21 11.43 7.81 7.14 18.52 63.71 41.26 30.96 27.90 49.33
RWM(n=17,c=16) 8.43 3.15 2.28 1.93 8.74 27.60 10.45 7.38 6.01 17.60 76.08 48.69 32.57 27.03 53.18
AVGL(n=17,c=16) 6.56 3.44 2.32 2.11 8.03 28.74 10.20 7.50 6.36 18.81 71.20 40.40 30.14 26.75 49.74
AANN(n=17,c=16) 5.32 2.89 2.01 1.87 8.61 24.50 9.20 6.71 6.23 16.19 67.85 42.36 30.58 26.74 49.27
OPT(n=17,c=32) 2.55 1.76 1.81 1.71 9.47 5.63 5.11 4.73 37.57 27.03 23.93 22.80
ANN(n=17,c=32) 4.56 2.61 2.04 1.84 5.88 18.04 7.23 5.98 5.33 12.41 47.53 32.03 27.70 23.92 38.89
EXP3(n=17,c=32) 5.19 2.51 2.00 1.81 6.27 20.20 8.52 5.86 5.19 14.95 58.11 34.55 26.06 22.93 42.96
Milstone(n=17,c=32) 4.06 2.45 2.05 1.86 6.06 15.48 7.39 6.25 5.60 13.59 53.64 32.59 26.29 24.08 41.98
RWM(n=17,c=32) 14.48 6.97 5.44 5.09 12.00 53.65 30.37 23.96 22.58 39.73
AVGL(n=17,c=32) 5.11 2.76 2.17 1.97 5.47 15.29 7.80 6.28 5.41 12.08 53.24 32.53 25.68 23.30 40.38
AANN(n=17,c=32) 3.80 2.32 1.89 1.80 6.23 14.43 6.77 5.34 5.00 11.50 53.33 31.17 24.00 22.29 39.95
OPT(n=13,c=16) 3.61 2.01 1.97 1.84 14.15 8.33 6.40 6.17 51.16 35.05 29.99 27.04
ANN(n=13,c=16) 6.42 2.88 2.48 1.92 9.35 25.11 11.76 7.86 6.30 18.18 69.97 41.99 31.98 26.94 49.42
EXP3(n=13,c=16) 7.15 3.12 2.32 2.10 7.69 26.90 10.26 7.69 6.58 18.17 69.50 44.28 31.37 27.13 50.06
Milstone(n=13,c=16) 5.92 3.09 2.30 2.11 9.06 24.25 10.47 8.08 6.85 17.99 68.96 40.12 30.88 27.66 49.99
RWM(n=13,c=16) 7.23 3.40 2.57 2.12 8.20 25.58 11.74 7.79 6.43 18.01 71.35 42.62 31.40 27.40 50.44
AVGL(n=13,c=16) 7.28 3.44 2.39 2.05 8.46 27.26 10.63 7.53 6.78 17.99 69.08 42.59 31.29 27.61 49.59
AANN(n=13,c=16) 5.72 3.15 2.16 2.01 8.62 25.38 10.11 7.55 6.40 17.64 67.84 44.35 31.41 26.84 49.72
OPT(n=13,c=32) 2.85 1.86 1.68 1.67 9.75 6.27 5.23 4.96 38.65 28.11 24.01 23.29
ANN(n=13,c=32) 4.28 2.75 2.06 1.82 6.79 15.47 7.92 6.23 5.40 12.92 50.62 34.82 26.56 23.60 39.49
EXP3(n=13,c=32) 4.74 2.62 2.26 1.89 5.42 20.97 7.58 5.96 5.35 13.85 56.78 33.96 26.15 23.37 41.67
Milstone(n=13,c=32) 4.10 2.68 2.17 2.01 7.12 14.37 8.52 6.76 6.25 12.96 51.71 34.30 27.11 24.46 41.59
RWM(n=13,c=32) 4.92 2.79 2.18 1.94 5.49 19.90 9.91 6.54 5.62 14.16 55.16 32.03 25.16 22.55 40.53
AVGL(n=13,c=32) 4.27 2.55 2.07 1.86 5.40 15.89 8.21 6.57 5.91 12.59 56.93 33.39 27.35 23.71 41.55
AANN(n=13,c=32) 4.05 2.43 1.93 1.77 7.45 15.77 7.01 5.57 5.37 12.70 53.95 31.61 25.19 23.03 40.38
OPT(n=13,c=64) 2.37 1.72 1.71 1.76 7.59 5.28 4.58 4.71 31.80 23.55 21.94 21.13
ANN(n=13,c=64) 3.28 2.06 1.79 1.71 5.40 12.33 6.68 6.03 5.12 11.11 42.84 27.47 23.14 20.95 34.70
EXP3(n=13,c=64) 3.45 2.45 1.86 1.77 4.03 10.57 5.89 4.90 4.69 9.81 39.56 24.67 21.80 20.29 32.53
Milstone(n=13,c=64) 3.51 2.46 2.11 1.81 5.54 12.17 7.06 6.47 5.10 10.60 40.93 29.54 24.58 22.21 35.29
RWM(n=13,c=64) 3.14 1.92 1.78 1.69 3.52 11.34 5.77 4.62 4.37 10.06 42.32 25.57 21.94 20.55 33.63
AVGL(n=13,c=64) 3.58 2.11 2.02 1.84 4.60 14.84 6.68 5.77 4.93 10.62 39.60 28.40 24.30 21.67 34.00
AANN(n=13,c=64) 3.28 2.19 1.67 1.61 5.45 11.43 6.19 5.09 4.78 10.31 41.07 25.94 21.98 20.58 33.42
OPT(n=9,c=16) 4.09 2.27 2.09 1.85 15.95 9.35 7.35 6.13 53.16 36.55 31.35 28.15
ANN(n=9,c=16) 6.66 3.42 2.40 1.94 10.26 27.90 11.36 7.73 6.70 18.77 70.05 45.69 33.24 28.08 51.53
EXP3(n=9,c=16) 7.33 4.14 2.70 2.15 9.00 31.39 11.33 7.84 6.38 19.50 71.70 45.19 32.27 28.87 51.30
Milstone(n=9,c=16) 5.92 3.33 2.31 1.94 10.30 29.25 12.10 8.12 6.98 19.88 67.03 42.73 31.99 29.02 50.73
RWM(n=9,c=16) 8.13 3.83 2.56 2.18 10.04 30.79 12.35 7.99 6.53 20.14 72.23 45.43 33.00 27.98 51.62
AVGL(n=9,c=16) 6.63 3.47 2.49 2.07 8.49 29.44 11.05 7.88 7.05 18.32 70.07 43.35 32.05 28.76 50.66
AANN(n=9,c=16) 6.96 3.44 2.34 2.04 9.40 27.45 11.95 8.08 7.24 18.51 69.79 46.35 33.65 28.35 51.14
OPT(n=9,c=32) 3.22 1.92 1.87 1.71 11.36 6.76 5.78 5.44 40.09 29.62 25.47 23.63
ANN(n=9,c=32) 4.43 2.89 1.96 1.72 6.94 17.02 8.40 6.24 5.73 13.17 58.55 34.40 27.61 24.05 42.11
EXP3(n=9,c=32) 4.77 2.89 2.08 1.89 5.60 17.99 8.60 6.80 5.45 14.45 58.66 34.01 26.02 23.04 41.96
Milstone(n=9,c=32) 4.62 2.57 2.01 1.87 7.80 17.03 8.75 6.75 5.93 13.77 54.68 33.70 26.88 23.96 42.19
RWM(n=9,c=32) 4.61 3.10 1.99 1.91 5.57 18.21 8.63 6.91 5.65 14.83 61.39 35.22 26.61 24.11 43.03
AVGL(n=9,c=32) 4.65 2.83 2.12 1.87 6.28 20.07 7.67 6.49 5.66 13.60 57.40 33.84 26.07 23.85 41.38
AANN(n=9,c=32) 4.28 2.65 1.99 1.83 7.27 18.30 8.20 6.15 5.45 13.72 54.20 33.86 26.14 23.48 41.10
OPT(n=9,c=64) 2.38 1.84 1.84 1.81 8.76 5.51 5.00 5.41 33.23 24.64 22.25 21.84
ANN(n=9,c=64) 3.80 2.33 2.00 1.80 6.43 13.35 7.02 6.10 4.97 11.64 43.02 30.58 26.18 22.58 35.65
EXP3(n=9,c=64) 4.08 2.55 1.97 1.82 4.68 14.42 7.50 6.02 5.04 11.32 49.18 28.89 23.16 21.20 36.44
Milstone(n=9,c=64) 3.62 2.37 1.90 1.81 6.45 13.14 7.66 5.90 5.61 11.75 37.66 28.86 24.12 22.37 34.50
RWM(n=9,c=64) 3.53 2.17 1.82 1.74 3.91 42.78 26.66 22.40 21.08 34.26
AVGL(n=9,c=64) 3.70 2.37 1.89 1.77 5.22 10.55 6.45 5.37 4.87 10.09 44.91 29.35 23.65 21.88 36.37
AANN(n=9,c=64) 3.61 2.04 1.68 1.67 6.13 11.44 6.89 5.16 5.04 9.93 43.06 27.15 22.50 21.11 34.84
OPT(n=9,c=128) 2.38 1.92 1.79 1.70 7.06 5.32 4.84 5.33 27.71 22.17 21.14 22.07
ANN(n=9,c=128) 3.11 2.05 1.76 1.73 5.26 9.10 5.50 4.77 4.53 8.57 34.97 26.14 23.74 20.25 31.99
EXP3(n=9,c=128) 2.92 1.83 1.71 1.68 3.37 8.32 5.53 4.60 4.32 7.92 34.38 23.07 20.29 19.94 29.75
Milstone(n=9,c=128)
RWM(n=9,c=128) 9.12 5.55 4.52 4.45 7.99
AVGL(n=9,c=128) 3.10 2.18 1.70 1.68 4.27 9.09 5.84 4.74 4.77 8.78 40.94 25.15 22.13 20.72 31.84
AANN(n=9,c=128) 3.11 2.30 1.76 1.73 5.25 9.97 5.53 4.59 4.41 8.57 34.18 24.55 21.70 20.45 30.00
OPT(n=7,c=16) 4.48 2.59 2.08 1.81 17.52 9.91 7.17 6.54 56.82 37.63 31.63 29.49
ANN(n=7,c=16) 6.97 3.50 2.62 1.92 10.60 28.77 12.01 8.88 7.18 19.32 69.56 46.45 36.02 29.13 51.79
EXP3(n=7,c=16) 7.76 4.14 2.85 2.19 9.19 29.49 12.39 8.55 6.93 19.71 70.74 47.85 35.52 29.46 52.62
Milstone(n=7,c=16) 6.52 3.45 2.54 2.12 10.72 29.17 11.59 8.78 7.56 19.68 68.68 44.33 33.53 29.84 51.51
RWM(n=7,c=16) 8.04 3.53 2.77 2.09 9.15 30.68 12.83 8.67 7.08 19.82 71.60 46.60 35.27 29.41 52.83
AVGL(n=7,c=16) 7.04 3.68 2.79 2.09 9.30 29.71 11.55 8.18 6.79 18.93 68.91 46.00 34.10 29.47 51.29
AANN(n=7,c=16) 7.19 3.50 2.72 2.04 11.19 27.84 12.12 9.34 6.90 19.30 68.37 46.61 36.01 29.35 51.84
OPT(n=7,c=32) 3.57 2.09 1.79 1.84 12.16 7.46 5.82 5.26 42.52 31.11 25.81 24.80
ANN(n=7,c=32) 4.66 2.90 2.18 1.85 7.71 17.69 8.58 7.02 5.26 13.43 54.18 34.81 28.09 23.96 41.77
EXP3(n=7,c=32) 5.02 3.15 2.35 1.91 6.30 18.67 8.96 6.81 5.58 14.57 58.87 36.40 27.99 24.62 43.54
Milstone(n=7,c=32) 4.76 2.88 2.25 1.95 8.32 18.59 8.92 6.81 6.33 15.09 53.84 33.23 27.56 23.97 41.48
RWM(n=7,c=32) 4.65 2.79 2.29 2.08 5.90 19.68 9.23 6.91 5.61 14.46
AVGL(n=7,c=32) 4.62 2.52 2.23 1.85 6.51 16.63 8.21 6.64 5.96 13.65 55.84 34.22 27.76 24.70 41.62
AANN(n=7,c=32) 4.41 2.74 2.29 1.93 8.52 17.23 8.35 6.74 5.63 14.22 54.19 34.66 28.12 24.72 41.75
Table 6: Full table for evaluation of ANN and AANN along with other sampling strategies.
16
