i
i
“letter” — 2017/8/28 — 0:14 — page 1 — #1 i
i
i
i
i
i
A dependency look at the reality of constituency
Xinying Chen ? Carlos Go?mez-Rodr??guez † and Ramon Ferrer-i-Cancho1 ‡
?Foreign Languages Research Center, School of Foreign Studies, Xi’an Jiaotong University, No.28 Xianning West Road, 710049 Xi’an, Shaanxi, P.R. China,†Universidade da
Corun?a, FASTPARSE Lab, LyS Research Group, Departamento de Computacio?n, Facultade de Informa?tica, Elvin?a, 15071 A Corun?a, Spain, and ‡Complexity & Qualitative
Linguistics Lab, LARCA Research Group, Departament de Cie?ncies de la Computacio?, Universitat Polite?cnica de Catalunya, Campus Nord, Edifici Omega. Jordi Girona Salgado
1-3. 08034 Barcelona, Catalonia, Spain
Submitted to Proceedings of the National Academy of Sciences of the United States of America
Recently, Nelson et al have addressed the fundamentalproblem of the neurophysiological support for complex
syntactic operations of theoretical computational models (1).
They interpret their compelling results as supporting the neu-
ral reality of phrase structure. Such a conclusion opens various
questions.
First, constituency is not the only possible reality for the
syntactic structure of sentences. An alternative is dependency,
where the structure of a sentence is defined by word pairwise
dependencies (Fig. 1). From that perspective, phrase struc-
ture is regarded as an epiphenomenon of word-word depen-
dencies and constituency (in a classical sense as that of X-bar
theory) has been argued to not exist (2). Furthermore, con-
stituency may not be universal and thus its suitability may
depend on the language (3). Dependency is a stronger alter-
native for its simplicity, its close relationship with merge (4),
its compatibility with recent cognitive observations (5) and
its success over phrase structure in computational linguistics,
where it has become predominant (6).
Second, the authors admit that a parser of the sentence
might transiently conclude that “ten sad students”... is a
phrase consistently with a transient decrease in activity (1st
paragraph of p. 4). Unfortunately, their parser does not ac-
count for that as shown by the counts in Fig. 2 A of (1). In
contrast, a standard dependency parser would because at that
point it would close the dependencies opened by “ten” and
“sad” (Fig. 1). This raises the question of whether the con-
clusions depend on the choice X-bar and particular parser as
a model of phrase structure. The conclusions in (1) may suffer
from circularity, namely the positive support for a particular
X-bar model could be due to the fact that the source was a toy
artificial X-bar grammar. Future analyses would benefit from
the use of natural sentences, sentences with realistic probabil-
ities that are also longer and more complex (sentence length
does not exceed 10 in (1)).
Third, dependency shows the limits of comparing phrase
structure models against n-gram models with n = 2, because
only about 50% of adjacent words are linked (7, 8), thus a
bigram model misses 50% of the dependencies. Bigrams are
a weak baseline, as the common practice in computational
linguistics is using at least smoothed trigram models, and of-
ten 5-gram models, to obtain meaningful predictions (9). A
higher-order lexical n-gram model would strengthen the cur-
rent results. The authors also employ more sophisticated n-
gram models. One is an unbounded model based on part-
of-speech categories, implying a dramatic loss of information
with respect to the original words which might explain its poor
performance. The other is a syntactic n-gram, but not enough
information is provided about its definition and implementa-
tion. Regardless, since the model is obtained from a corpus
derived from a toy grammar and lexicon, its probabilities are
likely to be unrealistic and thus it is problematic.
In sum, dependency offers a better approach to the syntactic
complexity of languages and merge. n-gram models of higher
complexity should be the subject of future research involving
realistic sentences.
1. Nelson MJ et al. (2017) Neurophysiological dynamics of phrase-structure build-
ing during sentence processing. Proceedings of the National Academy of Sciences
114(18):E3669–E3678.
2. Melcuk I (2011) Dependency in language-2011 eds. Gerdes K, Hajicova E, Wanner L.
3. Evans N, Levinson SC (2009) The myth of language universals: language diversity and
its importance for cognitive science. Behavioral and Brain Sciences 32:429–492.
4. Osborne T, Putnam M, Gross T (2011) Bare phrase structure, label-less trees, and
specifier-less syntax: Is minimalism becoming a dependency grammar? The Linguistic
Review 28:315364.
5. Go?mez-Rodr??guez C (2016) Natural language processing and the Now-or-Never bot-
tleneck. Behavioral and Brain Sciences 39:e74.
6. Kubler S, McDonald R, Nivre J, Hirst G (2009) Dependency Parsing. (Morgan and
Claypool Publishers).
7. Liu H (2008) Dependency distance as a metric of language comprehension difficulty.
Journal of Cognitive Science 9:159–191.
8. Ferrer-i-Cancho R (2004) Euclidean distance between syntactically linked words. Phys-
ical Review E 70:056135.
9. Jozefowicz R, Vinyals O, Schuster M, Shazeer N, Wu Y (2016) Exploring the limits of
language modeling. arXiv preprint arXiv:1602.02410.
10. McDonald R et al. (2013) Universal Dependency Annotation for Multilingual Parsing.
pp. 92–97.
ACKNOWLEDGMENTS. X.C. is supported by the Social Science Fund of Shaanxi
State (2015K001). C.G.R is funded by the European Research Council (ERC) un-
der the European Union’s Horizon 2020 research and innovation programme (grant
agreement No 714150 FASTPARSE), and by the TELEPARES-UDC project (FFI2014-
51978-C2-2-R) from MINECO (Ministerio de Economia y Competitividad). R.F.C is
funded by the grants 2014SGR 890 (MACDA) from AGAUR (Generalitat de Catalunya)
and the grant TIN2014-57226-P from MINECO.
Author contributions: X. C., C. G. R. and R. F. C. wrote the paper.
The authors declare no conflict of interest.
1 To whom correspondence should be addressed. E-mail: rferrericancho@cs.upc.edu.
Reserved for Publication Footnotes
www.pnas.org — — PNAS Issue Date Volume Issue Number 1–2
ar
X
iv
:1
70
8.
07
72
2v
1 
 [
cs
.C
L
] 
 2
4 
A
ug
 2
01
7
i
i
“letter” — 2017/8/28 — 0:14 — page 2 — #2 i
i
i
i
i
i
b
Ten sad students of Bill Gates should often sleep.
nummod
amod
nmod
case
name
aux
nsubj
advmod
Fig. 1. Syntactic dependency structure of the sentence in Fig 2 A of (1) according to Universal Dependencies (10).
2 www.pnas.org — — Footline Author
