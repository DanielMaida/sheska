Stochastic Primal-Dual Proximal ExtraGradient
descent for compositely regularized optimization
Tianyi Lina, Linbo Qiaob,?, Teng Zhangc, Jiashi Fengd, Bofeng Zhangb
aDepartment of Industrial Engineering and Operations Research, UC Berkeley, USA
bCollege of Computer, National University of Defense Technology, Changsha, China
cDepartment of Management Science and Engineering, Stanford University, USA
dNational University of Singapore, Singapore
Abstract
We consider a wide range of regularized stochastic minimization problems with
two regularization terms, one of which is composed with a linear function. This
optimization model abstracts a number of important applications in artificial
intelligence and machine learning, such as fused Lasso, fused logistic regression,
and a class of graph-guided regularized minimization. The computational chal-
lenges of this model are in two folds. On one hand, the closed-form solution
of the proximal mapping associated with the composed regularization term or
the expected objective function is not available. On the other hand, the calcu-
lation of the full gradient of the expectation in the objective is very expensive
when the number of input data samples is considerably large. To address these
issues, we propose a stochastic variant of extra-gradient type methods, namely
Stochastic Primal-Dual Proximal ExtraGradient descent (SPDPEG), and analyze
its convergence property for both convex and strongly convex objectives. For
general convex objectives, the uniformly average iterates generated by SPDPEG
converge in expectation with O(1/
?
t) rate. While for strongly convex objec-
tives, the uniformly and non-uniformly average iterates generated by SPDPEG
converge with O(log(t)/t) and O(1/t) rates, respectively. The order of the rate
of the proposed algorithm is known to match the best convergence rate for
?Corresponding author
Email address: qiao.linbo@nudt.edu.cn (Linbo Qiao)
Paper accepted by Neurocomputing August 29, 2017
ar
X
iv
:1
70
8.
05
97
8v
2 
 [
cs
.L
G
] 
 2
7 
A
ug
 2
01
7
first-order stochastic algorithms. Experiments on fused logistic regression and
graph-guided regularized logistic regression problems show that the proposed al-
gorithm performs very efficiently and consistently outperforms other competing
algorithms.
Keywords: Compositely Regularized Optimization, Stochastic Primal-Dual
Proximal ExtraGradient Descent
1. Introduction
In this paper, we are interested in solving a class of convex optimization
problems with both non-composite and composite regularization terms:
min
x?X
E? [l(x, ?)] + r1(x) + r2(Fx), (1)
where X ? Rd is a convex compact set with diameter Dx, the regularization
terms r1 : Rd ? R and r2 : Rl ? R are both convex but possibly nonsmooth,
and r2 is composed with a possibly non-diagonal penalty matrix F ? Rl×d
specifying the desired structured sparsity pattern in x. We denote l(·, ·) : Rd ×
? ? R as a convex and smooth loss function of a rule x for a sample data
{?i = (ai, bi)}, and define the corresponding expectation as l(x) = E? [l(x, ?)].
The above formulation covers quite a few popular models arising from statis-
tics and machine learning, such as Lasso [1] obtained by setting l(x, ?i) =
1
2
??a>i x? bi??2 and r1(x) = ? ?x?1 and r2 = 0, and linear SVM [2] obtained
by letting l(x, ?i) = max
(
0, 1? bi · a>i x
)
and r1(x) = (?/2) ?x?22 and r2 = 0,
where ? > 0 is a parameter. More importantly, we can accommodate prob-
lem (1) with more complicated structures by imposing the non-trivial regu-
larization term r2(Fx), such as fused Lasso [3], fused logistic regression and
graph-guided regularized minimization [4].
The standard algorithm applied to solve problem (1) is proximal gradient
descent [5]. However, there are two main difficulties: 1) Computing the exact
proximal gradient is intractable since the closed-form solution to the proximal
mapping of r1(x)+r2(Fx), or even single r2(Fx) is in usually unavailable; 2) the
2
computational complexity of the full gradient ?l(x) rapidly increases as the size
of samples grows, and is hence prohibitively expensive for modern data-intensive
applications.
A common way to suppress the former one is to introduce a new auxiliary
variable z with z = Fx and reformulate problem (1) as a linearly constrained
convex problem with respect to two variables x and z as follows:
min
x?X
E? [l(x, ?)] + r1(x) + r2(z),
s.t. Fx? z = 0. (2)
Then one can resort to Linearized Alternating Direction Method of Multipliers
(LADMM) [6, 7]. Very recently, Lin et al. [8] have explored the efficiency
of the extra-gradient descent [9, 10], and further showed the hybrid Extra-
Gradient ADM (EGADM) is very efficient on moderate size problems. However,
these methods are computationally expensive due to the computation of the full
gradient in each iteration.
To address the computational issue, several stochastic ADMM algorithms
[11, 12, 13, 14, 15] have been proposed. The idea is to draw a mini-batch of
samples and then compute a noisy sub-gradient of l(x)+r1(x) on the mini-batch
in each iteration. However, for problem (1) with non-smooth regularization
(which is actually common in practice), these sub-gradient type alternating
direction methods may be slow and unstable [16].
In this work, we propose a Stochastic Primal-Dual Proximal Extra-Gradient
Descent (SPDPEG), which inherits the advantages of EGADM and stochastic
methods. Basically, the proposed method computes two noisy gradients of l at
the k-th iteration by randomly drawing two data samples ?k+11 and ?
k+1
2 , and
then performs extra-gradient descent along the noisy gradients. We demonstrate
that the proposed algorithm is very efficient and stable in solving problem (1)
with possible non-smooth terms at large scale.
Our contribution: We propose a novel Stochastic Primal-Dual Proximal
Extra-Gradient Descent (SPDPEG). SPDPEG is efficient in solving large-scale
problems with composite and nonsmooth regularizations. We demonstrate its
3
theoretical convergence for both convex and strongly convex objectives. For
convex objectives, SPDPEG has the convergence rate of O(1/
?
t) in expecta-
tion with the uniformly average iterates. This convergence rate is known to be
the best possible for minimizing general convex objective using first-order noisy
oracle[17]. When the objective to be optimized is strongly convex, SPDPEG con-
verges at the rates of O(log(t)/t) and O(1/t) in expectation with the uniformly
and non-uniformly average iterates, respectively. This matches the convergence
rate of stochastic ADMM with a significantly stronger robustness in terms of
the numerical performance, as confirmed by encouraging experiments on fused
logistic regression and graph-guided regularized minimization tasks.
Related work: The first line of related work are various stochastic al-
ternating direction methods [11, 12, 13, 18, 19, 20, 14, 15] developed to solve
problem (2). They fall into two camps: 1) to compute the noisy sub-gradient
of l + r1 on a mini-batch of data samples and perform sub-gradient descent
[11, 12, 13, 18, 14]; 2) to approximate problem (2) using the finite-sum loss and
perform variance-reduced gradient descent or dual coordinate ascent [19, 20, 15].
For the first group of algorithms, drawing a noisy sub-gradient may lead
to the unstable numerical performance, especially on large-scale problems. In
the experimental section, we compare our algorithm against SGADM [18] and
demonstrate the significant improvement.
For the second group of algorithms, it is not always feasible to use the finite-
sum loss since we know nothing about the underlying distribution of data. In
specific, Zhong and Kwok [19] proposed a Stochastic Averaged Gradient-based
ADM (SAG-ADM) whose iteration complexity is O(1/t). However, SAG-ADM
needs to store a few variables and incurs a very high memory cost. Suzuki [20]
proposed a linearly convergent Stochastic Dual Coordinate Ascent ADM (SDCA-
ADM). However, a stronger assumption on r1 and r2 such as strong convex-
ity and smoothness is imposed. Zheng and Kwok [15] proposed a Stochastic
Variance-Reduce Gradient-based ADM (SVRG-ADM) for convex and non-convex
problems. However, SVRG-ADM only focuses on the finite-sum problem. In
contrast, our SPDPEG approach can be applied to solve problem (2) in very
4
general form.
Very recently, a stochastic variant of hybrid gradient method, namely SPDHG
[21], has been proposed to solve a class of compositely regularized minimization
problems with very special regularization. In specific, r1 ? 0 and r2(x) =
max
y?Y
?y, x? (See Assumption 3 in [21]). However, such assumption is very strong
and does not hold for many compositely regularized minimization problems.
This motivates us to consider problem (2) and develop SPDPEG approach.
The second line of related works is various extra-gradient methods. This
idea is not new and originally proposed by Korpelevich for solving saddle-point
problems and variational inequalities [9, 10]. The convergence and iteration
complexity of extra-gradient methods are established in [22] and [23] respec-
tively. There are also some variants of extra-gradient methods. Solodov and
Svaiter proposed a hybrid proximal extra-gradient method [24], whose iteration
complexity is established by Monteiro and Svaiter in [25, 26, 27]. Bonettini and
Ruggiero studied a generalized extragradient method for total variation based
image restoration problem [28]. To the best of our knowledge, this is the first
time that a stochastic primal-dual variant of extra-gradient type methods is
introduced to solve problem (1).
2. Problem Set-Up and Methods
We make the following assumptions that are common in optimization liter-
ature and usually hold in practice throughout the paper:
Assumption 2.1. The optimal set of problem (1) is nonempty.
Assumption 2.2. l(·) is continuously differentiable with Lipschitz continuous
gradient. That is, there exists a constant L > 0 such that
??l(x1)??l(x2)? ? L ?x1 ? x2? ,?x1, x2 ? X .
Assumption 2.2 holds for many problems in machine learning. For example, the
following least squares and logistic functions are two standard ones:
l(x, ?i) =
1
2
??a>i x? bi??2 or l(x, ?i) = log (1 + exp (?bi · a>i x)) ,
5
where ?i = (ai, bi) is a single data sample.
Assumption 2.3. The regularization functions r1 and r2 are both continuous
but possibly non-smooth; the associated proximal mapping for each individual
regularization admits a closed-form solution, i.e.,
proxri(x) = argmin
y
ri(y) +
1
2
?y ? x?22 (3)
can be calculated in a closed form for i = 1, 2.
Remark 2.4. We remark that Assumption 2.3 is reasonable for a class of opti-
mization problems regularized by `1-norm or nuclear norm, such as fused Lasso,
fused logistic regression, and graph-guided regularized minimization problems.
The proximal mapping of `1-norm can be computed as follows:[
prox?·?1(x)
]
i
= argmin
y
?y?1 +
1
2
?y ? xi?22
=
??? sign(xi)(|xi| ? 1) |xi| > 1,0 |xi| ? 1.
We clarify that the proximal mapping of r(x) and that of r(Fx) are totally
different and have different properties. For example, the proximal mapping of
?x?1 admits a closed-form solution but the proximal mapping of ?Fx?1 does not
admit in general when F is non-diagonal. We only assume that the proximal
mapping of r(x) admits a closed-form solution in Assumption 2.3 but expect to
address the case of r(Fx) whose proximal mapping does not admit a closed-form
solution in general.
Assumption 2.5. The gradient of the objective function l(x) is easy to esti-
mate. Any stochastic gradient estimation ?l(·, ?) for ?l(·) at x satisfies
E? [?l(x, ?)] = ?l(x),
and
E?
[
??l(x, ?)??l(x)?2
]
? ?2,
where ? > 0 is a constant number.
6
Algorithm 1 Stochastic Primal-Dual Proximal ExtraGradient (SPDPEG)
Initialize: x0, z0, and ?0.
for k = 0, 1, 2, · · · do
choose two data samples ?k+11 and ?
k+1
2 randomly;
update zk+1 according to Eq. (4);
update x?k+1 according to Eq. (5);
update ??k+1 according to Eq. (6);
update xk+1 according to Eq. (7);
update ?k+1 according to Eq. (8);
end for
Output: z?t =
t?
k=0
?k+1zk+1, x?t =
t?
k=0
?k+1x?k+1, and ??t =
t?
k=0
?k+1??k+1.
Assumption 2.6. l(·) is µ-strongly convex at x. In other words, there exists
a constant µ > 0 such that
l(y)? l(x)? (y ? x)>?l(x) ? µ
2
?y ? x?2 ,?y ? X .
We remark that our algorithm works even without Assumption 2.6. However,
the lower iteration complexity will be obtained with Assumption 2.6.
We introduce the Stochastic Primal-Dual Proximal ExtraGradient (SPDPEG)
method, and further discuss the choice of step-size. We define the augmented
Lagrangian function for problem (2) as
L? (z, x, ?) = r2(z) + r1(x) + ? (z, x, ?) +
?
2
?Fx? z?2 ,
where ? ? Rp is the dual variable associated with Fx = z. ? is defined as
? (z, x, ?) = l(x)? ??, Fx? z? .
The SPDPEG algorithm is based on the primal-dual update scheme where
(z, x) is primal variable and ? is a dual variable, and can be seen as an inexact
augmented Lagrangian method. The details are presented in Algorithm 1.
We provide details on following four important issues: how to solve the
primal and dual sub-problems easily, how to apply the noisy gradient and per-
7
form extra-gradient descent, how to choose step-size, and how to determine the
weights for the non-uniformly average iterates.
1. Update for z: The first sub-problem in Algorithm 1 is to minimize the
augmented Lagrangian function L? with respect to z, i.e.,
zk+1 := argmin
z
L?(z, xk;?k), (4)
which is equivalent to computing the proximal mapping of r2 and hence
admits a closed-form solution from Assumption 2.3.
2. Stochastic Gradient: According to Assumption 2.5, ? is known to be
easy for gradient estimation with respect to x, and the stochastic gradient
estimation G (z, x, ?; ?) is defined as
G (z, x, ?; ?) = ?l(x, ?)? F>?.
To update x, the SPDPEG algorithm takes a proximal extra-gradient step
using a stochastic gradient estimation G (z, x, ?; ?) and different step-sizes,
i.e.,
x?k+1 := proxck+1r1
(
xk ? ck+1G
(
yk+1, xk, ?k; ?k+11
))
, (5)
??k+1 := ?k ? ?
(
Fxk ? zk+1
)
, (6)
and
xk+1 := proxck+1r1
(
xk ? ck+1G
(
yk+1, x?k+1, ??k+1; ?k+12
))
, (7)
?k+1 := ?k ? ?
(
Fx?k+1 ? zk+1
)
. (8)
3. Step-Size ck+1: The choice of step-size ck+1 depends on whether the
objective function is strongly convex or not. The rate of convergence
varies with respect to different step-size rules. Moreover, a sequence of
vanishing step-sizes is necessary since we do not adopt any technique of
variance reduction in the proposed algorithm.
4. Non-Uniformly Average Iterates: [13] showed that non-uniform aver-
age iterates generated by stochastic algorithms converge with fewer itera-
tions. Inspired by this work, through non-uniformly averaging the iterates
8
of the SPDPEG algorithm and adopting a slightly modified step-size, we
manage to establish an accelerated convergence rate of O(1/t) in expec-
tation.
3. Main Result
In this section, we present the main result in this paper. For general convex
objectives, the uniformly average iterates generated by the SPDPEG algorithm
converge in expectation with O(1/
?
t) rate. While for strongly convex objec-
tives, the uniformly and non-uniformly average iterates generated converge in
expectation with O(log(t)/t) and O(1/t) rates, respectively. The computa-
tional complexity are O(d/
?
t), O(d log(t)/t) and O(d/t) since the per-
iteration complexity is the computational cost of the noisy gradient
on ?1 and ?2 and the proximal mapping, where d is the dimension of deci-
sion variable. The main theoretic results with respect to different settings are
summarized as follows:
1. Assuming that l is a general convex objective function, the step-size is
ck+1 = 1?
k+1+L?
, and the weight of the iterates is ?k+1 = 1t+1 , the proposed
SPDPEG algorithm converges with the O(1/
?
t) rate in expectation.
2. Assuming that l is a µ-strongly convex objective function, the step-size
is ck+1 = 2
µ(k+1)+2L?
, and the weight of the iterates is ?k+1 = 1t+1 , the
proposed SPDPEG algorithm converges with the O(log(t)/t) rate in ex-
pectation.
3. Assuming that l is a µ-strongly convex objective function, the step-size is
ck+1 = 4
µ(k+2)+4L?
, the weight of the iterates is ?k+1 = 2(k+3)(t+1)(t+6) , and the
dual variables are bounded by D? > 0 (this assumption is standard and
also adopted in [13]), the proposed SPDPEG algorithm converges with
the O(1/t) rate in expectation.
In the above, L? is defined as
L? = max
{
8??max(F
>F ) + µ,
?
8L2 + ??max(F>F ) + µ
}
,
9
where ?max(F
>F ) denotes the largest eigenvalue of F>F , and µ = 0 when l is
a general convex objective function.
We present the main theoretic result for uniformly average iterates under
general convex objective functions in the following theorem.
Theorem 3.1. Consider the SPDPEG algorithm with uniformly average iter-
ates. For any optimal solution (z?, x?), it holds that??E[l(x?t)] + E[r1(x?t)] + E[r2(z?t)]? l(x?)? r1(x?)? r2(z?)?? = O(1/?t), (9)??FE[x?t]? E[z?t]?? = O(1/?t). (10)
Note that this implies that the SPDPEG algorithm converges in expectation with
the O(1/
?
t) rate in terms of both the objective error and constraint violation.
We present the main theoretic result for uniformly average iterates under a
strongly convex objective function in the following theorem.
Theorem 3.2. Consider the SPDPEG algorithm with uniformly average iter-
ates. For any optimal solution (z?, x?), it holds that??E[l(x?t)] + E[r1(x?t)] + E[r2(z?t)]? l(x?)? r1(x?)? r2(z?)?? = O(log(t)/t), (11)??FE[x?t]? E[z?t]?? = O(log(t)/t). (12)
Note that this implies that the SPDPEG algorithm converges in expectation with
the O(log(t)/t) rate in terms of both the objective error and constraint violation.
We present the main theoretic result for non-uniformly average iterates under
a strongly convex objective function in the following theorem.
Theorem 3.3. Consider the SPDPEG algorithm with non-uniformly average
iterates. For any optimal solution (z?, x?), it holds that??E[l(x?t)] + E[r1(x?t)] + E[r2(z?t)]? l(x?)? r1(x?)? r2(z?)?? = O(1/t), (13)??FE[x?t]? E[z?t]?? = O(1/t). (14)
Note that this implies that the SPDPEG algorithm converges in expectation with
the O(1/t) rate in terms of both the objective error and constraint violation.
10
4. Proof
We first prove the key technical lemma which is very important to the proof
of Theorem 3.1-Theorem 3.3.
Lemma 4.1. The sequence
{
zk+1, x?k+1, ??k+1, xk+1, ?k+1
}
generated by the SPDPEG
algorithm satisfies the following inequality:
r1(x) + r2(z)? r1(x?k+1)? r2(zk+1) +
?????
z ? zk+1
x? x?k+1
?? ??k+1
?????
>?????
??k+1
G
(
zk+1, x?k+1, ??k+1; ?k+12
)
Fx?k+1 ? zk+1
?????
? 1
2ck+1
??x? xk+1??2 ? 1
2ck+1
??x? xk??2 ? 4ck+1 ???k+1??2 ? 4ck+1 ????k+1??2
? 1
2?
???? ?k??2 + 1
2?
???? ?k+1??2 + [ 1
2?
? 4ck+1?max(F>F )
] ???k ? ??k+1??2
+
[
1
2ck+1
? ??max(F
>F )
2
? 4ck+1L2
] ??xk ? x?k+1??2 + 1
2ck+1
??xk+1 ? x?k+1??2 , (15)
where ?k+1 and ??k+1 are respectively denoted by
?k+1 = ?l(xk, ?k+11 )??l(xk) and ??k+1 = ?l(x?k+1, ?
k+1
2 )??l(x?k+1). (16)
Proof. The first-order optimality condition for updating zk+1 is given by
r2(z)? r2(zk+1) +
?
z ? zk+1, ??k+1
?
? 0. (17)
For x?k+1, xk+1 ? X and any x ? X , the first-order optimality condition for
updating x?k+1 and xk+1 are given respectively by
r1(x)? r1(x?k+1) +
?
x? x?k+1, x?
k+1 ? xk
ck+1
+G
(
zk+1, xk, ?k; ?k+11
)?
? 0,
(18)
r1(x)? r1(xk+1) +
?
x? xk+1, x
k+1 ? xk
ck+1
+G
(
zk+1, x?k+1, ??k+1; ?k+12
)?
? 0.
(19)
Setting x = xk+1 in (18) and x = x?k+1 in (19), and summing two resulting
11
inequalities yields that
1
ck+1
??xk+1 ? x?k+1??2
?
?
xk+1 ? x?k+1, G
(
zk+1, xk, ?k; ?k+11
)
?G
(
zk+1, x?k+1, ??k+1; ?k+12
)?
?
??xk+1 ? x?k+1????G (zk+1, xk, ?k; ?k+11 )?G (zk+1, x?k+1, ??k+1; ?k+12 )?? ,
which implies that
??xk+1 ? x?k+1?? ? ck+1 ??G (zk+1, xk, ?k; ?k+11 )?G (zk+1, x?k+1, ??k+1; ?k+12 )?? .
(20)
Therefore, we get
r1(x
k+1)? r1(x?k+1) +
?
xk+1 ? x?k+1, G
(
zk+1, x?k+1, ??k+1; ?k+12
)?
(21)
?
?
xk+1 ? x?k+1, G
(
zk+1, x?k+1, ??k+1; ?k+12
)
?G
(
zk+1, xk, ?k; ?k+11
)?
?
?
xk+1 ? x?k+1, x?
k+1 ? xk
ck+1
?
? ?ck+1
??G (zk+1, x?k+1, ??k+1; ?k+12 )?G (zk+1, xk, ?k; ?k+11 )??2
? 1
2ck+1
??xk+1 ? xk??2 + 1
2ck+1
??xk+1 ? x?k+1??2 + 1
2ck+1
??x?k+1 ? xk??2 .
where the first inequality is obtained by letting x = xk+1 in (18) and the second
inequality follows from (20). Furthermore, we have
??G (zk+1, x?k+1, ??k+1; ?k+12 )?G (zk+1, xk, ?k; ?k+11 )??2 (22)
=
????k+1 +?l(x?k+1)? F>??k+1 ? [?k+1 +?l(xk)? F>?k]??2
? 4
???k+1??2 + 4 ????k+1??2 + 4L2 ??xk ? x?k+1??2 + 4?max(F>F )???k ? ??k+1??2 ,
where ?k+1 and ??k+1 are defined in (16). By substituting (22) into (21), and
12
then summing the resulting inequality and (19), we have
r1(x)? r1(x?k+1) +
?
x? x?k+1, G
(
zk+1, x?k+1, ??k+1; ?k+12
)?
(23)
? ?4ck+1
???k+1??2 ? 4ck+1 ????k+1??2 ? 4ck+1?max(F>F )???k ? ??k+1??2
?4ck+1L2
??xk ? x?k+1??2 ? 1
2ck+1
??xk+1 ? xk??2 + 1
2ck+1
??xk+1 ? x?k+1??2
+
1
2ck+1
??x?k+1 ? xk??2 ??x? xk+1, xk+1 ? xk
ck+1
?
= ?4ck+1
???k+1??2 ? 4ck+1 ????k+1??2 ? 4ck+1?max(F>F )???k ? ??k+1??2
+
1
2ck+1
??xk+1 ? x?k+1??2 + [ 1
2ck+1
? 4ck+1L2
] ??xk ? x?k+1??2
? 1
2ck+1
??x? xk??2 + 1
2ck+1
??x? xk+1??2 .
On the other hand, we have
?
?? ??k+1, F x?k+1 ? zk+1
?
(24)
=
1
?
?
?? ?k+1 + ?k+1 ? ??k+1, ?k ? ?k+1
?
= ? 1
2?
???? ?k??2 + 1
2?
???? ?k+1??2 ? 1
2?
???k+1 ? ??k+1??2 + 1
2?
???k ? ??k+1??2
? ? 1
2?
???? ?k??2 + 1
2?
???? ?k+1??2 + 1
2?
???k ? ??k+1??2 ? ??max(F>F )
2
??xk ? x?k+1??2 ,
where the last inequality holds since
?k+1 ? ??k+1 = ?
(
Fx?k+1 ? zk+1
)
? ?
(
Fxk ? zk+1
)
= ?
(
Fx?k+1 ? Fxk
)
.
Finally, combining (17), (23) and (24) yields (15). 
4.1. Proof of Theorem 3.1
Lemma 4.2. Suppose that
{
zk+1, x?k+1, ??k+1, xk+1, ?k+1
}
are generated by the
SPDPEG algorithm, and ?k+1 and ck+1 are defined in the main paper. For any
13
optimal solution (z?, x?, ??), it holds that
l(x?) + r1(x
?) + r2(z
?)? E[l(x?k+1)]? E[r1(x?k+1)]? E[r2(zk+1)] (25)
+E
?????
?????
z? ? zk+1
x? ? x?k+1
?? ??k+1
?????
>?????
??k+1
?F>??k+1
Fx?k+1 ? zk+1
?????
?????
?
?
k + 1 + L?
2
E
??x? ? xk+1??2 ? ?k + 1 + L?
2
E
??x? ? xk??2 ? 8?2?
k + 1
? 1
2?
E
???? ?k??2 + 1
2?
E
???? ?k+1??2 .
Proof. By the definition of L?, we have 12? ? 4c
k+1?max(F
>F ) ? 0 and 1
2ck+1
?
??max(F
>F )
2 ? 4c
k+1L2 ? 0. Plugging them into (15) yields that
r1(x) + r2(z)? r1(x?k+1)? r2(zk+1) +
?????
z ? zk+1
x? x?k+1
?? ??k+1
?????
>?????
??k+1
G
(
zk+1, x?k+1, ??k+1; ?k+12
)
Fx?k+1 ? zk+1
?????
? 1
2ck+1
??x? xk+1??2 ? 1
2ck+1
??x? xk??2 ? 4ck+1 ???k+1??2 ? 4ck+1 ????k+1??2
? 1
2?
???? ?k??2 + 1
2?
???? ?k+1??2 .
Moreover, we have
(
x? x?k+1
)>
G
(
zk+1, x?k+1, ??k+1; ?k+12
)
=
(
x? x?k+1
)>?l(x?k+1) + (x? x?k+1)> ??k+1 + (x? x?k+1)> [?F>??k+1]
? l(x)? l(x?k+1) +
(
x? x?k+1
)> [?F>??k+1]+ (x? x?k+1)> ??k+1.
Therefore, we conclude that
l(x) + r1(x) + r2(z)? l(x?k+1)? r1(x?k+1)? r2(zk+1) +
?????
z ? zk+1
x? x?k+1
?? ??k+1
?????
>?????
??k+1
?F>??k+1
Fx?k+1 ? zk+1
?????
?
?
k + 1 + L?
2
??x? xk+1??2 ? ?k + 1 + L?
2
??x? xk??2 ? 4?
k + 1 + L?
(???k+1??2 + ????k+1??2)
? 1
2?
???? ?k??2 + 1
2?
???? ?k+1??2 ? (x? x?k+1)> ??k+1.
14
Since xk and x?k+1 are independent of ?k+11 and ?
k+1
2 respectively, we take the
expectation on both sides of above inequality conditioning on ?k+12 , then ?
k+1
1 ,
and then {?j1, ?
j
2}j?k. Finally, we set (z, x) = (z?, x?), and conclude (25). 
We are ready to prove Theorem 3.1. For any ? ? Rp, we have
l(x?) + r1(x
?) + r2(z
?)? E[l(x?t)]? E[r1(x?t)]? E[r2(z?t)] + ?>
(
FE[x?t]? E[z?t]
)
= l(x?) + r1(x
?) + r2(z
?)? E[l(x?t)]? E[r1(x?t)]? E[r2(z?t)] + E
?????
?????
z? ? z?t
x? ? x?t
?? ??t
?????
>?????
??t
?F>??t
F x?t ? z?t
?????
?????
? 1
t+ 1
t?
k=0
{
l(x?) + r1(x
?) + r2(z
?)? E[l(x?k+1)]? E[r1(x?k+1)]? E[r2(zk+1)]
+E
?????
?????
z? ? zk+1
x? ? x?k+1
?? ??k+1
?????
>?????
??k+1
?F>??k+1
F x?k+1 ? zk+1
?????
?????
???????
? 1
t+ 1
t?
k=0
[?
k + 1 + L?
2
E
???x? ? xk+1???2 ? ?k + 1 + L?
2
E
???x? ? xk???2 ? 8?2?
k + 1
? 1
2?
E
????? ?k???2 + 1
2?
E
????? ?k+1???2]
? ? L?
2(t+ 1)
??x? ? x0??2 ? D2x + 16?2
2
?
t+ 1
? 1
2?(t+ 1)
???? ?0??2 , (26)
where the first inequality holds due to the convexity of l, r1 and r2. Note that
the optimality condition imply the following inequality
0 ? l(x?)+r1(x?)+r2(z?)?E[l(x?t)]?E[r1(x?t)]?E[r2(z?t)]+(??)>
(
FE[x?t]? E[z?t]
)
.
(27)
Now, define ? := ???? + 1. By using Cauchy-Schwarz inequality in (27), we
obtain
0 ? E[l(x?t)]+E[r1(x?t)]+E[r2(z?t)]? l(x?)?r1(x?)?r2(z?)+?
??FE[x?t]? E[z?t]?? .
(28)
By setting ? = ?? (FE[x?t]? E[z?t]) / ?FE[x?t]? E[z?t]? in (26), and noting that
15
??? = ?, we obtain
E[l(x?t)] + E[r1(x?t)] + E[r2(z?t)]? l(x?)? r1(x?)? r2(z?) + ?
??FE[x?t]? E[z?t]??
? L?D
2
x
2(t+ 1)
+
D2x + 16?
2
2
?
t+ 1
+
?2 +
???0??2
?(t+ 1)
. (29)
We now define the function
v(?) = min {l(x) + r1(x) + r2(z)|Fx? z = ?, x ? X} .
It is easy to verify that v is convex, v(0) = l(x?)+r1(x
?)+r2(z
?), and ?? ? ?v(0).
Therefore, from the convexity of v, it holds that
v(?) ? v(0) + ???, ?? ? l(x?) + r1(x?) + r2(z?)? ???? ??? . (30)
Let ?? = FE[x?t]? E[z?t], we have
E[l(x?t)] + E[r1(x?t)] + E[r2(z?t)] ? l(E[x?t]) + r1(E[x?t]) + r2(E[z?t]) ? v(??).
Therefore, combining (28), (29) and (30), we get
????? ???? ? E[l(x?t)] + E[r1(x?t)] + E[r2(z?t)]? l(x?)? r1(x?)? r2(z?)
? L?D
2
x
2(t+ 1)
+
D2x + 16?
2
2
?
t+ 1
+
?2 +
???0??2
?(t+ 1)
? ? ???? ,
which implies (9) and (10).
4.2. Proof of Theorem 3.2
Lemma 4.3. Let
{
zk+1, x?k+1, ??k+1, xk+1, ?k+1
}
be generated by the SPDPEG
Algorithm, and ?k+1 and ck+1 be defined in the main paper. For any optimal
solution (z?, x?), it holds that
l(x?) + r1(x
?) + r2(z
?)? E[l(x?k+1)]? E[r1(x?k+1)]? E[r2(zk+1)]
+E
?????
?????
z? ? zk+1
x? ? x?k+1
?? ??k+1
?????
>?????
??k+1
?F>??k+1
Fx?k+1 ? zk+1
?????
?????
? µ(k + 2) + 2L?
4
E
??x? ? xk+1??2 ? µ(k + 1) + 2L?
4
E
??x? ? xk??2 ? 16?2
µ(k + 1)
? 1
2?
E
???? ?k??2 + 1
2?
E
???? ?k+1??2 . (31)
16
Proof. Since 12? ? 4c
k+1?max(F
>F ) ? 0 and 1
2ck+1
? ??max(F
>F )
2 ? 4c
k+1L2 ? 0
and ck+1 < 1µ , we conclude from (15) that
r1(x) + r2(z)? r1(x?k+1)? r2(zk+1) +
?????
z ? zk+1
x? x?k+1
?? ??k+1
?????
>?????
??k+1
G
(
zk+1, x?k+1, ??k+1; ?k+12
)
Fx?k+1 ? zk+1
?????
? 1
2ck+1
??x? xk+1??2 ? 1
2ck+1
??x? xk??2 ? 4ck+1 ???k+1??2 ? 4ck+1 ????k+1??2
? 1
2?
???? ?k??2 + 1
2?
???? ?k+1??2 + µ
2
??x?k+1 ? xk+1??2 .
Moreover, we have
(
x? x?k+1
)>
G
(
zk+1, x?k+1, ??k+1; ?k+12
)
=
(
x? x?k+1
)>?l(x?k+1) + (x? x?k+1)> ??k+1 + (x? x?k+1)> [?A>??k+1]
? l(x)? l(x?k+1)? µ
2
??x? x?k+1??2 + (x? x?k+1)> [?A>??k+1]+ (x? x?k+1)> ??k+1.
Therefore, we conclude that
l(x) + r1(x) + r2(z)? l(x?k+1)? r1(x?k+1)? r2(zk+1) +
?????
z ? zk+1
x? x?k+1
?? ??k+1
?????
>?????
??k+1
?F>??k+1
F x?k+1 ? zk+1
?????
? µ(k + 1) + 2L?
4
???x? xk+1???2 ? µ(k + 1) + 2L?
4
???x? xk???2 ? 8
µ(k + 1) + L?
(????k+1???2 + ?????k+1???2)
? 1
2?
????? ?k???2 + 1
2?
????? ?k+1???2 ? (x? x?k+1)> ??k+1 + µ
2
???x? x?k+1???2 + µ
2
???x?k+1 ? xk+1???2
? µ(k + 2) + 2L?
4
???x? xk+1???2 ? µ(k + 1) + 2L?
4
???x? xk???2 ? 8
µ(k + 1) + 2L?
(????k+1???2 + ?????k+1???2)
? 1
2?
????? ?k???2 + 1
2?
????? ?k+1???2 ? (x? x?k+1)> ??k+1.
Since xk and x?k+1 are independent of ?k+11 and ?
k+1
2 respectively, we take the
expectation on both sides of above inequality conditioning on ?k+12 , then ?
k+1
1
and then {?j1, ?
j
2}j?k. Finally, we set (z, x) = (z?, x?), and conclude (31). 
17
We are ready to prove Theorem 3.2. For any ? ? Rp, we have
l(x?) + r1(x
?) + r2(z
?)? E[l(x?t)]? E[r1(x?t)]? E[r2(z?t)] + ?>
(
FE[x?t]? E[z?t]
)
? 1
t+ 1
t?
k=0
{
l(x?) + r1(x
?) + r2(z
?)? E[l(x?k+1)]? E[r1(x?k+1)]? E[r2(zk+1)]
+E
?????
?????
z? ? zk+1
x? ? x?k+1
?? ??k+1
?????
>?????
??k+1
?F>??k+1
Fx?k+1 ? zk+1
?????
?????
?????????
? 1
t+ 1
t?
k=0
[
µ(k + 2) + 2L?
4
E
??x? ? xk+1??2 ? µ(k + 1) + 2L?
4
E
??x? ? xk??2 ? 16?2
µ(k + 1)
? 1
2?
E
???? ?k??2 + 1
2?
E
???? ?k+1??2]
? ? µ+ 2L?
4(t+ 1)
??x? ? x0??2 ? 16?2 log(t+ 1)
µ(t+ 1)
? 1
2?(t+ 1)
???? ?0??2 .
where the first inequality holds due to the convexity of l, r1 and r2. By the
same argument as Theorem 3.1, we conclude (11) and (12).
4.3. Proof of Theorem 3.3
Lemma 4.4. Let
{
zk+1, x?k+1, ??k+1, xk+1, ?k+1
}
be generated by the SPDPEG
Algorithm, and ?k+1 and ck+1 be defined in the main paper. For any optimal
solution (z?, x?), it holds that
l(x?) + r1(x
?) + r2(z
?)? E[l(x?k+1)]? E[r1(x?k+1)]? E[r2(zk+1)]
+E
?????
?????
z? ? zk+1
x? ? x?k+1
?? ??k+1
?????
>?????
??k+1
?F>??k+1
Fx?k+1 ? zk+1
?????
?????
? µ(k + 4) + 4L?
8
E
??x? ? xk+1??2 ? µ(k + 2) + 4L?
8
E
??x? ? xk??2 ? 32?2
µ(k + 2)
? 1
2?
E
???? ?k??2 + 1
2?
E
???? ?k+1??2 . (32)
18
Proof. By the same argument as Lemma 4.3, we conclude from (15) that
r1(x) + r2(z)? r1(x?k+1)? r2(zk+1) +
?????
z ? zk+1
x? x?k+1
?? ??k+1
?????
>?????
??k+1
G
(
zk+1, x?k+1, ??k+1; ?k+12
)
F x?k+1 ? zk+1
?????
? 1
2ck+1
???x? xk+1???2 ? 1
2ck+1
???x? xk???2 ? 4ck+1 ????k+1???2 ? 4ck+1 ?????k+1???2
? 1
2?
????? ?k???2 + 1
2?
????? ?k+1???2 + µ
2
???x?k+1 ? xk+1???2 ,
and
(
x? x?k+1
)>
G
(
zk+1, x?k+1, ??k+1; ?k+12
)
? l(x)? l(x?k+1)? µ
2
??x? x?k+1??2 + (x? x?k+1)> [?A>??k+1]+ (x? x?k+1)> ??k+1.
Therefore, we conclude that
l(x) + r1(x) + r2(z)? l(x?k+1)? r1(x?k+1)? r2(zk+1) +
?????
z ? zk+1
x? x?k+1
?? ??k+1
?????
>?????
??k+1
?F>??k+1
Fx?k+1 ? zk+1
?????
? µ(k + 4) + 4L?
8
??x? xk+1??2 ? µ(k + 2) + 4L?
8
??x? xk??2 ? 16
µ(k + 2) + 4L?
(???k+1??2 + ????k+1??2)
? 1
2?
???? ?k??2 + 1
2?
???? ?k+1??2 ? (x? x?k+1)> ??k+1.
Since xk and x?k+1 are independent of ?k+11 and ?
k+1
2 respectively, we take the
expectation on both sides of above inequality conditioning on ?k+12 , then ?
k+1
1
and then {?j1, ?
j
2}j?k. Finally, we set (z, x) = (z?, x?), and conclude (32). 
19
We are ready to prove Theorem 3.3. For any ? ? Rp, we have
l(x?) + r1(x
?) + r2(z
?)? E[l(x?t)]? E[r1(x?t)]? E[r2(z?t)] + ?>
(
FE[x?t]? E[z?t]
)
? 2
(t+ 1)(t+ 6)
t?
k=0
(k + 3)
{
l(x?) + r1(x
?) + r2(z
?)? E[l(x?k+1)]? E[r1(x?k+1)]? E[r2(zk+1)]
+E
?????
?????
z? ? zk+1
x? ? x?k+1
?? ??k+1
?????
>?????
??k+1
?F>??k+1
Fx?k+1 ? zk+1
?????
?????
?????????
? 2
(t+ 1)(t+ 6)
t?
k=0
(k + 3)
[
µ(k + 4) + 4L?
8
E
??x? ? xk+1??2 ? µ(k + 2) + 4L?
8
E
??x? ? xk??2
? 32?
2
µ(k + 2)
? 1
2?
E
???? ?k??2 + 1
2?
E
???? ?k+1??2]
? ? 3µ+ 2L?
4(t+ 1)(t+ 6)
??x? ? x0??2 ? 96?2
µ(t+ 6)
? 2 ???
2
+ 2D2?
?(t+ 1)
,
where the first inequality holds due to the convexity of l, r1 and r2. By the
same argument as Theorem 3.1, we conclude (13) and (14).
5. Experiment
We apply our proposed SPDPEG algorithm to solve following two popular
problems: fused logistic regression (FLR) (33) and graph-guided regularized
logistic regression (GGRLR) (34) [19], which are formulated as follows
FLR: min
x
l(x) + ??x?1 + ??Lx?1, (33)
and
GGRLR: min
x
l(x) +
?
2
?x?22 + ??Fx?1. (34)
Here l(x) = 1
N
[
N?
i=1
l(x, ?i)
]
, where l(x, ?i) is the logistic loss on ?i and ? > 0 is a
parameter. L and F are penalty matrices promoting the desired sparse structure
of x. Specifically, L ? R(n?1)×n in problem (33) is specified as a matrix with all
ones on the diagonal, negative ones on the super-diagonal and zeros elsewhere,
and F in problem (34) is generated by sparse inverse covariance selection [29].
20
Table 1: Statistics of datasets.
dataset number of samples dimensionality
splice 1000 60
svmguide3 1243 21
mushrooms 8,124 112
a9a 32,561 123
w8a 64,700 300
hitech 2,301 10,080
k1b 2,340 21,839
classic 7,094 41,681
In the experiments, we compare our SPDPEG algorithm with the EGADM
algorithm [8] and six existing stochastic ADMM-type algorithms 1: SGADM [18],
SADMM [11], OPG-ADMM [12], RDA-ADMM [12], and two adaptive SADMM
(i.e., SADMMdiag and SADMMfull)[14]. We exclude online ADMM [30] since
[12] has shown that RDA-ADMM performs better than online ADMM. FSADMM
[19] is also excluded since it requires storage of all gradients, which results in
impractical performance in some complex applications [31].
The experiments are conducted on five binary classification datasets: splice,
svmguide3, mushrooms, a9a, and w8a 2 with large number of samples, clas-
sic, hitech, k1b 3 with high dimensionality. We set the parameters of SPDPEG
exactly following our theory while using the cross validation to select the param-
eters for other algorithms. For each dataset, we calculate the lipschitz constant
L as its classical upper bound L? = 0.25 max1?i?n ?ai?2. The regularization
parameter ? = 5 × 10?3 and ? = 5 × 10?4 for problem (33), and ? = 10?5
and ? = 10?2 for problem (34). To reduce statistical variability, experimental
results are repeated 5 rounds. Additionally, we use the metrics including objec-
1We use the implementation of SADMM, OPG-ADMM and RDA-ADMM provided by the
authors and two adaptive ADMM according to [14]
2https://www.csie.ntu.edu.tw/?cjlin/libsvm/
3https://www.shi-zhong.com/software/docdata.zip
21
1 2 3
Time
0.65
0.7
0.75
0.8
0.85
0 1 2
Time
0.4
0.5
0.6
0.7
0 1 2
Time
0.4
0.5
0.6
0.7
2 4 6 8 10
Time
0.9
0.95
1
0 2 4 6 8
Time
0.2
0.4
0.6
0 5 10
Time
0.2
0.4
0.6
1 2 3
Time
0.7
0.72
0.74
0.76
0.78
0 1 2 3
Time
0.5
0.6
0.7
0.8
0 1 2
Time
0.55
0.6
0.65
0.7
0 2 4 6 8
Time
0.7
0.75
0.8
0.85
0 5 10
Time
0.4
0.5
0.6
0.7
0 5 10
Time
0.4
0.5
0.6
0.7
0 2 4 6 8
Time
0.7
0.8
0.9
2 4 6 8 10
Time
0.2
0.4
0.6
0 2 4 6 8
Time
0.3
0.4
0.5
0.6
0.7
0 5 10
Time
0.8
0.85
0.9
0.95
1
0 5 10
Time
0
0.2
0.4
0.6
0.8
2 4 6 8 10
Time
0.4
0.6
0.8
2 4 6 8
Time
0.92
0.94
0.96
0 5 10
Time
0.2
0.4
0.6
0.8
0 2 4 6 8
Time
0.5
0.6
0.7
0.8
0 2 4 6
Time
0.8
0.85
0.9
0.95
0 5
Time
0
0.5
1
1.5
The Comparsion of All Methods on Fused Logistic Regression
0 2 4
Time
0.4
0.6
0.8
1
1.2
EGADM SGADM STOC-ADMM RDA-ADMM OPG-ADMM Ada-SADMMdiag Ada-SADMMfull SPDPEG
splice mushrooms svmguide3 a9a w8a k1b classic hitech
O
b
je
c
ti
v
e
 V
a
lu
e
T
e
s
t 
L
o
s
s
P
re
d
ic
ti
o
n
 A
c
c
u
ra
c
y
Figure 1: Comparison of SPDPEG with SGADM, SADMM, RDA-ADMM, OPG-ADMM, SADM-
Mdiag and SADMMfull on Fused Logistic Regression Task. First Row: Average objective
values. Second Row: Average test losses. Third Row: Average prediction accuracies.
tive value, test loss and prediction accuracy to compare our method with other
methods. The “objective value” means the sum of the loss function and regu-
larized terms evaluated on a training data sample, while the “test loss” means
the value of the loss function evaluated on a test data sample. Specifically, we
use objective function values on training datasets, test losses (i.e., l(x)) on test
datasets, and prediction accuracy on test datasets.
Figure 1 shows the objective value, test loss and prediction accuracy as
the functions of the time costs on the FLR task, where the objective function
is convex but not necessarily strongly convex. We observe that our method
mostly achieves the best performance, followed by six stochastic ADMM-type
algorithms, all of which outperform EGADM by a large margin. We find that
the prediction accuracy of the SPDPEG algorithm is competitive with other
algorithms, which supports the use of extra-gradient in the SPDPEG algorithm.
The performance of our SPDPEG algorithm on six datasets is the most stable
and effective among all methods.
22
0 1 2
Time
0.5
0.6
0.7
0.8
0 1 2
Time
0.4
0.5
0.6
0.7
0 1 2
Time
0.4
0.5
0.6
0.7
0 2 4 6 8
Time
0.88
0.9
0.92
0.94
0.96
0.98
0 5 10
Time
0.2
0.4
0.6
0 2 4 6 8
Time
0.2
0.4
0.6
1 2 3
Time
0.55
0.6
0.65
0 1 2 3
Time
0.64
0.66
0.68
0 1 2
Time
0.66
0.67
0.68
0.69
0 10 20
Time
0.75
0.8
0.85
0 10 20
Time
0.3
0.4
0.5
0.6
0.7
0 5 10 15
Time
0.4
0.5
0.6
0 20 40 60 80
Time
0.74
0.76
0.78
0.8
0.82
0 20 40 60 80
Time
0.4
0.5
0.6
0.7
0 50 100
Time
0.5
0.55
0.6
0.65
0 50 100
Time
0.8
0.85
0.9
0.95
1
0 50 100
Time
0
0.2
0.4
0.6
0.8
0 50 100
Time
0
0.2
0.4
0.6
0.8
0 20 40 60 80
0.95
0.96
0.97
0.98
0 20 40 60 80
Time
0.2
0.4
0.6
0 50 100
Time
0.3
0.4
0.5
0.6
0.7
0 20 40 60 80
0.85
0.9
0.95
0 20 40 60 80
Time
0.2
0.4
0.6
The Comparsion of All Methods on Graph-Guided Regularized Logistic Regression
0 20 40 60 80
Time
0.2
0.4
0.6
EGADM SGADM STOC-ADMM RDA-ADMM OPG-ADMM Ada-SADMMdiag Ada-SADMMfull
Time 
PEGSADM-SC1
Time
PEGSADM-SC2
splice mushrooms svmguide3 a9a w8a k1b classic hitech
O
b
je
c
ti
v
e
 V
a
lu
e
T
e
s
t 
L
o
s
s
P
re
d
ic
ti
o
n
 A
c
c
u
ra
c
y
Figure 2: Comparison of SPDPEG-SC1 (Uniformly Averaged) and SPDPEG-SC2 (Non-
Uniformly Averaged) with SGADM, SADMM, RDA-ADMM, OPG-ADMM, SADMMdiag and
SADMMfull on Graph-Guided Regularized Logistic Regression Task. First Row: Av-
erage objective values. Second Row: Average test losses. Third Row: Average prediction
accuracies.
We further compare our algorithm with other algorithms on the GGRLR
task, where the objective function is strongly convex. We use both uniformly
and non-uniformly averaged iterates, noted as SPDPEG-SC1 (Uniformly Aver-
aged) and SPDPEG-SC2 (Non-Uniformly Averaged). The experimental results
presented in Figure 2 show that our algorithm consistently outperforms other al-
gorithms, and exhibits the advantage with non-uniformly averaged iterates over
its counterpart with uniformly averaged iterates. This matches our analysis in
the previous sections.
6. Conclusions
In this paper, we proposed a novel algorithm, namely Stochastic Primal-Dual
Proximal ExtraGradient (SPDPEG), to resolve stochastic minimization problems
including two regularization terms, one of which is composed with a linear func-
tion F (x), as shown in problem (1). Problem (1) is computationally difficult
23
when the penalty matrix F is non-diagonal or the number of training samples
is large.
Inspired by the nice efficiency of EGADM, we developed an ADM-type opti-
mization scheme that employs proximal noisy extra-gradient descent to achieve
reasonable numerical efficiency and stability. For general convex objectives, we
showed that the uniformly average iterates converge in expectation with the
rate of O(1/
?
t); while for strongly convex objectives, the uniformly and non-
uniformly average iterates generated by the SPDPEG algorithm were proven
to converge in expectation with the O(log(t)/t) and O(1/t) rates, respectively.
It is worth mentioning that these rates are both known to be best possible
for first-order stochastic optimization algorithms. The numerical experiments
conducted on fused logistic regression and graph-guided regularized logistic re-
gression problems demonstrated that our proposed algorithm consistently out-
performs the other competing stochastic algorithms. A future research direction
is to consider incorporating variance reduction techniques into the SPDPEG al-
gorithm.
References
References
[1] R. Tibshirani, Regression shrinkage and selection via the lasso, Journal of
the Royal Statistical Society. Series B (Methodological) (1996) 267–288.
[2] C. Cortes, V. Vapnik, Support vector networks, Machine learning 20 (3)
(1995) 273–297.
[3] R. Tibshirani, M. Saunders, S. Rosset, J. Zhu, K. Knight, Sparsity and
smoothness via the fused lasso, Journal of the Royal Statistical Society:
Series B (Statistical Methodology) 67 (1) (2005) 91–108.
[4] J. Friedman, T. Hastie, R. Tibshirani, The elements of statistical learning:
Data mining, inference, and prediction, Springer Series in Statistics.
24
[5] N. Parikh, S. Boyd, Proximal algorithms, Foundations and Trends R© in
Optimization 1 (3) (2014) 127–239.
[6] S. Boyd, N. Parikh, E. Chu, B. Peleato, J. Eckstein, Distributed opti-
mization and statistical learning via the alternating direction method of
multipliers, Foundations and Trends R© in Machine Learning 3 (1) (2011)
1–122.
[7] J. Yang, X. Yuan, Linearized augmented lagrangian and alternating direc-
tion methods for nuclear norm minimization, Mathematics of Computation
82 (281) (2013) 301–329.
[8] T. Lin, S. Ma, S. Zhang, An extragradient-based alternating direction
method for convex minimization, Foundations of Computational Mathe-
matics (2015) 1–25.
[9] G. Korpelevich, The extragradient method for finding saddle points and
other problems, Ekonomika i Matematicheskie Metody 12 (1976) 747–756.
[10] G. Korpelevich, Extrapolation gradient methods and relation to modified
lagrangeans, Ekonomika i Matematicheskie Metody 19 (1983) 694–703.
[11] H. Ouyang, N. He, L. Tran, A. Gray, Stochastic alternating direction
method of multipliers, in: ICML, 2013, pp. 80–88.
[12] T. Suzuki, Dual averaging and proximal gradient descent for online alter-
nating direction multiplier method, in: ICML, 2013, pp. 392–400.
[13] S. Azadi, S. Sra, Towards an optimal stochastic alternating direction
method of multipliers, in: ICML, 2014, pp. 620–628.
[14] P. Zhao, J. Yang, T. Zhang, P. Li, Adaptive stochastic alternating direction
method of multipliers, in: ICML, 2015, pp. 69–77.
[15] S. Zheng, J. T. Kwok, Stochastic variance-reduced admm, ArXiv Preprint:
1604.07070.
25
[16] J. Duchi, Y. Singer, Efficient online and batch learning using forward back-
ward splitting, Journal of Machine Learning Research 10 (2009) 2899–2934.
[17] A. Agarwal, P. L. Bartlett, P. Ravikumar, M. J. Wainwright, Information-
theoretic lower bounds on the oracle complexity of stochastic convex opti-
mization, IEEE Transactions on Information Theory 58 (5) (2012) 32–35.
[18] X. Gao, B. Jiang, S. Zhang, On the information-adaptive variants of the
admm: an iteration complexity perspective, Optimization Online.
[19] W. Zhong, J. Kwok, Fast stochastic alternating direction method of mul-
tipliers, in: ICML, 2014, pp. 46–54.
[20] T. Suzuki, Stochastic dual coordinate ascent with alternating direction
method of multipliers, in: ICML, 2014, pp. 736–744.
[21] L. Qiao, T. Lin, Y.-G. Jiang, F. Yang, W. Liu, X. Lu, On stochastic primal-
dual hybrid gradient approach for compositely regularized minimization, in:
ECAI, Vol. 285, IOS Press, 2016, pp. 167–174.
[22] M. A. Noor, New extragradient-type methods for general variational in-
equalities, Journal of Mathematical Analysis and Applications 277 (2)
(2003) 379–394.
[23] A. Nemirovski, Prox-method with rate of convergence o(1/t) for variational
inequalities with lipschitz continuous monotone operators and smooth
convex-concave saddle point problems, SIAM Journal on Optimization
15 (1) (2004) 229–251.
[24] M. V. Solodov, B. F. Svaiter, A hybrid approximate extragradient-proximal
point algorithm using the enlargement of a maximal monotone operator,
Set-Valued Analysis 7 (4) (1999) 323–345.
[25] R. D. C. Monteiro, B. F. Svaiter, On the complexity of the hybrid proximal
extragradient method for the iterates and the ergodic mean, SIAM Journal
on Optimization 20 (6) (2010) 2755–2787.
26
[26] R. D. C. Monteiro, B. F. Svaiter, Complexity of variants of tseng’s modi-
fied fb splitting and korpelevich’s methods for hemi-variational inequalities
with applications to saddle-point and convex optimization problems, SIAM
Journal on Optimization 21 (4) (2011) 1688–1720.
[27] R. D. C. Monteiro, B. F. Svaiter, Iteration-complexity of block-
decomposition algorithms and the alternating direction method of mul-
tipliers, SIAM Journal on Optimization 23 (1) (2013) 475–507.
[28] S. Bonettini, V. Ruggiero, An alternating extragradient method for to-
tal variation-based image restoration from poisson data, Inverse Problems
27 (9) (2011) 095001.
[29] K. Scheinberg, S. Ma, D. Goldfarb, Sparse inverse covariance selection via
alternating linearization methods, in: NIPS, 2010, pp. 2101–2109.
[30] H. Wang, A. Banerjee, Online alternating direction method, in: ICML,
2012, pp. 1119–1126.
[31] R. Johnson, T. Zhang, Accelerating stochastic gradient descent using pre-
dictive variance reduction, in: NIPS, 2013, pp. 315–323.
27
