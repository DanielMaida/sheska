ProbFlow: Joint Optical Flow and Uncertainty Estimation
Anne S. Wannenwetsch1 Margret Keuper2 Stefan Roth1
1TU Darmstadt 2University of Mannheim
Abstract
Optical flow estimation remains challenging due to un-
textured areas, motion boundaries, occlusions, and more.
Thus, the estimated flow is not equally reliable across the
image. To that end, post-hoc confidence measures have
been introduced to assess the per-pixel reliability of the
flow. We overcome the artificial separation of optical flow
and confidence estimation by introducing a method that
jointly predicts optical flow and its underlying uncertainty.
Starting from common energy-based formulations, we rely
on the corresponding posterior distribution of the flow given
the images. We derive a variational inference scheme based
on mean field, which incorporates best practices from en-
ergy minimization. An uncertainty measure is obtained
along the flow at every pixel as the (marginal) entropy of
the variational distribution. We demonstrate the flexibility
of our probabilistic approach by applying it to two different
energies and on two benchmarks. We not only obtain flow
results that are competitive with the underlying energy min-
imization approach, but also a reliable uncertainty measure
that significantly outperforms existing post-hoc approaches.
1. Introduction
Optical flow estimation has been extensively studied for
more than three decades [5, 9, 14, 21, 36]. However, motion
boundaries, large displacements, and occlusions still lead
to erroneous flow fields, especially in challenging imaging
conditions. In contrast, flow predictions are almost error-
less in textured regions of uniform motion, which causes the
reliability of optical flow predictions to vary greatly across
the image. Uncertainty measures1 aim to predict the relia-
bility of the flow estimates and rate each estimate according
to its predicted accuracy [24, 27, 30]. One application of
such measures is to improve optical flow estimation using
different ways of post-processing [4, 36]. Moreover, un-
certainty measures represent an important tool when optical
flow is used as a cue for other computer vision tasks, such
1 Uncertainty measures are closely related to confidence measures as
their values are inversely related to confidence estimates.
(a) First frame of image pair (b) Ground truth optical flow
(c) Uncertainty map from our method (d) Flow estimate from our method
Figure 1. Our uncertainty measure accurately detects regions with
reliable flow predictions (dark blue) as well as parts of the image
with erroneous estimates (dark red). Best viewed on screen.
as image segmentation or tracking, which consequently de-
pend on the correctness of the flow. In such cases, error
propagation can be avoided if only flow estimates of high
confidence are taken into consideration, e.g. [33, 47].
Several post-hoc confidence measures have been intro-
duced to directly estimate the uncertainty of flow predic-
tions, e.g. [3, 20, 24, 30]. However, optical flow estima-
tion is known to be rather challenging as a one-shot process
[14, 15, 43]. We thus believe it is natural to ask why confi-
dence estimation should be restricted to non-iterative (post-
hoc) approaches. Moreover, flow algorithms for a concrete
problem are carefully selected for the task at hand and pro-
vide us with an underlying model. We, thus, argue that it is
very desirable to apply a model-inherent uncertainty mea-
sure tailored to the chosen method. We even go one step
further and aim to jointly predict optical flow and a corre-
sponding uncertainty in order to preserve and extract impor-
tant information contained in the flow estimation process.
Our work focuses on energy-based methods [3, 15, 43]
as they represent a flexible framework in widespread use.
Starting from a general energy formulation, we derive a
probability distribution of the flow field conditioned on the
input images. In this setting, optical flow can be determined
by minimizing the expected loss under the modeled poste-
rior and the corresponding uncertainty measure is naturally
obtained as the marginal entropy of the posterior at every
pixel. Due to the intractability of exact posterior inference
in general, we rely on variational approximate inference and
To appear in Proceedings of the IEEE International Conference on Computer Vision (ICCV), Venice, Italy, 2017.
c© 2017 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including
reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or
reuse of any copyrighted component of this work in other works.
ar
X
iv
:1
70
8.
06
50
9v
1 
 [
cs
.C
V
] 
 2
2 
A
ug
 2
01
7
use a mean-field approach. In this way, flow predictions and
uncertainty estimations can be obtained as the result of a
joint minimization problem that makes an additional train-
ing step of the uncertainty measure unnecessary [17, 30].
Fig. 1 shows an exemplary flow field and uncertainty pre-
diction of our probabilistic approach obtained on the Sintel
benchmark [11]. We term our approach ProbFlow.
To the best of our knowledge, we introduce the first fully
probabilistic flow estimation approach that shows competi-
tive results on established benchmarks. Moreover, our work
is unique in that it is broadly applicable to a large number
of existing energy-based optical flow algorithms and that
we demonstrate how to benefit from the probabilistic frame-
work in the context of uncertainty measures.
We apply ProbFlow to a classic Horn-Schunck-style en-
ergy as proposed in [43] and to the more recent EpicFlow
formulation [36]. We show that the obtained flow fields are
competitive with or even outperform the corresponding en-
ergy minimization results on the Middlebury [2] and Sintel
[11] datasets. To assess the performance of the uncertain-
ties estimated by our method, we rely on existing evaluation
approaches and propose a new criterion based on the Spear-
man’s rank correlation coefficient. Our uncertainty mea-
sure is clearly superior in comparison to various competing
approaches and significantly outperforms the best existing
methods. We further show the benefits of our uncertainty
estimates in an application to motion segmentation [22] by
generating highly reliable point trajectories.
2. Related Work
Starting from the work of Horn and Schunck [21], global
energy minimization approaches have been used exten-
sively for optical flow estimation; see [3, 15, 43] for in-
depth overviews of existing methods and best practices.
Despite their conceptual simplicity, global energy formula-
tions are still up-to-date [13, 29]. Moreover, energy-based
approaches are frequently used for post-processing optical
flow predictions obtained through other means [1, 14, 36].
In the past, a variety of optical flow uncertainty measures
have been proposed. Various simple methods are based on
the characteristics of the input data only, e.g. [3, 20]. We
refer to [24, 27] for a more comprehensive summary of such
confidence approaches. In comparison to our work, all of
these measures omit important sources of information by
not considering the estimated optical flow field itself.
A second class of confidence measures relies on an anal-
ysis of the estimated flow fields. Kondermann et al. [23]
learn a linear subspace of true spatio-temporal flow neigh-
borhoods and use the reconstruction error of an estimated
flow vector to evaluate its reliability. In [24], a probabilis-
tic model of flow patches is learned and approximated with
a Gaussian distribution, which yields a confidence measure
based on hypothesis testing. But this second class of confi-
dence measures does not consider all aspects of flow uncer-
tainty either, as the input images are not taken into account.
Recently, Mac Aodha et al. [30] proposed a confidence
measure based on the input as well as the output of an op-
tical flow algorithm. Using a multi-cue feature vector, a
classifier is trained to predict whether the endpoint error at
a certain pixel is smaller than a previously defined thresh-
old. The probability output of the classifier is used as a con-
fidence measure. However, [30] does not take advantage
of uncertainty information available in the flow estimation
procedure itself and requires a separate training step that is
unnecessary for our model-inherent approach.
Uncertainty measures tailored to energy minimization
approaches have been proposed especially for local energy
methods, e.g., [3, 31, 40, 41]. Model-inherent confidence
measures designed for global energy formulations are quite
rare, on the other hand. Kybic and Nieuwenhuis [27] intro-
duce a method that relies on bootstrap resampling. Based
on varying pixel contributions, repeated optical flow estima-
tion is performed and a confidence measure is determined as
the total standard deviation of the obtained flow predictions.
Bruhn and Weickert [8] propose the inverse local energy as
a confidence measure, which is applicable to a broad vari-
ety of energies. A large uncertainty is thus associated with
a strong violation of the local model assumptions encoded
in the energy formulation. Gehrig and Scharwa?chter [17]
use the energy and combine it with several features such as
the spatial and temporal flow variance in order to obtain a
real-time confidence estimate. In contrast to the two above
approaches, our method does not explicitly limit the recep-
tive field of the confidence measure by only considering the
local neighborhood of a pixel. Instead, uncertainty propa-
gation is facilitated by means of iterative spatial inference.
In the past, several methods based on a probabilistic flow
formulation have been proposed [35, 39, 40, 41]. However,
many of these works are based on simplifying assumptions
such as locally constant flow or a Gaussian distribution of
brightness constancy errors. In [19, 26, 44], probabilistic
approaches are applied to obtain improved models of opti-
cal flow. In contrast to our work, these methods do not apply
a fully probabilistic approach, but fall back to a maximum a-
posteriori (MAP) estimate, i.e. minimize the underlying en-
ergy. Glocker et al. [18] use flow uncertainties in a dynamic
Markov random field but rely on a discrete approach. The
most closely related work of Chantas et al. [12] applies a
variational-Bayes approach similar to ours. However, their
method is not designed as a stand-alone flow algorithm, but
only as an improved initialization in comparison to a simple
Horn-Schunck approach. Therefore, the obtained results are
not competitive with respect to the state of the art. More-
over, the paper does not take advantage of the probabilistic
approach in the context of uncertainty measures.
3. Energy Framework
Since our joint, model-inherent uncertainty estimation
developed below is broadly applicable to different kinds of
energy-based optical flow approaches [3, 15, 43], we first
introduce a formalization that allows us to describe previ-
ously proposed optical flow methods in a unified manner.
In the following, we estimate optical flow between an
image pair I = {I1, I2} and denote the estimate as
y = (yij)ij =
(
uij , vij
)T
ij
for pixels (i, j), i = 1, . . . , n,
j = 1, . . . ,m. Energy-based approaches estimate the op-
tical flow as the minimizer y? of an energy function
E (y; I) = ED (y; I) + ?SES (y) (1)
with ED (y; I) denoting a data term that encourages the
flow to be consistent with input images I1 and I2. The spa-
tial term ES (y) imposes a (smoothness) prior on the flow,
and ?S represents a trade-off parameter between the terms.
In the following, we use functions fD(·) and fS(·) to for-
malize violations of the assumptions underlying the chosen
optical flow model. For instance, the intensity difference
of corresponding pixels in I1 and I2 may be evaluated to
model a brightness constancy assumption for the data term.
So-called penalty functions ?D(·) and ?S(·) penalize viola-
tions of the assumptions modeled in fD(·) and fS(·). The
energy terms ED and ES can then be described as a sum of
the contributions from all pixels such that
ED (y; I) =
?
i,j
?D
(
fD
(
yij ; I
))
(2)
ES (y) =
?
i,j
?
(i?,j?)?S(i,j)
?S
(
fS
(
yij ,yi?j?
))
(3)
with S(i, j) describing a set of neighbors of pixel (i, j).
It has been shown that the estimated flow field y? can be
improved by adding a non-local term to the energy function
in Eq. (1) [25, 43]. Thus, we optionally use an additional
non-local term EN (y) akin to ES (y), considering an ex-
tended neighborhood N(i, j). Again, EN (y) is described
by a model assumption fN(·) and its corresponding penalty
function ?N(·).
Common choices for penalty functions ?(·) are, e.g., a
quadratic function [21], a Lorentzian function [5], or a (gen-
eralized) Charbonnier function [9, 43]. Many of these func-
tions can be described by the negative logarithm of a Gaus-
sian Scale Mixture (GSM) [44, 46]. Thus, in the following
we rely on this simple but powerful class of functions and
represent the penalty terms as GSMs of L components
? (z) = ? log
[
L?
l=1
?l N
(
z; 0, ?2l
)
]
(4)
with N (z;µ, ?) being a normal distribution with mean
µ = 0 and variance ? = ?l; the weights ?l ? 0 sum to 1. As
we will see, GSMs also benefit our probabilistic approach.
4. Probabilistic Interpretation and Inference
We now aim to approach optical flow estimation in a
probabilistic manner. As the energy function E (y; I) from
Eq. (1) describes a Markov random field, it is easy to derive
the corresponding posterior distribution in its Gibbs form as
p (y | I) = 1
Z
exp
{
? 1
T
E(y; I)
}
(5)
with partition function Z ? Z(I, T, ?S, ?N) and tempera-
ture T . In the following, w.l.o.g. we set T = 1 and intro-
duce an additional parameter ?D scaling the data term ED.
To ease probabilistic inference, we use the same proce-
dure as [43] and introduce an auxiliary flow field y? that
allows us to decouple the non-local potential from the re-
maining terms of the posterior distribution in Eq. (5), i.e.
p (y, y? | I) = 1
Z
exp
{
? ?DED(y; I)? ?SES(y) (6)
? ?CEC (y, y?)? ?NEN (y?)
}
with EC (y, y?) =
?
i,j ?yi,j ? y?i,j?22.
The log-posterior of Eq. (6) now has terms in the form of
the logarithm of a sum of exponentials, which is challenging
when deriving closed-form mean-field updates below. Here,
we benefit from our choice to model each penalty function
?(·) as a GSM. We follow [16, 28] to retain explicit latent
variables h = (hD,hS,hN), which are chosen to follow a
discrete distribution and to have a 1-of-L representation. We
then obtain an augmented penalty function ?? (z,h?) with
? ? {D,S,N} as
?? (z,h?) = ? log
[
L?
l=1
?
h?,l
l N
(
z; 0, ?2l
)h?,l
]
. (7)
At this point, the posterior p(y, y?,h | I) represents the
probabilistic equivalent of the energy E (y; I) in Eq. (1).
In our probabilistic setup, the flow estimate y? is chosen
to minimize the expected loss over p(y, y?,h | I), i.e.
y? = arg min
y?
Ep(y,y?,h | I) [l(y, y?)] (8)
with l (·, ·) being a suitable loss function such as the Aver-
age End-Point Error (AEPE) [34]. The desired uncertainty
measure can be obtained by considering the marginal distri-
bution p(yij |I) of the flow estimates. In particular, we pro-
pose to use the marginal entropy at every pixel as a model-
inherent uncertainty estimate of the flow prediction.
Inference. To obtain a flow estimate and the underlying
marginal distributions at every pixel, we rely on an approx-
imate inference scheme. We use variational inference [45]2
2Not to be confused with variational formulations common in flow.
and approximate p(y, y?,h | I) with the help of a tractable
distribution q(y, y?,h;?) and variational parameters ?.
Following a naive mean-field assumption, we choose the
parametric distribution q to be factorized over y, y?, as well
as h. The marginal distribution of flow vectors yij and y?ij
is assumed to be Gaussian, e.g.
q (yij ;?) ? N
(
yij ;µij ,?ij
)
. (9)
It is reasonable to assume the horizontal and vertical flow
components to be uncorrelated [37]. Thus, the covariances
?ij are modeled as diagonal matrices. As in [16, 28], the
distributions of the latent variables are chosen to be multi-
nomial
q(h?,ij ;?) =
L?
l=1
k
h?,ij,l
?,ij,l (10)
so that the parameters k?,ij,l ? 0 satisfy
?
l k?,ij,l = 1.
The approximating distribution q is then given as
q(y, y?,h;?) =
?
i,j
[
q (yij ;?) · q (y?ij ;?)
·
?
??{D,S,N}
q (h?,ij ;?)
]
(11)
with ? =
{
µij , µ?ij ,?ij , ??ij ,k?,ij
}
ij,?
.
Suitable variational parameters ?? of q are determined
such that the Kullback-Leibler (KL) divergence between p
and its approximating distribution q is minimized, i.e.
?? = arg min
?
DKL
(
q(y, y?,h;?) | p(y, y?,h | I)
)
(12)
= arg min
?
Eq(y,y?,h;?)
[
log q(y, y?,h;?)
]
? Eq(y,y?,h;?)
[
log p(y, y?,h | I)
]
. (13)
Due to the usage of explicit latent variables h, it is possible
to compute the expectations in Eq. (13) in an analytic way.
While the derivation of the corresponding equations is te-
dious, the individual steps are elementary; see supplemental
material for a more detailed explanation of the procedure.
We now estimate the flow by replacing the posterior p
in Eq. (8) with its approximating distribution q. When per-
forming Bayesian risk minimization of the AEPE, the opti-
cal flow prediction is obtained as y?ij = µij and, therefore,
corresponds to the mode of the variational distribution q.
As q(y, y?,h;?) was defined in a factorized way, the
corresponding marginal distribution at pixel (i, j) is given
as q (yij ;?) and our proposed model-inherent uncertainty
measure can be obtained as the marginal entropy
?ProbFlow = H(yij) = log
(
det (?ij)
)
+ const. (14)
5. Specific Models
We now apply our ProbFlow approach to two specific en-
ergy functions commonly used for optical flow estimation.
5.1. Probabilistic Classic Flow
We first consider a classical Horn-Schunck-based objec-
tive based on brightness constancy as given in [43]. Fol-
lowing the common approach to use a first-order Taylor ap-
proximation for a linearization of the data term, we obtain
fD (yij ; I) = I2
(
i+ u0ij , j + v
0
ij
)
? I1 (i, j)
+?2I2
(
i+ u0ij , j + v
0
ij
)T
(yij ? y0ij), (15)
where y0ij is the point of approximation and ?2I2 denotes
the spatial derivatives of I2. The smoothness prior in [43]
assumes small flow gradients over a 4-neighborhood of hor-
izontal and vertical flow components. The non-local term
encourages smoothness over a neighborhood of size 5 × 5.
In both cases, the smoothness assumption is represented as
fS (xij , xi?j?) = fN (xij , xi?j?) = xij ? xi?j? . (16)
Akin to energy minimization approaches [43], we found
the use of the non-local term crucial for obtaining accu-
rate optical flow estimates, since mean-field inference using
only the terms ED and ES is rather outlier-prone.
5.2. Probabilistic FlowFields
In a second setup, we aim for a probabilistic version of
FlowFields [1]. We follow Bailer et al. and consider the
energy used in the post-processing step of EpicFlow [36],
which uses a data term based on gradient constancy. As pro-
posed in [49], the image gradient terms are normalized w.r.t.
the spatial derivatives, which is helpful to avoid outliers in
the flow field. We obtain the linearized data assumption as
fD (yij ; I) = (17)?????
3?
r=1
?rij ?
[
?2Ir2
(
i+ u0ij , j + v
0
ij
)
??2Ir1 (i, j)
+ H
(
Ir2
(
i+ u0ij , j + v
0
ij
) )
(yij ? y0ij)
]?????
2
.
Here, r = 1, . . . , 3 indicates the RGB color channels,
H (Ir2 ) denotes the Hessian of I
r
2 , and ? is the Hadamard
product. The normalization coefficient ?rij is given as
?rij =
(
?rij,1
?rij,2
)
, ?rij,k =
1?
?H(Ir2 ) · ek?22 + ?2
(18)
with 2D unit vectors ek and a small constant ? > 0 [49].
The smoothness term of [36] is based on the flow gradi-
ent norm and uses additional filters of size 2×3 and 3×2 as
described in [38] when calculating the flow derivatives. To
keep the inference problem consistent, our approach differs
slightly from [36] in that we only use the forward gradient
filter described in Eq. (16) and obtain
fS (yij ,yi?j?) =
?
(uij ? ui?j?)2 + (vij ? vi?j?)2 (19)
for yi?j? in a 4-neighborhood of yij .
Following [36], a locally adaptive trade-off parameter
?S (x) = exp (????2I1 (x) ?) is used. Similar to MAP es-
timation, the minimization of the KL divergence in Eq. (13)
obtained from fD, fS, and ?S (x) performs well in practice.
Therefore, an additional non-local term can be neglected.
6. Implementation
Optical flow estimation by energy minimization is
known to be far from easy. Similarly, an application of
mean-field inference is non-trivial in this context. More-
over, it is essential to consider several details commonly
used with energy-based approaches to obtain satisfying re-
sults [15, 43]. A summary of basic design choices as well
as an extensive analysis of the influence of all described
specifics can be found in the supplemental material.
Common per-pixel updates, e.g. [48], do not work well
for the mean-field inference of Eq. (13). Instead, we keep
the corresponding optimization procedure as efficient as
possible and use a block-coordinate descent scheme updat-
ing flow estimates µ, variances ?, and latent variables k
in an alternating manner. We derive the gradient of the KL
divergence in Eq. (13), set it to zero, and obtain an update
equation for each set of variables (see supplemental). As
with MAP, we found a joint update of the flow predictions
at all pixels to be crucial to obtaining smooth flow fields.
Parameters. To determine suitable trade-off parameters
?, it is not sufficient to follow the common approach and
choose parameters that lead to the smallest AEPE on a train-
ing set [15, 43]. Instead, we evaluate the quality of both
the obtained flow predictions and the uncertainty measure
in order to ensure accurate flow estimates and meaningful
entropies. To assess the quality of our uncertainty measure,
we follow the approaches in [8, 24, 27, 30] and compute
so-called sparsification plots. To that end, the pixels of a
flow field are sorted according to the estimated uncertain-
ties. Subsequently, an increasing percentage of the pixels
is removed and the AEPE of the remaining pixels is calcu-
lated. In order to evaluate how well different uncertainty
measures perform on an entire dataset, we propose to nor-
malize the graphs, calculate the area under curve (AUC),
and average over the sequences. To consider the trade-off
between flow accuracy and quality of the uncertainty mea-
sure, we evaluate an F1-score
F1 =
AEPE · c AUC
AEPE + c AUC
(20)
over the training data with constant c weighting the influ-
ence of the two metrics. We then determine parameters us-
ing Bayesian optimization [42] of Eq. (20).
Concerning the penalty functions, we follow the ap-
proach in [44] and learn appropriate GSM models for data
likelihood, smoothness prior, as well as the non-local term
from the respective training datasets or a randomly chosen
subset thereof. Using manually determined variances ?l, a
simple expectation maximization algorithm is used to ob-
tain the corresponding weights ?l. We observe that GSMs
with L = 10 components perform well in practice. To save
computational time, we resort to L = 5 components for our
probabilistic implementation of Classic Flow.
Details. For Probabilistic Classic Flow, we follow the un-
derlying energy approach (ClassicA, [43]) and use zero flow
as an initialization; we denote the method as ProbClassicA.
Variances are initialized as ?init = 1e?7, latent variables as
kinit = 1/L. During the inference process, the parameter ?C
is annealed as in [43]. The parameters ?D, ?S, and ?N are
obtained with Bayesian optimization of the F1-score (Eq.
20) using c = 5. As suggested by Sun et al. in the context
of MAP, we use the variables µ? as the flow prediction. The
corresponding uncertainties are then obtained from ??.
For Probabilistic FlowFields, we use the state-of-the-art
FlowFields method [1] to generate sparse matches. Fol-
lowing Bailer et al., we apply the EpicFlow [36] post-
processing step with Sintel parameters to interpolate the
matches and use the results to initialize our algorithm.
This method will be denoted as ProbFlowFields. Vari-
ances and latent variables are initialized as for ProbClas-
sicA. Trade-off parameters ?D, ?S, and ? are obtained again
with Bayesian optimization [42]. The parameter of the F1-
score (Eq. 20) is chosen as c = 100 since the AEPE is sig-
nificantly higher on Sintel. We will make code available for
ProbClassicA as well as for ProbFlowFields.
7. Experiments & Results
In the following, we evaluate our probabilistic flow ap-
proach and assess the quality of our uncertainty measure by
comparing it to different approaches from the literature.
7.1. Competing uncertainty measures
We apply existing uncertainty measures on top of the
corresponding energy minimization approaches (i.e. Clas-
sicA or FlowFields) in order to analyze the benefit of our
combined flow prediction and uncertainty estimation. We
limit ourselves to a few select methods here and give a more
extensive comparison in the supplemental material.
Barron et al. [3] suggest a confidence measure based on
the spatial gradient of the input image. The corresponding
uncertainty is obtained as ?Gradient = ???2I1? using cen-
tral differences to approximate the gradient.
The learned confidence measure in [30] is based on a
classifier that predicts with probability p? whether the error
of the flow estimate at a certain pixel is smaller than a speci-
fied threshold T . We use the implementation of MacAodha
et al. and train ?Learned = ?p? as described in [30].
training test
Method AEPE rel. chg. AEPE rel. chg.
Classic++ [43] 0.285 -0.04 0.406 -0.07
ClassicA [43] 0.295 >-0.01 – –
ProbClassicA (ours) 0.296 0.00 0.435 0.00
Table 1. Average end-point error (AEPE) and its relative change
(rel. chg.) in comparison to ProbClassicA on Middlebury.
As proposed by Bruhn and Weickert [8], we use the local
energy contribution of the MAP estimate yMAP as a model-
inherent uncertainty ?Energy = E(yMAP; I). We apply the
non-linear version of the underlying energy function as it
shows an improved performance [10]. A data penalty for
out-of-boundary pixels is determined on the training set.
For an additional baseline, we perform a Laplace approx-
imation of the posterior p(y, y? | I) around yMAP. With H
denoting the Hessian of the linearized energy E(yMAP; I),
the covariance is given as ?L = H?1. Similar to our ap-
proach, we obtain ?Laplace = ? log
(
det (H)
)
+ const.
Finally, we consider an oracle uncertainty measure
?Oracle, for which the uncertainty of a pixel is given by its
end-point error. Thus, the estimate ?Oracle provides a bound
for the best possible uncertainty estimation. Note that we
consider oracle uncertainties from the predictions obtained
by MAP estimation; the oracle uncertainties for our flow
predictions show a very similar behavior.
7.2. ProbClassicA
We first evaluate the application of ProbFlow to the Clas-
sicA model described in Sec. 5.1. We rely on the Middle-
bury dataset [2], which has frequently been used to evaluate
the performance of optical flow confidence measures. In
order to compare different uncertainty approaches, ground
truth optical flow is needed. As the Middlebury training set
includes only 8 image pairs, we need to resort to training
and testing the uncertainty measures on the same data.
To compare the performance of different flow estima-
tion algorithms, we rely on the commonly used AEPE. Ta-
ble 1 gives results on Middlebury training and test. Our
method (ProbClassicA) performs on par with ClassicA. On
the one hand, this is to be expected as both approaches share
the same energy formulation and energy-based methods use
highly elaborate schemes. On the other hand, this is the first
time that a fully probabilistic method achieves competitive
results on a public benchmark. For completeness, we also
include the related Classic++, which shows slightly better
results as its median filtering step allows for a more effective
outlier suppression than an additional nonlocal term [25].
The median filter cannot easily be applied to our case, but
the results for ClassicA and ProbClassicA could be further
improved by adding weights to the nonlocal term [43].
The evaluation of uncertainty measures is more chal-
Uncertainty measure AUC rel. chg. CC rel. chg.
Gradient [3] 0.971 1.08 0.023 0.94
Laplace 0.656 0.41 0.160 0.57
Energy [8] 0.498 0.07 0.303 0.19
Learned [30] 0.496 0.06 0.324 0.13
ProbClassicA (ours) 0.466 0.00 0.374 0.00
Oracle 0.255 – 1.000 –
Table 2. Area under curve (AUC), Spearman’s rank correlation
coefficient (CC), and relative change (rel. chg.) in comparison to
our uncertainty measure on the Middlebury dataset.
Figure 2. Sparsification plots averaged over Middlebury training.
lenging. Sparsification plots are commonly considered
[8, 24, 27, 30]. To be able to compare the performance
of different uncertainty measures over an entire dataset, we
calculate the AUC as described in Sec. 6. However, as ar-
gued by Ma?rquez-Valle et al. [31], sparsification plots do
not allow to estimate how strongly an uncertainty estimate
is related to the underlying pixel errors. To address this, we
propose to compute the Spearman’s rank correlation coeffi-
cient (CC), which estimates how well the examined uncer-
tainty values can be mapped onto the corresponding end-
point errors using an arbitrary monotonic function.
Table 2 and the sparsification plots in Fig. 2 show that our
uncertainty predictions are clearly superior in detecting the
most reliable flow estimates. The gradient uncertainty has
almost no ability to rank the pixels according to their accu-
racy. Hence, the simple consideration of input data appears
insufficient. Similarly, the Laplace measure does not lead
to satisfying results. ?Energy and ?Learned lead to very simi-
lar AUC results, whereby the learned uncertainty takes sub-
stantial time for training. Our method improves the AUC
over previous ones by more than 6%. Moreover, the evalu-
ation of the CC reveals that our uncertainty measure shows
by far the highest correlation between the assigned uncer-
tainty value and the per-pixel end-point error with a relative
improvement of 13% over the learned uncertainty.
7.3. ProbFlowFields
Next, we apply our probabilistic approach to the com-
petitive FlowFields method as described in Sec. 6. We use
validation test
Method AEPE rel. chg. AEPE rel. chg.
Initialization 3.303 0.06 – –
FlowFields [1] 3.147 <0.01 5.727† <0.01
FlowFields? 3.161 0.01 – –
ProbFlowFields (ours) 3.127 0.00 5.696 0.00
ProbFlowFields + BS 3.052 -0.02 5.628 -0.01
Table 3. Average end-point error (AEPE) and relative change (rel.
chg.) w.r.t. to ProbFlowFields on Sintel. †FlowFields shows better
results than the ones published on the website. See text for details.
Uncertainty measure AUC rel. chg. CC rel. chg.
Gradient [3] 1.022 1.57 -0.009 1.02
Laplace 0.657 0.65 0.257 0.54
Energy [8] 0.470 0.18 0.434 0.23
Learned [30] 0.474 0.19 0.451 0.20
ProbFlowFields (ours) 0.398 0.00 0.563 0.00
Oracle 0.182 – 1.000 –
Table 4. Area under curve (AUC), Spearman’s rank correlation
coefficient (CC), and relative change (rel. chg.) in comparison to
our uncertainty measure on a Sintel benchmark validation set.
the more recent Sintel benchmark [11], which in compari-
son to the Middlebury dataset, allows to partition its more
than 1000 flow sequences into training and validation sets.
Exemplary flow and uncertainty estimates as well as the cor-
responding ground truth can be seen in Fig. 3.
Table 3 summarizes the AEPEs on our validation set
and the test set of the Sintel benchmark. For comparison,
we also report results of FlowFields?, based on the same
consistent EpicFlow energy variant underlying ProbFlow-
Fields, c.f . Sec. 5.2. Again, our estimates from ProbFlow-
Fields are competitive with its underlying energy method
and we observe results on par with the original FlowFields.
Evaluated on the Sintel test set, ProbFlowFields cur-
rently ranks 6th in comparison to previously published
methods. Please note that the FlowFields test results shown
in Table 3 are superior to the publicly available AEPE of
5.810. To suppress the effect of the random component in
the matches of FlowFields and thus have a fair comparison,
we have re-evaluated the original FlowFields implementa-
tion using the exact same matches as for our approach.
The performance of the competing uncertainty measures
is evaluated in Table 4. Again, the Gradient and Laplace
uncertainties perform considerably worse than the remain-
ing approaches. The measures ?Learned and ?Energy result
in similar AUC values. Our method again leads to a clear
improvement of over 18%. Moreover, we also improve the
CC by 20% in comparison to the second best measure. The
sparsification plots in Fig. 4 show that our uncertainty mea-
sure leads to the best result for all fractions of removed pix-
els. Strikingly, the energy-based uncertainty as well as the
Laplace measure show a strong increase of the AEPE when
only a small fraction of pixels are kept. This is clearly un-
desirable as it indicates that the optical flow predictions are
incorrect for pixels that are considered as highly reliable.
Our probabilistic approach does not show this behavior.
To illustrate the benefits of our uncertainties in post-
processing, we apply the fast bilateral solver [4] on top of
ProbFlowFields, improving the AEPE by 2.4% and 1.2%
on the validation and test set, respectively (c.f . Table 3). In
comparison, post-processing assuming equally reliable flow
yields an improvement of only 0.4% on the validation set.
In a last experiment, we evaluated the overall runtime on
our Sintel validation set (Intel Core i7-3930K, 3.2 GHz, 6
cores). The average runtimes are 19.9s for FlowFields and
38.1s for ProbFlowFields, i.e. the additional estimation of
uncertainties has an overhead of ?1.9x. The best post-hoc
uncertainty measure (learned [30]) requires 123.8s on aver-
age, thus takes significantly longer than our joint approach.
7.4. Application to motion segmentation
We now show the benefit of our uncertainty estimates
when optical flow is used as a cue for motion segmentation.
Current state-of-the-art methods (e.g. [22, 33]) build upon
precomputed point trajectories, i.e. spatio-temporal curves
that describe the individual point motion over extended pe-
riods. Ideally, such point trajectories are sampled with a
regular density, they are long and reliable [6].
We build on the minimum cost multicut approach of [22]
to obtain sparse motion segmentations. We follow Keuper
et al. and generate point trajectories as in [6]. In a first setup,
we use FlowFields [1] to compute forward-backward (FB)
flow for all consecutive image pairs. Subsequently, points
are sampled on a regular grid in the first frame and tracked
as long as (1) their FB flows are consistent, and (2) the gra-
dient magnitude of the flow is below a threshold. If tracks
are stopped, new points are inserted to preserve sampling
regularity unless no points can be tracked in a region.
In a second setting, we apply ProbFlowFields using pa-
rameters trained on Sintel to compute FB flow along with
normalized forward uncertainties ?F. Here, we keep the FB
condition (1) and use a new condition (3) such that we end
any track passing through location (i, j) if the pixel uncer-
tainty ?Fij falls below an empirically determined threshold.
Our results in terms of segmentation precision, recall, F-
measure, as well as the achieved trajectory density on the
FBMS-59 dataset [33] are given in Table 5. For both ap-
proaches, sparse trajectories are sampled at 8 pixel distance,
and framewise dense segmentations are computed using the
variational approach of [32]. Moreover, we compare to the
current state-of-the-art [22] on FBMS-59, which is based on
Large Displacement Optical Flow (LDOF) [7].
The evaluation of sparse segmentations shows that
ProbFlow allows for a higher average point density of more
Figure 3. Examples of ground truth (top), flow predictions (middle), and uncertainty estimates (bottom) from ProbFlowFields on Sintel.
Figure 4. Sparsification plots averaged over Sintel validation.
than 1.18% compared to a maximal density of 0.89% with
FlowFields [1] or LDOF [7]. Low point density usually
indicates flow inconsistencies in homogeneous regions and
therefore a more uneven sampling. As can be seen by the
increased F-measure on sparse and especially on densified
segmentations, the improvement in the trajectory computa-
tion directly translates to an improved segmentation quality.
To illustrate the complementarity of our uncertainties
to the FB condition, we perform an experiment using
ProbFlowFields estimates and the FB check (1), but not the
corresponding uncertainties, i.e. omitting (3). In this case,
the F-measure of the sparse matches drops significantly. Us-
ing FlowFields estimates and the post-hoc measure ?Energy,
the uncertainties again complement (1). However, the usage
of ProbFlowFields yields a higher F-measure, thus showing
a clear benefit also in this application.
8. Conclusion
To address the issue that optical flow estimates are
not equally reliable throughout an image, we introduced
ProbFlow – a probabilistic framework for the joint estima-
tion of optical flow and its underlying uncertainty. Start-
ing from conventional energy minimization methods, we
derived the posterior distribution and used variational infer-
Training set (29 seq.) D P R F
LDOF [7] 0.81% 86.73% 73.08% 79.32%
FlowFields (1+2) 0.83% 87.19% 74.33% 80.25%
FlowFields (1) 1.17% 85.10% 71.36% 77.63%
FlowFields + ?Energy 1.17% 84.62% 72.57% 78.13%
ProbFlowFields (1+3) 1.18% 87.68% 75.13% 80.92%
ProbFlowFields (1) 1.34% 84.96% 72.14% 78.03%
FlowFields [1] dense 100% 86.14% 67.28% 75.55%
ProbFlowFields dense 100% 87.00% 70.15% 77.67%
Test set (30 seq.) D P R F
LDOF [7] 0.87% 87.88% 67.70% 76.48%
FlowFields (1+2) 0.89% 86.88% 69.74% 77.37%
ProbFlowFields (1+3) 1.19% 84.99% 72.83% 78.44%
FlowFields [1] dense 100% 84.38% 61.03% 70.83%
ProbFlowFields dense 100% 85.41% 66.93% 75.05%
Table 5. Motion segmentation results on the FBMS-59 dataset.
We report point density (D), average precision (P), average recall
(R), and F-measure. All results are computed for a sparse trajec-
tory sampling at 8 pixel distance with MCe motion segmentation
[22]. All results for LDOF are taken from [22].
ence to estimate the flow and a model-inherent uncertainty.
This is unlike existing uncertainty measures that detect re-
gions of unreliable flow in a post-hoc step. We applied our
approach to two different energy formulations on the Mid-
dlebury and Sintel benchmarks, where we obtain competi-
tive flow estimates and significantly improved uncertainties.
Applying our uncertainty estimates in the context of motion
segmentation, we were able to discard erroneous flow esti-
mates and generate highly reliable point trajectories.
Acknowledgments. The research leading to these results has re-
ceived funding from the European Research Council under the
European Union’s Seventh Framework Programme (FP/2007–
2013)/ERC Grant agreement No. 307942. We would like to thank
Tobias Plo?tz and Jochen Gast for helpful discussions.
References
[1] C. Bailer, B. Taetz, and D. Stricker. Flow fields: Dense corre-
spondence fields for highly accurate large displacement op-
tical flow estimation. ICCV, 2015. 2, 4, 5, 7, 8
[2] S. Baker, D. Scharstein, J. P. Lewis, S. Roth, M. J. Black,
and R. Szeliski. A database and evaluation methodology for
optical flow. Int. J. Comput. Vision, 92(1):1–31, 2011. 2, 6
[3] J. L. Barron, D. J. Fleet, and S. S. Beauchemin. Performance
of optical flow techniques. Int. J. Comput. Vision, 12(1):43–
77, 1994. 1, 2, 3, 5, 6, 7
[4] J. T. Barron and B. Poole. The fast bilateral solver. ECCV,
2016. 1, 7
[5] M. J. Black and P. Anandan. The robust estimation of mul-
tiple motions: Parametric and piecewise-smooth flow fields.
Comput. Vis. Image Und., 63(1):75–104, 1996. 1, 3
[6] T. Brox and J. Malik. Object segmentation by long term
analysis of point trajectories. ECCV, 2010. 7
[7] T. Brox and J. Malik. Large displacement optical flow: De-
scriptor matching in variational motion estimation. IEEE T.
Pattern Anal. Mach. Intell., 33(3):500 – 513, 2011. 7, 8
[8] A. Bruhn and J. Weickert. A confidence measure for
variational optic flow methods. In R. Klette, R. Kozera,
L. Noakes, and J. Weickert, editors, Geometric Properties
for Incomplete Data, pp. 283–298. Springer, 2006. 2, 5, 6, 7
[9] A. Bruhn, J. Weickert, and C. Schno?rr. Lucas/Kanade meets
Horn/Schunck: Combining local and global optic flow meth-
ods. Int. J. Comput. Vision, 61(3):211–231, 2005. 1, 3
[10] M. Brumm, J. M. Marcinczak, and R. Grigat. Improved con-
fidence measures for variational optical flow. VISAPP, 2015.
6
[11] D. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black. A
naturalistic open source movie for optical flow evaluation.
ECCV, 2012. 2, 7
[12] G. K. Chantas, T. Gkamas, and C. Nikou. Variational-Bayes
optical flow. J. Math. Imaging Vision, 50(3):199–213, 2014.
2
[13] Q. Chen and V. Koltun. Full flow: Optical flow estimation
by global optimization over regular grids. CVPR, 2016. 2
[14] A. Dosovitskiy, P. Fischer, E. Ilg, P. Ha?usser, C. Haz?rbas?,
V. Golkov, P. v.d. Smagt, D. Cremers, and T. Brox. Flownet:
Learning optical flow with convolutional networks. ICCV,
2015. 1, 2
[15] D. Fortun, P. Bouthemy, and C. Kervrann. Optical flow mod-
eling and computation: A survey. Comput. Vis. Image Und.,
134:1–21, 2015. 1, 2, 3, 5
[16] J. Gast, A. Sellent, and S. Roth. Parametric object motion
from blur. CVPR, 2016. 3, 4
[17] S. K. Gehrig and T. Scharwa?chter. A real-time multi-cue
framework for determining optical flow confidence. ICCV
Workshops, 2011. 2
[18] B. Glocker, N. Paragios, N. Komodakis, G. Tziritas, and
N. Navab. Optical flow estimation with uncertainties through
dynamic MRFs. CVPR, 2008. 2
[19] V. M. Govindu. Revisiting the brightness constraint: Proba-
bilistic formulation and algorithms. ECCV, 2006. 2
[20] H. Haußecker and H. Spies. Motion. In B. Ja?hne,
H. Haußecker, and P. Geißler, editors, Handbook of Com-
puter Vision and Applications, vol. 2. Academic Press, 1999.
1, 2
[21] B. K. P. Horn and B. G. Schunck. Determining optical flow.
Artificial Intelligence, 17(1–3):185–203, 1981. 1, 2, 3
[22] M. Keuper, B. Andres, and T. Brox. Motion trajectory seg-
mentation via minimum cost multicuts. ICCV, 2015. 2, 7,
8
[23] C. Kondermann, D. Kondermann, B. Ja?hne, and C. Garbe.
An adaptive confidence measure for optical flows based on
linear subspace projections. DAGM, 2007. 2
[24] C. Kondermann, R. Mester, and C. Garbe. A statistical con-
fidence measure for optical flows. ECCV, 2008. 1, 2, 5, 6
[25] P. Kra?henbu?hl and V. Koltun. Efficient nonlocal regulariza-
tion for optical flow. ECCV, 2012. 3, 6
[26] K. Krajsek and R. Mester. Bayesian model selection for op-
tical flow estimation. DAGM, 2007. 2
[27] J. Kybic and C. Nieuwenhuis. Bootstrap optical flow con-
fidence and uncertainty measure. Comput. Vis. Image Und.,
115(10):1449–1462, 2011. 1, 2, 5, 6
[28] A. Levin, Y. Weiss, F. Durand, and W. T. Freeman. Effi-
cient marginal likelihood optimization in blind deconvolu-
tion. CVPR, 2011. 3, 4
[29] Y. Li, D. Min, M. S. Brown, M. N. Do, and J. Lu. SPM-
BP: Sped-up PatchMatch belief propagation for continuous
MRFs. ICCV, 2015. 2
[30] O. Mac Aodha, A. Humayun, M. Pollefeys, and G. J. Bros-
tow. Learning a confidence measure for optical flow. IEEE
T. Pattern Anal. Mach. Intell., 35(5):1107–1120, 2013. 1, 2,
5, 6, 7
[31] P. Ma?rquez-Valle, D. Gil, and A. Herna?ndez-Sabate?. A com-
plete confidence framework for optical flow. ECCV Work-
shops and Demonstrations, 2012. 2, 6
[32] P. Ochs and T. Brox. Object segmentation in video: A hi-
erarchical variational approach for turning point trajectories
into dense regions. ICCV, 2011. 7
[33] P. Ochs, J. Malik, and T. Brox. Segmentation of moving
objects by long term video analysis. IEEE T. Pattern Anal.
Mach. Intell., 36(6):1187 – 1200, 2014. 1, 7
[34] M. Otte and H.-H. Nagel. Optical flow estimation: Advances
and comparisons. ECCV, 1994. 3
[35] D. Piao, P. G. Menon, and O. J. Mengshoel. Computing
probabilistic optical flow using Markov random fields. Com-
putational Modeling of Objects Presented in Images. Funda-
mentals, Methods, and Applications, pp. 241–247. Springer,
2014. 2
[36] J. Revaud, P. Weinzaepfel, Z. Harchaoui, and C. Schmid.
EpicFlow: Edge-preserving interpolation of correspon-
dences for optical flow. CVPR, 2015. 1, 2, 4, 5
[37] S. Roth and M. J. Black. On the spatial statistics of optical
flow. Int. J. Comput. Vision, 74(1):33–50, 2007. 4
[38] S. Roth and M. J. Black. Steerable random fields. ICCV,
2007. 4
[39] S. Roy and V. Govindu. MRF solutions for probabilistic op-
tical flow formulations. ICPR, 2000. 2
[40] E. P. Simoncelli, E. H. Adelson, and D. J. Heeger. Probability
distributions of optical flow. CVPR, 1991. 2
[41] A. Singh. An estimation-theoretic framework for image-flow
computation. ICCV, 1990. 2
[42] J. Snoek, H. Larochelle, and R. P. Adams. Practical Bayesian
optimization of machine learning algorithms. NIPS*2012,
pp. 2951–2959. 5
[43] D. Sun, S. Roth, and M. J. Black. A quantitative analysis of
current practices in optical flow estimation and the principles
behind them. Int. J. Comput. Vision, 106(2):115–137, 2014.
1, 2, 3, 4, 5, 6
[44] D. Sun, S. Roth, J. P. Lewis, and M. J. Black. Learning
optical flow. ECCV, 2008. 2, 3, 5
[45] M. J. Wainwright and M. I. Jordan. Graphical models, expo-
nential families, and variational inference. Foundations and
Trends in Machine Learning, 1(1–2):1–305, 2008. 3
[46] M. J. Wainwright and E. P. Simoncelli. Scale mixtures of
gaussians and the statistics of natural images. NIPS*1999. 3
[47] A. Wedel, A. Meißner, C. Rabe, U. Franke, and D. Cre-
mers. Detection and segmentation of independently moving
objects from dense scene flow. EMMCVPR, 2009. 1
[48] J. Winn and C. M. Bishop. Variational message passing. J.
Mach. Learn. Res., 6:661–694, 2005. 5
[49] H. Zimmer, A. Bruhn, and J. Weickert. Optic flow in har-
mony. Int. J. Comput. Vision, 93(3):368–388, 2011. 4
ProbFlow: Joint Optical Flow and Uncertainty Estimation
– Supplemental Material –
Anne S. Wannenwetsch1 Margret Keuper2 Stefan Roth1
1TU Darmstadt 2University of Mannheim
Preface. In this supplemental material we derive update equations for the mean-field inference in Eq. (13) and show a
proof for the solution of the Bayesian risk minimization in Eq. (8). We give further implementation details of ProbClassicA
and ProbFlowFields, and present an analysis considering different design choices. Finally, we evaluate the performance of
additional uncertainty measures and apply, for completeness, ProbFlowFields on the Middlebury benchmark [2].
A. Mean-field Update Equations
In the following, we show how to derive the mean-field update equations for ProbClassicA. Update equations for ProbFlow-
Fields can be obtained similarly.
Notation. Note that strictly speaking the latent variables are given as h = (h?,ij,l)?,ij with ? ? {D,S1, . . . ,Sp,N1, . . . ,Nq},
p = |S(i, j)|, q = |N(i, j)| as we have separate latent variables for all penalty functions. In the following, we therefore use
the notation hSe , hNe and fSe(·), fNe(·) for e ? S(i, j) and e ? N(i, j), respectively. The flow vector of the correspond-
ing neighboring pixel is denoted as ye. Moreover, GSM parameters ?l and ?l differ for data, smoothness, and non-local
potentials. For better readability, we drop indices D, S, and N that explicitly distinguish between the different GSMs.
Variational objective. As shown in Eq. (13), variational parameters ?? are determined to minimize the Kullback-Leibler
divergence between posterior p and its approximating distribution q, i.e.
?? = arg min
?
DKL
(
q(y, y?,h;?) | p(y, y?,h | I)
)
(22a)
= arg min
?
Eq(y,y?,h;?)
[
log q(y, y?,h;?)
]
? ?? ?
¬, entropy term
?Eq(y,y?,h;?)
[
log p(y, y?,h | I)
]
? ?? ?
­
. (22b)
Recall that we defined the variational distribution q in Eq. (11) as
q(y, y?,h;?) =
?
i,j
[
q (yij ;?) · q (y?ij ;?) ·
?
?
q (h?,ij ;?)
]
. (23)
Then, the entropy term in ¬ can be split up as follows:
Eq(y,y?,h;?)
[
log q(y, y?,h;?)
]
= Eq(y,y?,h;?)
[?
i,j
log q (yij ;?) +
?
i,j
log q (y?ij ;?) +
?
i,j
?
?
log q (h?,ij ;?)
]
(24a)
=
?
i,j
Eq(yij ;?) log q (yij ;?) +
?
i,j
Eq(y?ij ;?) log q (y?ij ;?) +
?
i,j
?
?
Eq(h?,ij ;?) log q (h?,ij ;?) . (24b)
Using the well-known entropy of a Gaussian and a multinomial distribution, we obtain
Eq(y,y?,h;?)
[
log q(y, y?,h;?)
]
= ?1
2
?
i,j
log
(
det(?ij)
)
? 1
2
?
i,j
log
(
det(??ij)
)
+
?
i,j
?
?
?
l
k?,ij,l log k?,ij,l + const.
(25)
In order to evaluate the term ­ in Eq. (22b), it is necessary to compute
log p(y, y?,h | I)
= log
?
? 1
Z
?
i,j
[?
l
?
hD,ij,l
l N
(
fD
(
yij ; I
)
; 0, ?2l
)hD,ij,l
]?D
·
?
e?S(i,j)
[?
l
?
hSe,ij,l
l N
(
fSe
(
yij ,ye
)
; 0, ?2l
)hSe,ij,l
]?S
· exp
[
? fC
(
yij , y?ij
)2
]?C
·
?
e?N(i,j)
[?
l
?
hNe,ij,l
l N
(
fNe
(
y?ij , y?e
)
; 0, ?2l
)hNe,ij,l
]?N ?
? (26a)
=
?
i,j
?
??D
?
l
hD,ij,l
(
log ?l + logN
(
fD
(
yij ; I
)
; 0, ?2l
))
+ ?S
?
e?S(i,j)
?
l
hSe,ij,l
(
log ?l + logN
(
fSe
(
yij ,ye
)
; 0, ?2l
))
? ?C fC
(
yij , y?ij
)2
+ ?N
?
e?N(i,j)
?
l
hNe,ij,l
(
log ?l + logN
(
fNe
(
y?ij , y?e
)
; 0, ?2l
))
?
?? logZ (26b)
=
?
i,j
?
??D
?
l
hD,ij,l
(
log ?l ? log ?l ?
fD
(
yij ; I
)2
2?2l
)
+ ?S
?
e?S(i,j)
?
l
hSe,ij,l
(
log ?l ? log ?l ?
fSe
(
yij ,ye
)2
2?2l
)
? ?C fC
(
yij , y?ij
)2
+ ?N
?
e?N(i,j)
?
l
hNe,ij,l
(
log ?l ? log ?l ?
fNe
(
y?ij , y?e
)2
2?2l
)?
?+ const, (26c)
where we have defined fC
(
yij , y?ij
)
= ?yij ? y?ij?2. We now take the expectation over h and simplify the remaining
expectations as
Eq(y,y?,h;?) log p(y, y?,h | I)
= Eq(y,y?;?)
?
i,j
?
??D
?
l
kD,ij,l
(
log ?l ? log ?l ?
fD(yij ; I)
2
2?2l
)
+ ?S
?
e?S(i,j)
?
l
kSe,ij,l
(
log ?l ? log ?l ?
fSe
(
yij ,ye
)2
2?2l
)
? ?C fC
(
yij , y?ij
)2
+ ?N
?
e?N(i,j)
?
l
kNe,ij,l
(
log ?l ? log ?l ?
fNe
(
y?ij , y?e
)2
2?2l
)?
?+ const (27a)
=
?
i,j
?
??D
?
l
kD,ij,l
(
log ?l ? log ?l
)
+ ?S
?
e?S(i,j)
?
l
kSe,ij,l
(
log ?l ? log ?l
)
+ ?N
?
e?N(i,j)
?
l
kNe,ij,l
(
log ?l ? log ?l
)
?
?
?
?
i,j
?
??D
?
l
kD,ij,l
2?2l
Eq(y;?) fD
(
yij ; I
)2
? ?? ?
®, gD
+?S
?
e?S(i,j)
?
l
kSe,ij,l
2?2l
Eq(y;?) fSe
(
yij ,ye
)2
? ?? ?
¯, gSe
+ ?C Eq(y,y?;?) fC
(
yij , y?ij
)2
? ?? ?
°, gC
+?N
?
e?N(i,j)
?
l
kNe,ij,l
2?2l
Eq(y?;?) fNe
(
y?ij , y?e
)2
? ?? ?
±, gNe
?
?+ const. (27b)
To solve the expectation value w.r.t. the linearized brightness constancy in ®, we define a = I2
(
i+ u0ij , j + v
0
ij
)
?I1 (i, j)
and b = ?2I2
(
i+ u0ij , j + v
0
ij
)T
. Then we have that
gD
(
µij ,?ij ; I
)
= Eq(y;?) fD
(
yij ; I
)2
(28a)
= Eq(yij ;?)
[(
a+ bT
(
yij ? y0ij
))2]
(28b)
= Eq(yij ;?)
[
a2 + 2abT
(
yij ? y0ij
)
+
(
yij ? y0ij
)T(
bbT
)(
yij ? y0ij
)]
(28c)
= a2 + 2abT
(
µij ? y0ij
)
+
(
µij ? y0ij
)T(
bbT
)(
µij ? y0ij
)
+ Tr
(
bbT?i,j
)
. (28d)
We solve the expectation value gSe in ¯ for an exemplary function fSe
(
yij ,ye
)
= uij ? ue. All remaining terms as well as
the terms gNe in ± can be resolved in the same manner. Using A1 =
(
1 0
0 0
)
, it holds that
gSe
(
µij ,?ij ,µe,?e
)
= Eq(y;?) fSe
(
yij ,ye
)2
(29a)
= Eq(ye;?) Eq(yij ;?)
[(
yij ? ye
)T
A1
(
yij ? ye
)]
(29b)
= Eq(ye;?)
[(
µij ? ye
)T
A1
(
µij ? ye
)
+ Tr
(
A1?ij
)]
(29c)
=
(
µij ? µe
)T
A1
(
µij ? µe
)
+ Tr
(
A1?ij
)
+ Tr
(
A1?e
)
(29d)
=
(
µ
(1)
ij ? µ(1)e
)2
+
(
?ij
)
1,1
+
(
?e
)
1,1
(29e)
with µ(1)ij denoting the first (i.e., horizontal) component of the mean flow vector at pixel (i, j). The term gC in ° can be
determined as
gC
(
µij ,?ij , µ?ij , ??ij
)
= Eq(y,y?;?) fC
(
yij , y?ij
)2
(30a)
= Eq(yij ;?) Eq(y?ij ;?)
[(
yij ? y?ij
)T(
yij ? y?ij
)]
(30b)
= Eq(y?ij ;?)
[(
µij ? y?ij
)T(
µij ? y?ij
)
+ Tr
(
?ij
)]
(30c)
=
(
µij ? µ?ij
)T(
µij ? µ?ij
)
+ Tr
(
?ij
)
+ Tr
(
??ij
)
. (30d)
Update equations. To obtain update equations, we compute the derivative of the KL divergence in Eq. (22b), set it to zero,
and solve for the desired variable. Please note that update equations for boundary pixels may slightly differ from the ones
shown below. From now on, spatial derivatives of I are denoted as Ix and Iy , the temporal derivative is given as It. Moreover,
diag(·) represents a diagonal matrix and we define vectors K? =
(?
l
k?,ij,l
?2l
)
ij
.
As the update of each mean flow estimate µij depends on other entries of µ, it is desirable to jointly solve for all compo-
nents of the flow field. Therefore, µ is obtained as the solution of a linear equation system, c.f . [50, 37], such that
Ax = b, x =
(
µ
(1)
11 , . . . , µ
(1)
nm, µ
(2)
11 , . . . , µ
(2)
nm
)T
, A = AD + AS + AC, b = bD + bS + bC. (31)
The components of the linear equation system are determined as
AD = ?D
(
diag
(
KD
)
diag
(
I2x
)
diag
(
KD
)
diag
(
Ix · Iy
)
diag
(
KD
)
diag
(
Ix · Iy
)
diag
(
KD
)
diag
(
I2y
)
)
, (32a)
AS = ?S
(
FT1 diag
(
KS1
)
F1 + F
T
2 diag
(
KS2
)
F2 0
0 FT1 diag
(
KS3
)
F1 + F
T
2 diag
(
KS4
)
F2
)
, (32b)
AC = 2?CI, (32c)
bD = ADy0 ? ?D
(
diag
(
KD
)
diag
(
Ix · It
)
1
diag
(
KD
)
diag
(
Iy · It
)
1
)
, bS = 0, bC = 2?Cµ?. (32d)
Here, F1 and F2 represent filter matrices corresponding to the derivative filters H1 =
[
1,?1
]T
and H2 =
[
1,?1
]
, which
are used in fSe
(
yij ,ye
)
, c.f . [37]. I is the identity matrix and 0 is a matrix of all zeros.
When updating the auxiliary flow means µ?, a 5 × 5 neighborhood has to be considered. Therefore, a joint update of all
estimates is computationally expensive and we follow [43] assuming fixed values for neighboring pixels, i.e.
µ?ij,t =
(
µ?
(1)
ij,t
µ?
(2)
ij,t
)
, µ?
(k)
ij,t =
2?C µ
(k)
ij + ?N
?
e?Nk(i,j)
(
KNke
)
ij,t?1 · µ?
(k)
e,t?1
2?C + ?N
?
e?Nk(i,j)
(
KNke
)
ij,t?1
. (33)
Here, Nk(i, j) represents the set of neighbors in terms of the kth optical flow component.
For the flow variances ? =
(
?1 0
0 ?2
)
we derive a closed-form update dependent only on the latent variables k with
?1 =
(
?D diag
(
I2x
)
·KD + ?S
[
abs
(
FT1
)
·KS1 + abs
(
FT2
)
·KS2
]
+ 2?C
)?1
(34a)
and ?2 =
(
?D diag
(
I2y
)
·KD + ?S
[
abs
(
FT1
)
·KS3 + abs
(
FT2
)
·KS4
]
+ 2?C
)?1
, (34b)
where the absolute value function abs(·) is applied element-wise.
We assume fixed neighboring values also for the update of the auxiliary flow variances ??, and obtain
??ij,t =
(
??
(1)
ij,t 0
0 ??
(2)
ij,t
)
, ??
(k)
ij,t =
1
2?C + ?N
?
e?Nk(i,j)
(
KNke
)
ij,t?1
. (35)
To derive an update equation for a latent variable k?,ij , we need to consider a Lagrangian function including the KL
divergence in Eq. (22b) as well as the constraint
?
l k?,ij,l = 1. Solving the resulting linear equation system analytically
gives us, e.g.,
kD,ij,l =
(
?l
?l
)?D
exp
[
??D
gD
(
µij ,?ij ; I
)
2?2l
]
· ZD,ij (36a)
with ZD,ij =
(
L?
l=1
(
?l
?l
)?D
exp
[
??D
gD
(
µij ,?ij ; I
)
2?2l
])?1
(36b)
for the latent variables of the data term using the expectation values gD
(
µij ,?ij ; I
)
as derived in Eq. (28d). Update equations
for the remaining latent variables are derived similarly.
B. Bayesian Risk Minimization
We aim to show that the solution of the Bayesian risk minimization in Eq. (8) is given as y?ij = µij when replacing the
posterior p with its approximating distribution q and using the Average End-Point Error (AEPE) as a loss function.
Recall that the AEPE is defined as l (y, y?) =
?
i,j ` (yij , y?ij) =
?
i,j ?yij ? y?ij?2 with
?2 ` (a? x, x?) = (a? x? x?) /?a? x? x??2 (37a)
= ? (x? (a? x?)) /?x? (a? x?) ?2 (37b)
= ??2 ` (x,a? x?) (37c)
for arbitrary a ? R2. W.l.o.g. we minimize the expected risk of l(y, y?) and therefore set f(y?) = Eq(y,y?,h;?) [l(y, y?)]. Note
that we omit the variational parameters ? in the following for brevity. Using the properties of q, we obtain
f(y?) =
?
Y
?
Y?
?
H
q(y, y?,h) · l (y, y?) dy dy? (38a)
q fac.
=
?
Y
q(y) · l (y, y?) dy (38b)
=
?
i,j
?
R2
q(yij) · ` (yij , y?ij) dyij
? ?? ?
=:fij(y?ij)
. (38c)
For fixed yij ? R2, the function q(yij) · ` (yij , y?ij) is convex in y?ij . Therefore, the objective f(y?) is convex in y? and the
Bayesian risk minimization has a unique solution given by
yij = arg min
y?ij
fij(y?ij). (39)
It only remains to be shown that?2 fij(y?ij) = 0 holds for y?ij = µij . Setting y?ij = µij we obtain
? µij
??
q(? ) · ?2 `
(
? ,µij
)
d?
(z1=??µij)
=
? 0
??
q(µij + z1) · ?2 `
(
µij + z1,µij
)
dz1 (40a)
q sym.
=
? 0
??
q(µij ? z1) · ?2 `
(
µij + z1,µij
)
dz1 (40b)
(z2=µij?z1)
=
? ?
µij
q(z2) · ?2 `
(
2µij ? z2,µij
)
dz2 (40c)
(37a)?(37c)
= ?
? ?
µij
q(z2) · ?2 `
(
z2,µij
)
dz2 (40d)
and finally
?2 fij
(
µij
)
=
?
R2
q(? ) · ?2 `
(
? ,µij
)
d? (41a)
=
? µij
??
q(? ) · ?2 `
(
? ,µij
)
d? +
? ?
µij
q(? ) · ?2 `
(
? ,µij
)
d?
(40d)
= ?
? ?
µij
q(? ) · ?2 `
(
? ,µij
)
d? +
? ?
µij
q(? ) · ?2 `
(
? ,µij
)
d? (41b)
= 0. (41c)
C. Implementation Details
In this section, we present our design choices following the best-practices of energy-based optical flow techniques, and
give an analysis evaluating the influence of the specifics. Moreover, we give details of our post-processing approach using
the fast bilateral solver [4].
C.1. ProbClassicA
In our ProbClassicA algorithm, we perform three steps of graduated non-convexity and apply coarse-to-fine estimation
with 10 warping steps per layer. As in [43], we restrict the flow update to an absolute value of 1 and pre-process the images
using a structure-texture decomposition. Spline-based cubic interpolation as well as an averaging of image gradients ?2I1
and?2I2 are applied. During the inference, the variable sets {µ,?,k} and
{
µ?, ??, k?
}
are updated in an alternating way. As
an inner update step, we apply five iterations of the block-coordinate descent scheme on µ, ? and k. For the set
{
µ?, ??, k?
}
,
a number of three inner updates performs better.
C.2. ProbFlowFields
For ProbFlowFields, we follow [36] and pre-smooth images using a Gaussian kernel of size 9 × 9 with ? = 1.1. For
warping, we apply bilinear interpolation and averaged image derivatives. Moreover, we perform five warping steps, each
with five iterations of our block-coordinate descent scheme. We follow Revaud et al. and compute optical flow updates with
30 iterations of successive over relaxation, which performs noticeably faster than the solver used in [43].
C.3. Evaluation of design choices
Table 6 summarizes results of AEPE, AUC, and CC on the Middlebury and Sintel benchmarks using varying setups of
ProbClassicA and ProbFlowFields. In a first step, we evaluate a setting for ProbClassicA in which parameters ?D, ?S, and ?N
are determined by having the Bayesian optimization [42] consider only the AEPE or only the AUC instead of the F1-score
ProbClassicA Middlebury AEPE rel. chg. AUC rel. chg. CC rel. chg.
Baseline 0.296 – 0.466 – 0.374 –
Bayesian optim. w.r.t. AEPE only 0.290 -0.02 0.471 0.01 0.351 0.06
Bayesian optim. w.r.t. AUC only 0.312 0.05 0.436 -0.06 0.451 -0.21
EN = EC = 0 0.411 0.39 0.889 0.91 0.125 0.67
No structure-texture decomposition 0.290 -0.02 0.445 -0.05 0.361 0.03
ProbFlowFields Sintel validation AEPE rel. chg. AUC rel. chg. CC rel. chg.
Baseline 3.127 – 0.398 – 0.563 –
Bayesian optim. w.r.t. AEPE only 3.128 <0.01 0.475 0.19 0.407 0.28
Bayesian optim. w.r.t. AUC only 3.219 0.03 0.381 -0.04 0.644 -0.14
Spatially constant ?S 3.127 0.00 0.400 <0.01 0.562 <0.01
?rij = 1 3.125 >-0.01 0.396 >-0.01 0.548 0.03
No gradient averaging 3.135 <0.01 0.398 0.00 0.557 0.01
No Gaussian smoothing 3.135 <0.01 0.441 0.11 0.497 0.12
10 warping steps 3.123 >-0.01 0.421 0.06 0.538 0.04
Table 6. Analysis of several design choices for ProbClassicA on Middlebury and ProbFlowFields on the Sintel validation set. Bold entries
denote strong deviations from the baseline.
proposed in Eq. (20). In both cases, we observe that the performance w.r.t. the evaluation metric that is not considered during
the Bayesian optimization drops significantly. This highlights the importance of the F1-score to balance the accuracy of
flow and uncertainty estimates. Moreover, we show that the AEPE as well as the performance of the uncertainty measure is
clearly inferior if no additional nonlocal term is applied (EN = EC = 0). When using ProbClassicA without structure-texture
decomposition as pre-processing, we surprisingly obtain improved results for the AEPE (2%) as well as the AUC (5%). This
is in contrast to energy minimization, where this pre-processing helps [43]. For fairness of comparison to the underlying
energy minimization approach, we continue to use a structure-texture decomposition.
Considering ProbFlowFields, we observe the same behavior as for ProbClassicA when Bayesian optimization is carried
out only with respect to one of the evaluation metrics. Note that the parameter setting obtained by a Bayesian optimization
w.r.t. to the AEPE performs better than the baseline on the training set even though no improvement of the AEPE is visible
on the validation set. The usage of a spatially constant trade-off parameter ?S, turning off the normalization of the spatial
derivatives (?rij = 1, c.f . Eqs. (17) and (18)), and not averaging the image gradients, respectively, only lead to minor changes.
When no Gaussian smoothing is applied for image pre-processing, a clear effect on the AUC as well as the CC can be
observed whereas the AEPE is only slightly changed. Finally, the application of 10 warping steps only results in small
improvements of the AEPE and even decreases the performance of the uncertainty measure. This justifies the usage of a
reduced number of 5 steps to save computational time.
C.4. Post-processing using the fast bilateral solver
As described in Sec. 7.3, we apply the fast bilateral solver [4] on top of ProbFlowFields in order to illustrate the benefits of
uncertainty predictions for a further improvement of the flow estimates. In doing so, we normalize the estimated uncertainties
with a sigmoid function and invert the values to obtain the confidences required by the fast bilateral solver. A Bayesian
optimization [42] is performed on our Sintel training set to obtain appropriate sigmoid parameters as well as a suitable
trade-off parameter for the fast bilateral solver. See Fig. 5 for a screenshot of the private Sintel benchmark table showing
results after post-processing (ProbFlowFields + BS). For the reported baseline, we process the estimates of ProbFlowFields
assuming a uniform confidence of 0.5.
D. Additional Uncertainty Measures
In the following, we evaluate several additional uncertainty measures on the Middlebury as well as the Sintel benchmark.
Haußecker and Spies [20] introduce three confidence measures based on the spatio-temporal structure tensor
S = G(??) ?
[
(?3I)(?3I)T
]
with ?3I = (Ix, Iy, It)T, (42)
where Ix and Iy denote the spatial image derivatives computed with central differences and It is the temporal difference
between I1 and I2. Following [30], we smooth the derivatives with a Gaussian filter G(??) of size 7 × 7 and a standard
Figure 5. Screenshot of private Sintel table (final) showing results for ProbFlowFields and ProbFlowFields + BS (status as of July 2017).
Uncertainty measure AUC rel. chg. CC rel. chg.
Ct [20] 1.058 1.27 -0.106 1.28
Cs [20] 1.014 1.18 -0.057 1.15
Cc [20] 0.967 1.08 -0.022 1.06
Ev3 [27] 0.989 1.12 0.058 0.84
Noise 0.512 0.10 0.286 0.24
ProbClassicA (ours) 0.466 0.00 0.374 0.00
Oracle 0.255 – 1.000 –
Table 7. Area under curve (AUC), Spearman’s rank correlation co-
efficient (CC), and relative change (rel. chg.) in comparison to the
our uncertainty measure on the Middlebury dataset.
Uncertainty measure AUC rel. chg. CC rel. chg.
Ct [20] 1.130 1.84 -0.128 1.23
Cs [20] 1.154 1.90 -0.149 1.26
Cc [20] 0.915 1.30 0.129 0.77
Ev3 [27] 1.024 1.57 -0.030 1.05
Noise 0.512 0.29 0.382 0.32
ProbFlowFields (ours) 0.398 0.00 0.563 0.00
Oracle 0.182 – 1.000 –
Table 8. Area under curve (AUC), Spearman’s rank correlation co-
efficient (CC), and relative change (rel. chg.) in comparison to our
uncertainty measure on a Sintel benchmark validation set.
deviation ?? = 2. In [20], eigenvalues ?1, ?2, and ?3 of S are computed such that ?1 ? ?2 ? ?3. Uncertainty measures are
then obtained as
?Ct = ?
(
?1 ? ?3
?1 + ?3
)2
, ?Cs = ?
(
?1 ? ?2
?1 + ?2
)2
, and ?Cc = ?Ct ??Cs. (43)
Moreover, we evaluate a baseline uncertainty measure as used in [27] defined as ?Ev3 = ??3.
Finally, we compare to a sampling-based measure similar to the idea of Kybic and Nieuwenhuis [27]. That is, we estimate
the uncertainty as the variance of the optical flow estimates resulting from small, random perturbations of the input data.
Specifically, we apply zero-mean Gaussian noise on the input images and determine appropriate values for the variance of
the noise on the training set. The uncertainty measure is then obtained as ?Noise =
?
?2u + ?
2
v with ?u and ?v denoting the
standard derivation of the horizontal and vertical flow estimates per pixel.
As can be seen in Tables 7 and 8, all measures based on the structure tensor perform considerably worse than our proposed
uncertainty measure. ?Cc and ?Ev3 lead to more meaningful uncertainties than the two remaining approaches on both
datasets, but perform similar to the simple gradient-based measure [3]. The noise uncertainty – especially on the Middlebury
dataset – performs comparably to ?Energy and ?Learned. However, our ProbFlow approach clearly leads to superior results.
E. ProbFlowFields on Middlebury
For completeness, we report the results of ProbFlowFields on Middlebury. To reproduce the Middlebury results shown
in [1] we applied the default settings of the EpicFlow interpolation. Moreover, we use GSM potentials trained on the Sintel
training test
Method AEPE rel. chg. AEPE rel. chg.
Initialization 0.307 0.38 – –
FlowFields [1] 0.240 0.08 0.331† 0.10
FieldsFields? 0.230 0.04 – –
ProbFlowFields (ours) 0.222 0.00 0.301 0.00
Table 9. Average end-point error (AEPE) and relative change (rel.
chg.) in comparison to the ProbFlowFields method on the Middle-
bury benchmark. †Please note that we did not re-evaluate Flow-
Fields, but show the publicly available results.
Uncertainty measure AUC rel. chg. CC rel. chg.
Gradient [3] 1.244 1.72 -0.077 1.21
Laplace 0.539 0.18 0.297 0.20
Energy [8] 0.563 0.23 0.253 0.32
Learned [30] 0.473 0.04 0.374 >-0.01
ProbFlowFields (ours) 0.457 0.00 0.371 0.00
Oracle 0.247 – 1.000 –
Table 10. Area under curve (AUC), Spearman’s rank correlation
coefficient (CC), and relative change (rel. chg.) in comparison to
the energy uncertainty measure on the Middlebury training set.
...
...
...
Figure 6. Screenshot of private Middlebury table showing results for ProbFlowFields and ProbClassicA (status as of July 2017).
dataset for our ProbFlowFields approach. The results evaluating the AEPE on the Middlebury benchmark can be found in
Table 9. We outperform the original FlowFields approach on training and test and obtain improved results in comparison to
FlowFields?. Please note that the Middlebury benchmark policy allows no more than one entry per method in the public table.
Therefore, we decided to show the results of ProbFlowFields on the Middlebury website whereas the results of ProbClassicA
from Table 1 of the main paper are only visible in a private table, see Fig. 6 for a screenshot.
Table 10 shows an evaluation of different uncertainty measures. In contrast to our remaining experiments, the Laplace and
learned uncertainty measures both outperform the energy-based approach. Our uncertainty measure is slightly outperformed
by ?Learned w.r.t. the CC metric. However, ProbFlowFields shows clearly superior results considering the AUC.
References
[50] T. Brox, A. Bruhn, N. Papenberg, and J. Weickert. High accuracy optical flow estimation based on a theory for warping. ECCV,
2004. 3
