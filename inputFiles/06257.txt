Notes: A Continuous Model of Neural Networks
Part I: Residual Networks
Zhen Li ? Zuoqiang Shi †
August 22, 2017
Abstract
In this series of notes, we try to model neural networks as as discretizations of
continuous flows on the space of data, which can be called flow model. The idea comes
from an observation of their similarity in mathematical structures. This conceptual
analogy has not been proven useful yet, but it seems interesting to explore.
In this part, we start with a linear transport equation (with nonlinear transport
velocity field) and obtain a class of residual type neural networks. If the transport
velocity field has a special form, the obtained network is found similar to the original
ResNet (He et al., 2016). This neural network can be regarded as a discretization of
the continuous flow defined by the transport flow.
In the end, a summary of the correspondence between neural networks and trans-
port equations is presented, followed by some general discussions.
1 Transport Equation
Consider the following terminal value problem (TVP) for linear transport equation:{
?tu + v(t, x) · ?u = 0, x ? Rd, t ? [0, T ]
u(T, x) = f(x), x ? Rd.
(1)
Here v is a Rd-valued function, called the transport velocity field. It can be chosen in
different ways. We will consider firstly the general form, then a special type:
v(t, x) = W (2)(t)a
(
W (1)(t) + b(1)(t)
)
+ b(2)(t), (2)
where W (1)(t),W (2)(t) ? Rd×d, b(1)(t), b(2)(t) ? Rd. The activation a is a Rd-valued
nonlinear function, which is Lipschitz continuous.
?Department of Mathematics, HKUST. Email: lishen03@gmail.com
†Yau Mathematical Sciences Center, Tsinghua University. Email: zqshi@tsinghua.edu.cn
1
ar
X
iv
:1
70
8.
06
25
7v
1 
 [
cs
.L
G
] 
 2
1 
A
ug
 2
01
7
It is well known that the solution of equation (1) is transported along characteristics.
The characteristics are defined as solutions of the initial value problems (IVP) of the
ODE: {
x? = v(t, x), t ? [0, T ]
x(0) = x0,
(3)
where x0 ? Rd. Along the solution curve x = q(t), it is easy to verify that
d
dt
u(t, q(t)) = (?tu(t, x) + q?(t) · ?u(t, x))x=q(t) (4)
= (?tu(t, x) + v(t, q(t)) · ?u(t, x))x=q(t) = 0. (5)
So u remains unchanged along the curve. See Figure 1 for a conceptual illustration.
Therefore
u(0, x0) = u(t, q(t)) = u(T, q(T )) = f(q(T )). (6)
Figure 1: Illustration of characteristics. Here x, u(t, x) ? R.
2 Method of Characteristics
In this part we will use the method of characteristics to solve (1). In order to make
the following approximations reasonable, we assume that the change of v(t, x) with t
and x is regular enough. Especially, we assume that the solution of (1) and (3) exist
for the posed conditions, and they are regular enough.
Let {tk}Lk=0 with t0 = 0 and tL = T be a partition of [0, T ] ? R such that for
any k = 1, . . . , L, sk = tk ? tk?1 is small enough. Let x = q(t) be a characteristic of
the transport equation (1), i.e. the solution of (3), and denote xk = q(tk). Denote
Vk(x) = v(tk, x) and uk(x) = u(tk, x) for any x ? R. See Figure 2 for a illustration of
the discretization.
2
Figure 2: Illustration of discretization.
Near time tk, the ODE (3) is approximately
x? = Vk(xk) ? Vk(xk?1). (7)
Use Euler method to integrate this ODE from tk?1 to tk, we get
xk ? xk?1 +
? tk
tk?1
Vk(xk?1)dt (8)
? xk?1 + skVk(xk?1) (9)
= (id + skVk)(xk?1). (10)
Therefore
xL = (id + sLVL)(xL?1) (11)
= (id + sLVL) ? · · · ? (id + s1V1)(x0) (12)
If the terminal value function of u is given as uL = f , we might be able to use (12) to
get the initial value u0. According to (6),
u0(x0) = uL(xL) = f ? (id + sLVL) ? · · · ? (id + s1V1)(x0) (13)
3 Neural Network Representation
3.1 General form
The discrete solution (13) of the terminal value problem of transport equation (1) is
valid for any x0 ? Rd. Its basic structure is shown in Figure 3. This structure reminds
us of the ResNet (He et al., 2016), but it is merely a formal one. In order to see the
actual structure, we need to specify the definition of Vk’s.
Figure 3: Basic structure of a general ResNet. Notice that {skVk}Lk=1 are generally nonlinear
functions of the input.
3
3.2 A Special Type
In order to get a ResNet with explicit 2-layer block, consider the special type of trans-
port velocity field given by (2). Denote
W
(1)
k = W
(1)(tk), b
(1)
k = b
(1)(tk), (14)
W
(2)
k = W
(2)(tk), b
(2)
k = b
(2)(tk). (15)
W
(2)
k = skW
(2)
k , b
(2)
k = skb
(2)
k . (16)
By using the method of characteristics as before, we can get
xk = xk?1 + W
(2)
k a
(
W
(1)
k xk?1 + b
(1)
k
)
+ b
(2)
k . (17)
It generates a 2-layer ResNet block, which is much more like the original ResNet. 1
Figure 4 illustrates its basic structure.
Figure 4: Basic structure of the 2-layer ResNet block.
At a first glance, it appears that directly define the transport velocity as (2) is not
natural. But it is actually reasonable. The inner weights W (1) and b(1) are used to
adjust the distribution of the transport velocity field according to the location of input.
Notice that the activation a is usually non-negative, or even restricted to [0, 1]. Thus
the outer weights W (2) and b(2) are needed to adjust the direction and magnitude of
the transport velocity. Both inner weights and outer weights are necessary ingredients
of the velocity field.
3.3 Discussions
The ResNets obtained here are special. Firstly, as we can see in (10) and (17), due
to the time step sk, the residual term can be made sufficiently small comparing with
the leading term xk. This is a necessary condition for the ResNet to be modeled by
transport equation.
Secondly, the weights of the ResNet changes slowly from block to block. More
specifically, the weights on the same positions of adjacent ResNet blocks should be close
to each other, because they are assumed to be discretizations of continuous functions
of time. For example, W
(1)
k is close to W
(1)
k?1, W
(2)
k is close to W
(2)
k?1, and so on.
1One concern is that the layers of this ResNet are of the same width. This may not be a series problem
in theory, because they can always be embedded in to the same high dimensional space.
4
3.4 Summary
Clearly, there is a correspondence between the ResNets (10) (17) and the transport
equation (1). This correspondence also exists in plain nets (neural networks without
residual shortcuts). It is summarized in Table 1 and illustrated in Figure 5.
Table 1: Correspondence between neural network and transport equation
Neural Network Transport Equation
layer k time tk
weights W k, bias bk, activation a transport velocity field v(t, x)
output function f terminal value function u(T, ·) = f
prediction map F initial value map u(0, ·)
label y initial value u(0, X) = F (X)
feedforward solving IVP of characteristic equation
prediction solving TVP of transport equation
supervised learning solving inverse problem
Figure 5: Correspondence between neural network and transport equation.
4 Discussions
Claim: The discussions here are not intended to provide solution to any problem in
theory and applications of neural networks, nor be able to answer any concern. They
only present our ideas about some of candidate directions that could be explored.
5
1. In the viewpoint of solving terminal value problem of transport equation, we can
also consider other numerical methods intentionally designed for PDEs, besides of
the method of characteristics. For example, we can increase regularity of solutions
by adding dissipative terms to the transport equation.
2. The training of neural networks could be considered as solving inverse problem
of transport equation. It means that both initial value (on samples) and terminal
value are posed. The task is to find a transport velocity field (depending on time)
that transports the initial value to the terminal value. It will be made clearer in
the rest parts of this series of notes.
3. The correspondence provides one way to partially explain why deep is good for
neural networks. Usually, in order to learn a good prediction map, the dataset
needs to be distorted significantly. But the distortion provided by each layer
is very limited. So it needs more layers to accomplish the total distortion. In
the language of differential equations, deep means fine partition and small time
step size. It allows the discretization to be more smooth and regular, such that
each layer makes only a small progress. At the same time, the initial and terminal
conditions can get a better match through the solution, hence the learnt prediction
map is more accurate and can generalize better.
Acknowledgement
Zhen Li would like to show his gratitude to the support of professors Yuan Yao and
Yang Wang from the Department of Mathematics, HKUST.
References
He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image
recognition. In Computer Vision and Pattern Recognition (CVPR), 2016 IEEE
Conference on.
6
