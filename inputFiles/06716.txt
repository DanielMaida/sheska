 1 
What caused what? An irreducible account of actual causation. 
Larissa Albantakis1,*, William Marshall1, Erik Hoel2, Giulio Tononi1,* 
 
1Department of Psychiatry, Wisconsin Institute for Sleep and Consciousness, University of Wisconsin-Madison, WI, USA 
2Department of Biological Sciences, Columbia University, New York, NY, USA 
*Corresponding authors: albantakis@wisc.edu, gtononi@wisc.edu 
 
Actual causation is concerned with the question “what caused what?”. Consider a transition between two subsequent 
observations within a system of elements. Even under perfect knowledge of the system, a straightforward answer to 
this question may not be available. Counterfactual accounts of actual causation based on graphical models, paired 
with system interventions, have demonstrated initial success in addressing specific problem cases. We present a 
formal account of actual causation, applicable to discrete dynamical systems of interacting elements, that considers 
all counterfactual states of a state transition from ! ? 1 to !. Within such a transition, causal links are considered 
from two complementary points of view: we can ask if any occurrence at time ! has an actual cause at ! ? 1, but 
also if any occurrence at time ! ? 1 has an actual effect at !. We address the problem of identifying such actual 
causes and actual effects in a principled manner by starting from a set of basic requirements for causation (existence, 
composition, information, integration, and exclusion). We present a formal framework to implement these 
requirements based on system manipulations and partitions. This framework is used to provide a complete causal 
account of the transition by identifying and quantifying the strength of all actual causes and effects linking two 
occurrences. Finally, we examine several exemplary cases and paradoxes of causation and show that they can be 
illuminated by the proposed framework for quantifying actual causation. 
 
1. Introduction 
The nature of cause and effect has been much debated 
in both philosophy and the sciences. To date, there is no 
single widely accepted account of causation, and the 
various sciences focus on different aspects of the issue 
(Illari et al., 2011). In physics, no formal notion of 
causation seems even required to describe the dynamical 
evolution of a physical system by a set of mathematical 
laws. At most, the notion of causation is reduced to the 
most basic requirement that causes must precede and be 
able to influence their effects. Accordingly, physics restricts 
causes to occurrences within the past light cone of their 
effects—no further constraints are imposed as to “what 
caused what”. However, moving beyond fundamental 
physics and its reliance on laws of nature, the search for 
causal mechanisms behind observed phenomena is an 
important goal of scientific inquiry.  
In this context, two broad aspects of causation can be 
distinguished: Natural, health, and social sciences are 
typically concerned with general (or type) causation—the 
question whether the type of occurrence A generally 
“brings about” the type of occurrence B, e.g., whether stress 
causes illness. By contrast, computer science, philosophy, 
and the law, tend to focus on the underlying notion of 
actual (or token) causation—the question “what caused 
what” given a specific occurrence A followed by a specific 
occurrence B. For example, Anne was hurrying to an 
important appointment on a slippery road, wearing high-
heels; she fell and broke her leg. Did the fact that she was 
wearing high-heels cause her injury, or was it the slippery 
road, her running, or any combination of the above? Even 
with perfect knowledge of all circumstances, the prior 
system state, and the outcome, there often is no 
straightforward answer to the “what caused what” question. 
This has been demonstrated by a long list of controversial 
examples conceived, analyzed, and debated primarily by 
philosophers (Lewis, 1986; Pearl, 2000; Paul and Hall, 
2013; Halpern, 2016). 
During the last decades, a number of attempts to 
operationalize the notion of causation and to give it a 
formal description have been developed, most notably in 
computer science, probability theory, statistics (Good, 
1961; Suppes, 1970; Pearl, 1988, 2000), and neuroscience, 
e.g., (Tononi et al., 1999). Important advances have been 
made in providing criteria for distinguishing causal 
relations from mere correlations. This distinction is 
important for controlling a system via interventions, since 
in general it is not possible to predict how a system will 
react to perturbation purely from observed correlations1. In 
other words, merely observing that occurrence A is reliably 
followed by occurrence B is insufficient to establish 
whether the relation between A and B is causal or not. 
Leaving aside metaphysical accounts of the nature of 
causation (see (Paul and Hall, 2013, Chapter 2), for an 
overview), formal frameworks of causal analysis invariably 
rely on the notion of counterfactuals (Lewis, 1973; Pearl, 
2000; Woodward, 2004; Yablo, 2004)—alternative 
                                                            
1 For example, observing a positive correlation between the 
number of predator species A and B, both eating rabbits, 
does not imply that species A will decrease if species B is 
diminished by intervention. The opposite may be the case 
(because now there is more rabbits for species A). 
 2 
occurrences “counter to fact”2. Graphical methods paired 
with system interventions (Pearl, 2000) have proven 
especially valuable for developing causal explanations. 
Given a causal network that represents how the state of 
each variable depends on other system variables via a 
“structural equation” (Pearl, 2000), the effects of 
interventions imposed from outside the network can be 
evaluated by setting certain variables to a specific value. 
This operation has been formalized by Pearl, who 
introduced the “do-operator”, $%(' = )), which signifies 
that a subset of system variables ' has been actively set 
into state ) rather than being passively observed in this state 
(Pearl, 2000). In contrast to statistical, ‘Bayesian’ networks, 
causal networks, are thus characterized through 
dependencies identified by intervention, not observation. 
Because statistical dependence does not imply causal 
dependence, and vice versa (Ay and Polani, 2008), the 
conditional probability of occurrence B after observing 
occurrence A, + , -  may differ from the probability of 
occurrence B after enforcing A + , $%(-) .  
The causal networks approach has also been applied to 
the case of actual causation (Pearl, 2000; Halpern and 
Pearl, 2005; Halpern, 2015), where statistical inferences 
about populations or processes rather than individual 
occurrences cannot be meaningfully applied. In this 
framework, system interventions can be used to evaluate 
whether and to what extent an occurrence was necessary or 
sufficient for a subsequent occurrence by assessing 
counterfactuals. Approaches differ primarily depending on 
which sets of variables are held fixed and which are 
allowed to vary during the analysis—in other words—
which counterfactual states are taken into account. While 
promising results have been obtained in specific cases, no 
single proposal to date has characterized actual causation in 
a universally satisfying manner (Paul and Hall, 2013; 
Halpern, 2016).  
Below we present a formal account of actual causation 
that considers all counterfactual states, which allows us to 
express causal analysis in probabilistic, informational 
terms. We aim at providing a causal account of “what 
caused what”, given a transition ./01 = 2/01 ? 4/ = 5/  
(“2/01 precedes 5/”) between two subsequent observations 
within a discrete dynamical system 6 with ./01, 4/ ? 6, 
constituted of interacting elements (Fig. 1). Within such a 
transition, unlike previous accounts of actual causation 
(e.g., (Pearl, 2000; Paul and Hall, 2013; Halpern, 2016), but 
see (Chajewska and Halpern, 1997)), causal links are 
considered from the perspective of both causes and effects. 
Specifically, we ask both if an occurrence 9/ = :/ ? 4/ =
                                                            
2 Note that counterfactuals here strictly refer to possible 
states within the system’s state space other than the actual 
one and not to abstract notions such as other “possible 
worlds” as in (Lewis, 1973), see also (Pearl, 2000, Chapter 
7). 
5/ at time ! has an actual cause at ! ? 1; and also, if an 
occurrence '/01 = )/01 ? ./01 = 2/01  at time ! ? 1 has 
an actual effect at !. We identify such actual causes and 
actual effects and demonstrate that both perspectives are 
essential to give a complete causal account of the transition 
./01 = 2/01 ? 4/ = 5/. Moreover, by considering all 
possible counterfactual states, we quantify the strength of 
causal links between occurrences and their actual 
cause/effect in informational terms.  
The analysis is based on five principles that 
characterize potential causation—the causal constraints 
exerted by a mechanism in a given state—namely intrinsic 
existence, composition, information, integration, and 
exclusion, identified in the context of integrated 
information theory (IIT; Oizumi et al., 2014; Albantakis 
and Tononi, 2015). Originally developed as a theory of 
consciousness (Tononi, 2015; Tononi et al., 2016), IIT 
provides the tools to identify all potential causes and effects 
within a discrete dynamical system in a state-dependent 
manner and to quantify their power. Here we employ this 
very set of tools to identify actual causes and effects and to 
quantify their strength. This formal framework for actual 
causation is based on first principles and is applicable to a 
wide range of systems, whether deterministic and 
probabilistic, with feedforward and recurrent architectures. 
In the following, we will first formally describe the 
proposed causal analysis. We then demonstrate its utility on 
a set of examples, which illustrate the need to characterize 
both causes and effects, the fact that causation can be 
compositional, and the importance of identifying 
irreducible causes and effects for obtaining a complete 
causal account. Finally, we illustrate several prominent 
paradoxical cases from the actual causation literature, 
including overdetermination and prevention. 
 
2. Theory 
IIT’s approach to potential causation is concerned with 
the intrinsic cause-effect power of a physical system 
(intrinsic existence). The IIT formalism starts from a 
discrete dynamical system 6 in its current state ;/ and asks 
how the system’s elements, alone and in combination 
(composition), constrain the potential past and future states 
of the system (information), and whether they do so above 
and beyond their parts (integration). Using the measures 
developed within IIT, for any subset < = =/ ? 6 = ;/, we 
can identify the maximally irreducible set of potential 
causes and effects within the system (exclusion), and 
quantify its irreducible cause-effect power (integrated 
information >) (Oizumi et al., 2014; Tononi, 2015). 
Translated to the case of actual causation, IIT’s five 
principles are as follows: 
(1) Existence: Causes and effect are actual. The actual 
cause of an occurrence 9/ = :/ must actually have 
happened at ! ? 1, and the actual effect of an occurrence 
 3 
'/01 = )/01 must actually have happened at !, in a system 
that actually exists. An actual cause is thus an occurrence at 
! ? 1 and an actual effect an occurrence at ! as observed in 
that system. 3 
(2) Composition: Causes and effects are structured. 
Any subset of an observation can be an occurrence with its 
own actual cause or effect. Likewise, a subset of an actual 
cause/effect can itself also be a separate actual cause/effect 
of a different occurrence within the transition.  
(3) Information: Causes and effects are specific. An 
occurrence must increase the probability that its actual 
cause/effect occurred compared to its expected probability 
when the occurrence is not specified (that is, when all 
possible states of the occurrence are considered with equal 
probability). In other words, an actual cause/effect must be 
distinguishable from noise. 
(4) Integration: Causes and effects are irreducible. 
Only irreducible occurrences can have actual causes or 
effects. An occurrence must determine its actual 
cause/effect irreducibly, above and beyond its parts.  
(5) Exclusion: Causes and effects are definite. An 
occurrence can have at most one actual cause/effect, which 
is the smallest set of elements whose state is most 
irreducibly determined by the occurrence.  
 
Fig. 1: Example physical system. (A) A discrete 
dynamical system constituted of 2 interacting elements: an 
OR- and AND-logic gate. Arrows denote connections 
between these elements. (B) The same system unfolded over 
two consecutive time-steps can be represented as a 
directed, acyclic graph (DAG). (C) The system described by 
its entire set of transition probabilities. Since this particular 
system is deterministic all transitions have a probability of 
either p = 0 or p = 1. 
                                                            
3 Note that this is also a requirement in the original and 
modified versions of the Halpern and Pearl account of 
actual causation (Halpern and Pearl, 2005; Halpern, 2015). 
It corresponds to their first clause (“AC1”), that for ' =
)/01 to be an actual cause of 9 = :/ both must actually 
happen in the first place. 
To apply these principles to the analysis of actual 
causation, we assume a discrete dynamical system 6, 
constituted of ? interacting elements <@ with A = 1, … , ? 
(Fig. 1A). Each element must have at least two internal 
states, which can be observed and manipulated, and is 
equipped with a Markovian input-output function C@ that 
determines the element’s output state depending only on the 
previous system state: =@,/ = C@(6/01 = ;/01). This means 
that all elements are conditionally independent given the 
past state ;/01 of the system and any system transition 
6/01 = ;/01 ? 6/ = ;/ can be represented within a 
directed, acyclic, causal network (Fig. 1B) (Pearl, 2000; Ay 
and Polani, 2008; Halpern, 2015). Alternatively, 6 is fully 
described by its transition probabilities (Fig. 1C): 
 + 6/ = ;/ 6/01 = ;/01 =	 
= + <@,/ = =@,/|	6/01 = ;/01 ,				?	;/, ;/01.
H
@I1 	 			( 1 )	
In contrast to other accounts of (actual) causation, we 
explicitly interpret 6 as a physical system with interacting 
physical elements, as opposed to abstract variables, like, for 
example, a ‘desire for chocolate’. Examples of physical 
systems include a set of neurons in the brain or logic gates 
in a computer. In reality, 6 will always be an (approximate) 
causal model of a given physical system, constructed from 
interventional experiments4. However, our objective is to 
formulate a quantitative account of actual causation, 
without confounding issues due to incomplete knowledge, 
such as estimation biases of probabilities from finite 
sampling, or latent variables. For this reason, we assume 
full knowledge about the physical system, that is, we 
assume a perfect causal model that is equivalent to it. For 
simplicity, without loss of generality, we restrict our 
analysis to systems of binary elements. As the final 
prerequisites, we define an observation as the state of a 
subset of elements within system 6 at a particular instant, 
e.g. ./01 = 2/01, or  4/ = 5/  with  ./01, 4/ ? 6. To 
simplify notation, observations are subsequently denoted by 
their state only, e.g., 5/ instead of 4/ = 5/.  
2.1 Actual causes and actual effects   
The objective of this section is to introduce the notion 
of a causal account for a transition of interest 2/01 ? 5/ as 
the set of all causal links within the transition. A causal link 
exists between an occurrence )/01 ? 2/01 and its actual 
effect at !, or an occurrence :/ ? 5/ and its actual cause at 
! ? 1.  
                                                            
4 The transition probabilities can, in principle, be 
determined, by perturbing the system into all possible states 
and observing the resulting transitions. Alternatively, the 
causal network can be constructed by experimentally 
identifying the input-output function of each element. 
Crucially, merely observing the system without 
experimental manipulation is insufficient to identify causal 
relationships in most situations. 
OR AND
A
OR
AND AND
OR
C
t–1 t
1 0 0 0
0 1 0 0
0 1 0 0
0 0 0 1
TPM
B
Physical	system
Causal	DAG
pa
st
	st
at
e	
t–
1
current	state	t
OR AND
A
OR
AND AND
OR
B
t–1 t
ON OFF
1 0 0 0
0 1 0 0
0 1 0 0
0 0 0 1
TPM
C
Physical	system	in	a	sta e
Transition	!"#$ ? &"
pa
st
	st
at
e	
t–
1
current	state	t
ON OFF
 4 
 
Fig. 2: Example transition. (A) The example system of Fig. 1 transitions from state {10} ? {10}. (B) Example effect 
repertoires indicating how the occurrence  )/01 = {OR = 1} constrains the probability distributions of 9/ = {OR} and 9/ = 
{OR, AND}, respectively. (C) Example cause repertoires indicating how the occurrences  :/ = {OR = 1} and :/ = {OR, 
AND = 10} respectively constrain the probability distribution of '/01 = {OR}. 
 
Below, we will formally define causal link, actual 
cause, actual effect, and causal account following the five 
principles outlined above: existence, composition, 
information, integration, and exclusion.  
 
Existence. A transition of interest 2/01 ? 5/ for a 
causal account is defined by a pair {./01 = 2/01; 4/ = 5/} 
of subsequent observations within a system 6 (note that . 
and 4 can contain the same elements), consistent with the 
system’s transition probabilities. 
Elements in 6\. at ! ? 1 are considered exogenous 
and are treated as background conditions, meaning that 
their states are held fixed during the causal analysis. Bold 
letters are used for the two observations 2/01; 5/ 	that 
define the transition 2/01 ? 5/, while )/01 or :/ denote 
occurrences within the transition, i.e. subsets of 2/01 or 5/. 
As a first example, we consider the transition 2/01 ? 5/ 
shown in Fig. 2A of the physical system 6 depicted in Fig. 
1. Here,  2/01 = 5/ = {OR, AND = 10}, consistent with 
the system’s transition probabilities (Fig. 1C).  
Composition. For a causal account of the transition 
2/01 ? 5/, we are interested in determining all causal links 
between occurrences )/01 and their actual effects, and 
occurrences :/ and their actual causes. To this end, we 
consider the power set of 2/01 as occurrences that could 
have actual effects and the power set of 5/ as occurrences 
that could have actual causes. Multi-element occurrences 
are termed “high-order” occurrences in the following. 
Given a particular occurrence )/01, we test the power set of 
5/ to identify the actual effect of )/01. Given a particular 
occurrence	:/, we test the power set of 2/01 to identify the 
actual cause of :/.  
In the example transition shown in Fig. 2A, causal 
links may thus exist between the occurrences {OR = 1}, 
{AND = 0}, and {OR, AND = 10} at ! ? 1 and !, and any 
of their possible effects or causes {OR = 1}, {AND = 0}, 
and {OR, AND = 10} at ! and ! ? 1, respectively. 
Information. In order to identify a causal link between 
two occurrences )/01 and :/, it is not enough to consider 
the actual states )/01 and :/; other, hypothetical states 
(counterfactuals) must also be taken into account (Lewis, 
1973; Pearl, 2000). Our objective here is to quantify the 
strength of causal links rigorously, beyond merely 
identifying them. By taking all possible counterfactuals into 
account (Eqn. 1), we can express causal strength utilizing 
interventional conditional probabilities and probability 
distributions (Tononi et al., 1999; Ay and Polani, 2008; 
Balduzzi and Tononi, 2008; Hoel et al., 2013; Oizumi et al., 
2014). To this end, we assume a maximum entropy 
distribution over the possible states of ./01 throughout the 
analysis, i.e. + ./01 = 2 = ?. 01	?2 ? ?P, where ?P 
denotes the set of all possible states of . (see Discussion 
4.1). This ensures that causal strength reflects constraints 
due to the occurrence itself, rather than a predisposition of 
elements to be in one state rather than another. Roughly 
speaking, we assess how much of a difference an 
occurrence )/01 (or :/) makes to the probability of a 
candidate effect (or cause) compared to all possible states 
of '/01 (or 9/). 
To identify actual causes and effects and to quantify 
causal links, we start from the notions of a cause- and effect 
repertoire—the interventional conditional probability 
distributions over the potential past and future states of the 
system (Oizumi et al., 2014; Albantakis and Tononi, 2015), 
see also (Tononi, 2015; Marshall et al., 2016b) for a general 
mathematical definition. The effect repertoire specifies how 
an occurrence )/01 constrains the potential future states of a 
set of elements 9/ (potential effects). Likewise, the cause 
repertoire specifies how an occurrence :/ constrains the 
potential past states of a set of elements '/01 (potential 
causes) (Fig. 2B, C). 
 The effect repertoire of an occurrence )/01 over a 
single element 9@,/ is a conditional probability distribution 
over 9@,/  given )/01  in the causal network. The 
complementary cause repertoire of a single element 
OR
AND AND
OR
t–1 t
A Transition	!"#$ ? &"
candidate	effect candidate	cause
OR
AND AND
OR
AND
OR
AND
OR
1 0 undetermined
B
c
OR
AND AND
OR
Example	effect	repertoires
( )" = OR -"#$ = {OR = 1}
C Example	cause	repertoires
AND
OR
AND
OR
AND
OROR OR OR
( )" = {OR, AND} -"#$ = {OR = 1} ( 5"#$ = OR 6" = {OR = 1} ( 5"#$ = OR 6" = {OR, AND = 10}
 5 
occurrence at time !, 9@,/ = :@,/, is a conditional probability 
distribution over a set of elements '/01 given :@,/. These 
and all subsequently defined probabilities and probability 
distributions can be derived from the system’s transition 
probabilities (Eqn. 1) by conditioning on the state of the 
occurrence and marginalizing across the states of 
undetermined elements .Q0R\'/01 and 4Q\9/. Explicitly, 
for a single element occurrence 9@,/ the effect repertoire is: 
S 9@,/ )/01 =
=
1
|T.\U|
+ 9@,/ $% )/01, ./01\'/01 = V
W?X.\Y
,		
and the corresponding cause repertoire, using Bayes’ rule, 
is:  
S '/01 :@,/ =
Z :@,/ $% '/01, ./01\'/01 = V[?\.\Y
Z :@,/ $% . = V[?\.
.		
Cause- and effect repertoires capture all the constraints 
that the occurrence )/01 exerts on a set of elements 9/, and 
vice versa for :/ and '/01. However, in the general case of 
multiple elements in 9/, the conditional probability 
distribution + 9/ )/01  not only contains dependencies of 9/ 
on )/01, but also correlations between elements in 9/ due to 
common inputs from elements in ./01\'/01, which should 
not be counted as constraints due to )/01. To discount such 
correlations, we define the effect repertoire over a set of 
elements 9/ as: 
S 9/ )/01 = S 9@,/ )/01@ .		 	 	 	    ( 2 ) 
In the same manner, we define the cause repertoire as: 
S '/01 :/ =
1
]
S '/01 :@,/@ ,			 	 	 			( 3 ) 
with		
^ = 	 S '/01 = ) :@,/@_?XY
.	 	
In summary, the effect- and cause repertoires 
S 9/ )/01  and S '/01 :/ , respectively, are interventional 
conditional probability distributions that specify constraints 
due to an occurrence on the potential past and future states 
of a set of system elements, enforcing a maximum entropy 
distribution over the possible states of ./01 and eliminating 
correlative constraints due to common inputs5.  
In the context of actual causation, the question of 
interest is “what caused what” given knowledge of “what 
actually happened” over two consecutive time steps. To this 
end, given the actual state of 9/, we define an effect ratio `a 
                                                            
5 In general, S 9/ )/01 ? + 9/ )/01 . It is only in the 
special case that '/01 already includes all inputs (all 
parents) of 9/, or determines 9/ completely, that S 9/ )/01  
is equivalent to + 9/ )/01  because of the conditional 
independence of all 9@,/ (see Eqn. 1). 
for the occurrence )/01 and a subsequent occurrence :/ 
within the transition 2/01 ? 5/, as: 
`a()/01, 	:/) = logf S :/ )/01 S :	/ ,	 ( 4 ) 
where S :	/ = S :@,/@ ,	  and 
S :@,/ =
1
|g.|
+ :@,/ $% ./01 = )_?g. . In words, the 
effect ratio `a is the probability of an occurrence at ! when 
constrained by an occurrence at ! ? 1, divided by its 
probability when unconstrained. The unconstrained effect 
repertoire S :@,/  is the probability of 9@,/ = :@,/ given a 
uniform distribution of all possible input states to element 
9@,/. 
 Similarly, given the actual state of '/01, we define the 
cause ratio `h for the occurrence :/ and a prior occurrence 
)/01 within the transition 2/01 ? 5/, as: 
`h )/01, 	:/ = logf S )/01 :/ S )/01 .		 			( 5 )	
In words, the cause ratio `h is the probability of an 
occurrence at ! ? 1 when constrained by an occurrence at !, 
divided by its probability when unconstrained. The 
unconstrained cause repertoire S )/01 = 	 ?. 01 is a 
uniform distribution over the possible states of ./01. Using 
base 2 logarithms in Eqn. 4 and 5 sets the unit of ` to bits. 
A positive effect ratio `a )/01, 	:/ > 0 means that the 
occurrence )/01 increases the likelihood of the particular 
occurrence :/, compared to its probability in the case of 
maximal uncertainty about the state of ./01. The 
occurrence )/01 thus makes a positive difference in 
bringing about :/. This is a necessary, but not sufficient 
condition for 	:/ to be an actual effect of )/01. Similarly, a 
positive cause ratio `h )/01, 	:/ > 0 means that the 
occurrence 	:/ increases the likelihood of the particular 
occurrence )/01 having happened, compared to maximum 
entropy ?. 01. Again, this is a necessary, but not sufficient 
condition for )/01 to be an actual cause of 	:/. Both `a and 
`h can be interpreted as measures of the information that 
one occurrence specifies about the other.  
In Fig. 3A, for example, the occurrence )/01 = {OR = 
1} raises the probability of 	:/ = {OR = 1}, and vice versa 
(Fig. 3B), with `a )/01, 	:/ = `h )/01, 	:/ = 	0.415	bits6. 
By contrast, the occurrence )/01 = {OR = 1} lowers the 
probability of occurrence 	:/ = {AND = 0} and also of the 
2nd-order occurrence 	:/ = {OR, AND = 10}. Thus, 
neither	:/ = {AND = 0} nor 	:/ = {OR, AND = 10} can be 
actual effects of )/01 = {OR = 1}. Likewise, the 
occurrence 	:/ = {OR = 1} lowers the probability of )/01 = 
{AND = 0}, which can thus not be the actual cause of 	:/. 
                                                            
6 In general, the cause and effect ratios for an occurrence 
pair ()/01,	:/) are not identical. In other words, there is no 
equivalent of Bayes’ rule for  S, as generally  
S :	/ 	S )/01 :/ ? S )/01 	S :/ )/01 , since Eqn 2 and 3 
do not follow the standard definitions for conditional 
probability distributions.  
 6 
 
Fig. 3: Information and Integration. (A, B) Example effect and cause ratios. The state that actually occurred is selected 
from the effect or cause repertoire (green is used for effects, blue for causes). Its probability is compared to the probability of 
the same state without constraints (overlaid distributions without fill). (C, D) Irreducible effect and cause ratios. The 
probability of the actual state in the effect or cause repertoire is compared against its probability in the partitioned effect or 
cause repertoire (overlaid distributions without fill). Of all 2nd–order occurrences shown, only :/ = {OR, AND = 10} 
irreducibly constrains  )/01 = {OR, AND = 10}.  
   
Integration. The irreducibility of a candidate causal 
link is tested by ‘disintegrating’ it into independent parts, 
that is, by rendering the connections between the parts 
ineffective (Balduzzi and Tononi, 2008; Janzing et al., 
2013; Oizumi et al., 2014; Albantakis and Tononi, 2015). 
Practically, this is accomplished by taking the product of 
the marginal distributions corresponding to each part.  
In the case of a candidate causal link from )/01 to :/ 
()/01 ? :/), the effect repertoire under the disintegrating 
partition r is defined as: 
S 9/ )/01 s =	 
S 9Z,/ )Z,/01Z?{1…t} ×	S 9/\ 9Z,/Z?{1…t} ,	 			( 6 )	
where the occurrence )/01 is partitioned into 1 ? w ?
|)/01| parts )Z,/01, + = 1…w. Each of these )Z,/01 is then 
paired with a different, potentially empty, non-overlapping 
set 9Z,/ ? 9/, with the additional condition that if w = 1 
then 91,/ = ?. The product of the corresponding w effect 
repertoires is multiplied by the unconditioned effect 
repertoire of the remaining elements 9/\ 9Z,/Z?{1…t} . In 
case there are no such left-over elements of 9/, 
S 9/\ 9Z,/Z?{1…t} = 	S ? = 1. Defined in this way, the 
disintegrating partition	r guarantees that the partitioned 
occurrence could not have an irreducible effect on any 
subset of 9/, as any constraints that )/01 might have on 9/ 
are split into independent parts. 
 In the same way, the cause repertoire under a 
disintegrating partition r of the candidate causal link from 
:/ to )/01 ()/01 ? :/) is defined as: 
S '/01 :/ s = 	 S 'Z,/01 :Z,/Z?{1…t} ×
S '/01\ 'Z,/01Z?{1…t} ,	 	 	 	 	 			(	7	)	
where now the occurrence :/ is partitioned into 1 ? 	w ?
|:/|	parts :Z,/, + = 1…w. Each of these :Z,/ is then paired 
with a different, potentially empty, non-overlapping set 
'Z,/01 ? '/01, with '1,/01	 = ? if w = 1. The product of 
the corresponding w cause repertoires is multiplied by the 
unconditioned cause repertoire of the remaining elements 
'/01\ 'Z,/01Z?{1…t} , again with 	S ? = 1. Defined in 
this way, the disintegrating partition	r guarantees that the 
partitioned occurrence could not have an irreducible cause 
in any subset of '/01, as any constraints that :/ might have 
on '/01 are split into independent parts.  
MIP
candidate	effect
OR
AND AND
OR
A
OR
AND AND
OR OR
AND AND
OR
Example	effect	ratios	for	!"#$ = {OR	=	1}		 B Example	cause	ratios	for	&" = {OR	=	1}		
( 6" -"#$
((6")	
6"={OR=1}
:; !<#=,	&< = ?@AB
=
C.EF
= 0.415 bits
OR AND
OR
AND
6"={AND=0} 6"={OR,	AND=10}
GH -"#$,	6" < 0 bits GH -"#$,	6" < 0 bits
candidate	cause
AND
OR
AND
OR
-"={OR=1} -"={AND=0} -"={OR,	AND=10}
:J !<#=,	&< = ?@AB
C.KK
C.F
= 0.415 bits
OR AND
OR
AND
GL -"#$,	6" < 0 bits GL -"#$,	6" = logP
Q.RR
Q.PS
= 0.415 bits
AND
OR
AND
OR
AND
OR
AND
OR
C Example	irr.	effect	ratios	for	!"#$ = {OR,AND	=	10}		 D Example	irr.	cause	ratios	for	&" = {OR,AND	=	10}		
( -"#$ 6"
((-"#$)	
candidate	effect
OR
AND AND
OR OR
AND AND
OR OR
AND AND
OR
( 6" -"#$
( 6" -"#$ TUV
6"={OR=1}
WH -"#$,	6" = 0 bits
OR AND
OR
AND
6"={AND=0} 6"={OR,	AND=10}
WH -"#$,	6" = 0 bits WH -"#$,	6" = 0 bits
candidate	cause
AND
OR
AND
OR
-"={OR=1} -"={AND=0} -"={OR,	AND=10}
WL -"#$,	6" = 0 bits
OR AND
OR
AND
WL -"#$,	6" = 0 bits XJ !<#=,	&< = ?@AB
C.F
C.YY
= 0.=E bits
AND
OR
AND
OR
AND
OR
AND
OR
( -"#$ 6"
( -"#$ 6" TUV	
MIP MIP MIP MIP MIP
 7 
Since single element occurrences cannot be further 
partitioned, w has to be 1. In that case, only a complete 
partition between )/01 and :/ disintegrates the occurrence 
and Eqn. 6 and 7 thus reduce to S :	/  and S )	/01 , 
respectively.  
By comparing `a )/01, 	:/  against `a )/01, 	:/ s, we 
can measure the difference a partition r makes to the effect 
ratio of the occurrence )/01 on a candidate effect 	:/, and 
similarly for `h )/01, 	:/ . However, there are many ways 
to partition an occurrence: we define r()/01 ? :/) as the 
set of all possible disintegrating partitions of an occurrence 
)/01 and its candidate effect :/, and r()/01 ? :/) as the 
set of all possible disintegrating partitions of an occurrence 
:/ and its candidate cause )/01. A measure of irreducibility 
requires choosing the partition r that makes the least 
difference to the ratio, which is denoted the minimum 
information partition (MIP) (Oizumi et al., 2014; 
Albantakis and Tononi, 2015), respectively: 
MIP )/01 ? :/ = 	 argmins	?s _ÇÉÑ?ÖÇ 	`a )/01, :/
? `a )/01, :/ s 		
and 
MIP )/01 ? :/ = 	 argmins	?s _ÇÉÑ?ÖÇ 	 `a )/01, :/
? `a )/01, :/ s .		
Note that the MIP will typically depend on the specific 
transition 2/01 ? 5/, however we omit this dependence 
from notation for simplicity, as it will be clear from the 
context which transition is being considered.  
Taken together, we define the irreducible effect ratio 
Üa as the difference between the intact ratio and the ratio 
under the MIP: 
Üa()/01,	:/) = `a )/01, 	:/ ? `a )/01, 	:/ MIP	
= logf S :/ )/01 S :/ )/01 MIP ,	 	 	 			( 8 )	
and the irreducible cause ratio Üh: 
Üh()/01,	:/) = `h )/01, 	:/ ? `h )/01, 	:/ MIP	
= logf S )/01 :/ S )/01 :/ MIP .	 	 	 			( 9 )	
A positive irreducible effect ratio (Üa()/01,	:/) > 0) 
signifies that the occurrence )/01 has an irreducible effect 
on 	:/, which is necessary but not sufficient for 	:/ to be an 
actual effect of )/01. Likewise, a positive irreducible cause 
ratio (Üh()/01,	:/) > 0) means that :/ has an irreducible 
cause in )/01, which is a necessary but not sufficient 
condition for )/01 to be an actual cause of :/. Note that for 
single element occurrences Ü()/01,	:/) = `()/01,	:/) , 
because only the complete partition disintegrates the 
occurrence. 
In our example transition, the occurrence )/01 = {OR, 
AND = 10} (Fig. 3C) is reducible. This is because )/01 = 
{OR = 1} is sufficient to determine the occurrence 
:/ = {OR = 1} with probability 1.0 and )/01 = {AND = 0} 
is sufficient to determine the occurrence :/ = {AND = 0} 
with probability 1.0, thus there is nothing to be gained by 
considering the two elements together as a 2nd-order 
occurrence at ! ? 1. By contrast, the occurrence 	:/ = {OR, 
AND = 10} determines the particular past state of )/01 = 
{OR, AND = 10} with higher probability than the two 
single occurrences :/ = {OR = 1} and :/ =	{AND = 0} 
taken separately (Fig. 3D, right). Thus, the 2nd-order 
occurrence	:/ = {OR, AND = 10} is irreducible 
(Üh()/01,	:/) = 0.17 bits, with )/01 = {OR, AND = 10}) 
(see Discussion 4.4). 
Exclusion: For a particular occurrence )/01, the power 
set of 5/ has to be considered as candidate effects, and the 
power set of 2/01 has to be considered as candidate causes 
of a particular occurrence :/. It is possible that an 
occurrence )/01 has multiple candidate effects :/ for which 
Üa )/01, :/ > 	0, as well as that an occurrence 	:/ has 
multiple candidate causes )/01 for which Üh )/01, :/ > 	0. 
We define the irreducibility of an occurrence as its 
maximum irreducible effect (or cause) ratio over all 
possible candidate effects (or causes), 
Üa
áàâ )/01 = max
ÖÇ?5Q
Üa()/01, :/),	
and 
Üh
áàâ :/ = max
_ÇÉÑ?2QÉR
Üh 	)/01, :/ .	
If Üáàâ = 0, then the occurrence is said to be reducible. 
The exclusion principle enforces that every occurrence 
can at most have one actual cause and one actual effect, 
which is the smallest set of elements whose state is most 
irreducibly determined by the occurrence.7 For example, 
:/ = {OR = 1} has two candidate causes )/01 = {OR = 1} 
and )/01 = {OR, AND = 10} with  Üh )/01, :/ =
Üh
áàâ :/ = 0.415	bits. In this case, )/01 = {OR = 1} is 
the actual cause of :/ = {OR = 1}. The exclusion principle 
thus avoids causal overdetermination, which arises from 
counting multiple causes or effects for a single occurrence. 
Note, however, that in a perfectly symmetric system 
indeterminism about the actual cause or effect is possible 
(see Results).  
Having established the above causal principles, we are 
now in a position to formally define the actual cause and 
the actual effect of an occurrence: 
 
Definition 2.1.a) The actual cause of an irreducible 
occurrence :/ ? 5/ within the transition 2/01 ? 5/ is the 
minimal set of elements )/01?  for which the irreducible 
cause ratio of :/ is maximal: 
)/01
? :/ = argmin
_?2? ÖÇ
|)|,	where	
                                                            
7 The exclusion requirement is related to the third clause 
(“AC3”) in the original and modified versions of actual 
causation as proposed by (Halpern and Pearl, 2005) and 
(Halpern, 2015), which states that the actual cause must be 
minimal, meaning that no subset can also satisfy the 
conditions for being an actual cause. 
 8 
2? :/ = )/01 ? 2/01 Üh )/01, :/ = Üh
áàâ :/ }.		
 
Definition 2.1.b) The actual effect of an irreducible 
occurrence )/01 ? 2/01 within the transition 2/01 ? 5/ is 
the minimal set of elements :/? for which the irreducible 
effect ratio of )/01 is maximal: 
:/
? )/01 = argmin
Ö?5? _ÇÉÑ
|:|,		where	
5? )/01 = :/ ? 5/	 Üa )/01, :/ = Üa
áàâ )/01 }.		
 
Based on definitions 2.1.a) and b): 
 
Definition 2.2: A causal link within a transition 
2/01 ? 5/ consists of an occurrence )/01 ? 2/01 with an 
actual effect :/?()/01) and its associated maximally 
irreducible effect ratio Üaáàâ )/01 > 0, or an occurrence 
:/ ? 5/ with an actual cause )/01? (:/) and its associated 
maximally irreducible cause ratio Üháàâ :/ > 0: 
)/01 ? :/ = )/01, :/ = :/
?()/), Üa
áàâ )/01 		
or  
)/01 ? :/ = :/, )/01 = )/01
? (:/), Üh
áàâ 	:/ 		 	
The strength of a causal link is determined by its Üaáàâ 
or Üháàâ value. Reducible occurrences (Üaáàâ )/01 = 0 or 
	Üh
áàâ :/ = 	0) cannot form a causal link.   
 
Definition 2.3: A causal account å of a transition 
2/01 ? 5/ is the set of all causal links )/01 ? :/ and 
)/01 ? :/ within the transition. 
 
Under this definition, all actual causes and actual 
effects contribute to the causal account å 2/01 ? 5/ . 
Crucially, the fact that there is a causal link )/01 ? :/ does 
not necessarily imply that the reverse causal link )/01 ? :/ 
exists, and vice versa. In other words, just because :/ is the 
actual effect of )/01, the occurrence )/01 does not have to 
be the actual cause of :/. It is therefore not redundant to 
include both directions in å(2/01 ? 5/), as illustrated by 
examples of overdetermination and prevention in the 
Results section (see also Discussion 4.2). 
Fig. 4 shows the entire causal account of our example 
transition. Intuitively, in this simple example, )/01 = {OR 
= 1} has the actual effect :/ = {OR = 1} and is also the 
actual cause of :/ = {OR = 1}, and the same for )/01 = 
{AND = 0} and 	:/ = {AND = 0}. Nevertheless, there is 
also a causal link )/01 ? :/, between the 2nd-order 
occurrence :/ = {OR, AND = 10} and its actual cause 
)/01 = {OR, AND = 10}, which is irreducible to its parts, 
as shown in Fig. 3D (right). The complementary link, 
however, does not exist, )/01 ? :/,  it is reducible (Fig. 3C, 
right). The causal account å shown in Fig. 4 provides a 
complete causal explanation for “what happened” and 
“what caused what” in the transition 2/01 ? 5/.  
2.2 Irreducibility of the causal account 
The previous section defined the causal account of a 
particular transition 2/01 ? 5/ with ./01, 4/ ? 6. A causal 
account is the set of all causal links within a given 
transition. Occurrences without causes or effects, or links 
that are reducible to their parts are not included in the 
causal account. As with potential causation (Oizumi et al., 
2014; Albantakis and Tononi, 2015), the principle of 
integration can be applied not only to individual causal 
links, but also to the causal account as a whole. That is, 
given a transition, is it best characterized by a single, 
integrated causal account, or by multiple, independent 
causal accounts? In other words, to what extent are the 
observations 2/01, 5/ and the corresponding transition 
2/01 ? 5/ irreducible to their parts?   
To test the irreducibility of observation 2/01 and its set 
of actual effects in 5/, the transition 2/01 ? 5/ is 
partitioned into independent parts in the same way that 
)/01 ? :/ is partitioned when assessing Üa()/01,	:/), 
resulting in a set of possible partitions r(2/01 ? 5/). We 
then define the irreducibility of 2/01 as the total strength of 
actual effects (causal links of the form )/01 ? :/)	lost from 
the causal account å due to the MIP: 
éa 2/01 ? 5Q = Üa
áàâ
	_?Ö	?	å ()) 	?
Üa
áàâ
	_?Ö	?	åèêë ) íìî	 	 	 	 	 (	10	)	
In the same way, the irreducibility of observation 5/ 
and its set of actual causes in 2/01 is defined as the total 
strength of actual causes (causal links of the form )/01 ?
:/) lost from the causal account å due to the MIP: 
éh 2/01 ? 5/ = Üh
áàâ
	_?Ö	?	å (:) 	?
Üh
áàâ
	_?Ö	?	åèêë : íìî	 	 	 	 	 	( 11 )	
where the MIP is again the partition that makes the least 
difference out of all possible partitions r(2/01 ? 5/).  
As for causal links, the irreducibility of a single 
element observation 2/01 or 5/ reduces to Üaáàâ of its one 
actual effect :/, or Üháàâ of its one actual cause )/01, 
respectively. 
By considering the union of possible partitions, 
r 2/01 ? 5/ = 	r(2/01 ? 5/) ? r(2/01 ? 5/), we can 
moreover assess the overall irreducibility of the transition 
2/01 ? 5/. A transition 2/01 ? 5/ is reducible if there is a 
partition r ? r ñó01 ? 5/  such that the total strength of 
causal links in å 2/01 ? 5/  is unaffected by the partition. 
Based on this notion we define the irreducibility of a 
transition 2/01 ? 5/ as: 
é 2/01 ? 5/ = Ü
áàâ( å) ? Üáàâ( åíìî),				( 12 )	
where 
Üáàâ( å) = Üa
áàâ
	_?Ö	?	å ) + Üh
áàâ
	_?Ö	?	å (:)	 is a 
summation over the strength of all causal links in the causal 
account å 2/01 ? 5/ , and the same for the partitioned 
causal account åíìî. 
 9 
Fig. 4 shows the causal account åíìî of the transition 
2/01 ? 5/ with 2/01 = 	5/ = {OR, AND = 10} under its 
MIP into w = 2 parts with  21,/01 = OR = 1 , 2f,/01 =
AND = 1 ,  51,/ = OR = 1 ,  and 5f,/ = AND = 0 ,	with 
é 2/01 ? 5/ = 0.17. This is the causal strength that 
would be lost if we treated 2/01 ? 5/ as two separate 
transitions instead of a single one.  
The measures defined in this section provide the means 
to exhaustively assess “what caused what” in a system 
transition  ;/01 ? ;/, to evaluate more parsimonious causal 
explanations against the complete causal account of ;/01 ?
;/, and to evaluate the strength of specific causal links of 
interest in a particular context (background conditions).  
 
Fig. 4: Irreducible Causal Account. There are two 1st–
order occurrences with actual effects and actual causes. In 
addition, the 2nd–order occurrence :/ = {OR,AND = 10} 
has an actual cause )/01 = {OR,AND = 10}. A partition of 
the transition along the MIP destroys this 2nd–order causal 
link, leading to é = 0.17	bits. 
 
3. Results 
In the following, we will present a series of examples 
to illustrate the quantities and objects defined in the theory 
section and address several dilemmas taken from the 
literature on actual causation. 
3.1 The same transition can have different 
causal accounts: disjunction, conjunction, 
biconditional, and prevention 
Fig. 5 shows 4 different types of logic gates with two 
inputs each, all transitioning from the input state 2/01 = 
{AB = 11} to the output state 5/ = {C = 1}, {D = 1}, {E = 
1} or {F = 1}. From a dynamical point of view, without 
taking the causal structure of the mechanisms into account, 
the same occurrences are happening in all four situations. 
However, analyzing the causal accounts of these transitions 
reveals differences in the number and type of irreducible 
occurrences and their actual causes and effects.  
The first example (Fig. 5A – OR gate), is a case of 
symmetric overdetermination (Pearl, 2000, Chapter 10): 
each input to C would have been sufficient for {C = 1}, yet 
both {A = 1} and {B = 1} occurred at ! ? 1. By contrast, in 
the second example (Fig. 5B – AND gate), both {A = 1} 
and {B = 1} are necessary for {D = 1}. These two examples 
(Fig. 5A and B) are often referred to as the disjunctive and 
conjunctive versions of the ‘forest-fire’ example (Halpern 
and Pearl, 2005; Halpern, 2015, 2016), where lightning 
and/or a match being dropped result in a forest fire. The 
main question is whether, assuming both the lightning and 
the match being dropped occurred, the actual cause of the 
forest fire was a) one of the two, b) either, or c) both 
together. Here we argue that separating actual effects from 
actual causes and acknowledging compositional causes and 
effects can resolve these situations. 
In the disjunctive case (Fig. 5A), each of the inputs to 
C has an actual effect, {A = 1} ? {C = 1} and {B = 1} ?	
{C = 1}, as they raise the probability of {C = 1} compared 
to its unconstrained probability. Yet, by the causal 
exclusion principle, the occurrence {C = 1} can only have 
one actual cause. The link between {C = 1} and {AB = 11} 
is reducible with Üh = 0, so {AB = 11} is not the actual 
cause of {C = 1}. Instead, either {A = 1}, or {B = 1} is the 
actual cause, but not both. Note that which of the two inputs 
it is remains undetermined, since they are perfectly 
symmetric in this example (compare to Results 3.3 below). 
In the conjunctive example (Fig. 5B), each input alone 
has an actual effect, {A = 1} ? {C = 1} and {B = 1} ? {C 
= 1} (with higher strength than in the disjunctive case), but 
here also the 2nd-order occurrence of both inputs together 
has an actual effect, {AB = 11} ? {D = 1}. Thus, there is a 
composition of actual effects. Again, the occurrence {D = 
1} can only have one actual cause; here the 2nd–order cause 
{AB = 11} ? {D = 1}, with Üháàâ = 2.0 (note that {A = 1} 
and {B = 1} are not separate causes, as was concluded 
instead by (Halpern, 2015)). 
The importance of high-order occurrences is 
emphasized by the third example (Fig. 5C), where E is a 
‘logical biconditional’ (an XNOR) of its two inputs. In this 
case, the individual occurrences {A = 1} and {B = 1} by 
themselves make no difference in bringing about {E = 1}; 
their effect ratios are zero. For this reason, they cannot have 
actual effects and cannot be actual causes. Only the 2nd-
order occurrence {AB = 11} specifies {E = 1}, which is its 
actual effect, {AB = 11} ?	{E = 1}. Likewise, {E = 1} only 
specifies the 2nd-order occurrence {AB = 11}, not its parts 
taken separately, thus {AB = 11} ? {E = 1}.   
OR
AND AND
OR
!"#$ =>{OR,6AND6=610}6? &"= {OR,6AND6=610}
WH[\] = 0.415 bits
WH[\] = 0.415 bits
WL[\] = 0.415 bits
WL[\] = 0.415 bits
- ? 6? -? ? 6
AND
OR
AND
OR
OR
AND AND
OR
actual6effect actual6cause
AND
OR
AND
OR
OR
AND AND
OR
AND
OR
AND
OR
g = gL = 0.17 bits
MIP
gH = 0 bits
Causal6account6Z !"#$ ? &"
?W[\]>(   Z) = 1.83 bits
WL[\] = 0.17 bits
 10 
 
Fig. 5: Four dynamically identical transitions can have different causal accounts. Shown are the transitions (top) and 
their respective causal accounts (bottom). The causal links lost through the MIP are indicated in red.  
Note that the causal strength in this example is lower 
than in the case of the AND-gate, since {D = 1} is less 
likely to begin with than {E = 1}. 
The last example (Fig. 5D) demonstrates a case in 
which the transition 2/01 ? 5/  is reducible 
(é 2/01 ? 5/ = 0). In this case, {B = 1} ? {F = 1} and 
{B = 1} ? {F = 1}; however, {A = 1} does not have an 
actual effect and is not an actual cause. This example can be 
seen as a case of prevention: {B = 1} causes {F = 1}, which 
prevents any effect of {A = 1}. In a popular narrative 
accompanying this example, {A = 1} is an assassin putting 
poison in the King’s tea, while a bodyguard administers an 
antidote {B = 1}, preventing the King’s death {F = 1} 
(Halpern, 2016)8. Since {A = 1} does not contribute to any 
causal links, partitioning element A away from B and F, 
which is equivalent to replacing element A by ‘noise’ 
(maximum entropy), does not make a difference to the 
causal account and thus é 2/01 ? 5/ = 0. Note that the 
causal account is state dependent: for a different transition, 
element A may have an actual effect or contribute to an 
actual cause, and the causal account may become 
irreducible.  
Taken together, the above examples demonstrate that 
the causal account and the causal strength of individual 
causal links within the account capture differences in 
sufficiency and necessity of the various occurrences in their 
respective transitions. In particular, not all occurrences at 
! ? 1 with actual effects end up being actual causes of 
occurrences at !.  
                                                            
8 Note however that this causal model is equivalent to an 
OR-gate, as can be seen by switching the state labels of A 
from ‘0’ to ‘1’ and vice versa. The discussed transition 
would correspond to the case of one input to the OR-gate 
being ‘1’ and the other ‘0’. Since the OR-gate switches on 
(‘1’) in this case, the ‘0’ input has no effect and is not a 
cause. 
3.2 Linear threshold units 
A generalization of simple, linear logic gates, such as 
OR- and AND-gates, are binary linear threshold units 
(LTUs). Given ? equivalent inputs '/01, a single LTU 9/ 
will turn on (‘1’) if the number of inputs in state ‘1’ exceeds 
a given threshold k,  
+ 9/ = 1|)/01 = 	
1			AC	 )@,/01
H
@I1
? †,
0		AC	 )@,/01
H
@I1
< †.
	
LTUs are of great interest, for example, in the field of 
neural networks, since they comprise one of the simplest 
model mechanisms for neurons, capturing the notion that a 
neuron fires if it received sufficient synaptic inputs. One 
example is a Majority-gate, which outputs ‘1’ iff more than 
half of its inputs are ‘1’.  
Fig. 6 displays the causal account of a Majority-gate M 
with 4 inputs for the transition 2/01 = ABCD = 1110 ?
5/ = {M = 1}. All of the inputs in state ‘1’, as well as their 
high-order occurrences, have actual effects on {M = 1}. 
Occurrence {D = 0}, however, does not work towards 
bringing about {M = 1}: it reduces the probability for {M = 
1} and thus does not contribute to any actual effects or the 
actual cause. As with the AND-gate in the previous section, 
there is composition of actual effects in the causal account. 
Yet, there is only one actual cause, {ABC = 111} ? {M = 
1}. In this case, it happens to be that the 3rd-order 
occurrence {ABC = 111} is minimally sufficient for {M = 
1}—no smaller set of input elements would suffice. Note 
however, that the actual cause )/01? ({M = 1}) is not 
determined based on sufficiency, but because {ABC = 111} 
is the set of elements maximally constrained by the 
occurrence {M = 1}. Nevertheless, causal analysis as 
illustrated here will always identify a minimally sufficient 
set of input elements as the actual cause of an LTU, for any 
B
A
C
!"#$ =>{AB6=611}6? &"= {C =61}
g = gH = gL = 0.4156bits6
MIP B
A
D
!"#$ =>{AB6=611}6? &"= {D6=61}
MIP
g = 3.06bits
B
A
E
!"#$ =>{AB6=611}6? &"= {E6=61}
MIP
g = gH +gL = 2.06bits
B
A
F
!"#$ =>{AB6=611}6? &"= {F6=61}
MIP
g = gH = 06bits
0
1
1
1pa
st
6st
at
e6
t–
1
A66B i C" = 1
0
0
0
1pa
st
6st
at
e6
t–
1
A66B i D" = 1
1
0
0
1pa
st
6st
at
e6
t–
1
A66B i E" = 1
1
0
1
1pa
st
6st
at
e6
t–
1
A66B i F" = 1
A B C D
OROgate ANDOgate
- ? 6? WH[\]
{A6=61}6? {C6=61}
{B6=61}6? {C6= 1}
0.4156bits
0.4156bits
0.4156bits
- ? 6? WL[\]
{A6=61}? {C6=61}
- ? 6? WH[\]
{A6=61}6? {D6=61}
{B6=61}6? {D6=61}
{AB6=611}6? {D6=61}
1.06bits
1.06bits
1.06bits
2.06bits
- ? 6? WL[\]
{AB6=611}6? {D6=61}
- ? 6? WH[\]
{AB6=611}6? {E6=61} 1.06bits
1.06bits
- ? 6? WL[\]
{AB6=611}? {E6=61}
- ? 6? WH[\]
{B6=61}6? {F6=61} 0.4156bits
0.4156bits
- ? 6? WL[\]
{B6=61}? {F6=61}
Disjunction Conjunction Biconditional Prevention
 11 
number of inputs n and any threshold k. Furthermore, any 
occurrence of all ‘1’ elements with size less than or equal to 
the threshold k will be irreducible, with the LTU as their 
actual effect.  
 
Fig. 6: A linear threshold unit with four inputs and 
threshold k = 3 (Majority gate). Note that this transition is 
reducible as no causal links are lost through the MIP. 
 
Theorem 1: For a transition 2/01 ? 5/, where 4/ is a 
single LTU with n inputs and a threshold of k, and ./01 is 
the set of n inputs to the LTU, the following holds: 
1. The actual cause of {9/ = 1} is '/01 = )/01 , 
such that )/01 = † and min )/01 = 1.  
2. If min )/01 = 1 and )/01 ? †, then {9/ = 1}	is 
the actual effect of '/01 = )/01 ; otherwise )/01 
has no actual effect, it is reducible. 
 
Proof: See Supplementary Proof 1. 
 
Note that an LTU in the off (‘0’) state, {9/ = 0}, has 
equivalent results with the role of ‘0’ and ‘1’ reversed, and 
a threshold of n – k. In the case of overdetermination, e.g. 
the transition 2/01 = ABCD = 1111 ? 5/ = {M = 1} , 
where all inputs to the Majority-gate are ‘1’, the actual 
cause will again be a subset of 3 input elements in state ‘1’9. 
However, which of the possible sets remains undetermined 
                                                            
9 Halpern (2016) considers a voting scenario with eleven 
voters for two candidates, which is equivalent to a 
Majority-gate with 11 inputs. In the case of all 11 inputs in 
state ‘1’, he arrives at the conclusion that any subset of 6—
any minimally sufficient set—is a cause for :/ = {M = 1} 
(or ‘Suzy winning’ in his example), according to the 
modified HP definition of an actual cause. Note, however, 
that the modified HP account does not generally identify 
minimally sufficient sets as the actual cause. Given 10 
voters, all voting for ‘1’, the modified HP account would 
identify any subset of 5 voters as the actual cause (the 
number of ‘1s’ that would have to be switched to change 
the outcome of M), while still 6 votes are necessary and 
minimally sufficient for a majority (M = 1). 
due to symmetry, just as in the case of the OR-gate in Fig. 
5A. 
The transition 2/01 = ABCD = 1110 ? 5/ = {M =
1} shown in Fig. 6 is reducible with é 2/01 ? 5/ = 0, 
since input element D does not contribute to the causal 
account. Consider instead the causal account of the 
transition 2/01 = ABC = 111 ? 5/ = {M = 1} , where 
{D = 0} at ! ? 1 is treated as a background condition 
(Supplementary Fig. S1). Doing so results in a causal 
account with the same causal links but higher causal 
strengths. This captures the intuition that A, B, and C’s ‘yes 
votes’ are more important if it is already determined that D 
will ‘vote no’. 
3.3 Disjunction of conjunctions 
Another case often considered in the actual causation 
literature is a disjunction of conjunctions, that is, an OR-
operation over two or more AND-operations. In the general 
case, a disjunction of conjunctions is an element 9/ that is a 
disjunction of k conditions, each of which is a conjunction 
of ?§ input elements )@,§,/01, for A = 1…?§,	and • = 1… †,  
+(9/ = 1|)/01) = 	
0							AC	 )@,§,/01 < ?§	?	•
H¶
@I1
	
1										%!?®©™A=®																			
	
Here we consider a simple example, A	 ? B ? C (Fig. 
7). The debate over this example is mostly concerned with 
the type of transition shown in Fig. 7: 
2/01 = ABC = 101 ? 5/ = {? = 1}, and the question 
whether	 A = 1  is a cause of {D = 1} even if B = 0.10 
Again, the distinction between actual effects and actual 
causes, as well as the quantitative assessment of actual 
causes and actual effects, can help to resolve issues of 
actual causation in this type of examples.  
As shown in Fig. 7, with respect to actual effects, both 
{A = 1} ?	{D = 1} and {C = 1} ? {D = 1}, with {C =1} 
having a stronger actual effect. With respect to actual 
causes, only {C = 1} ? {D = 1}, being the maximally 
irreducible cause with Üháàâ({D = 1}) = 0.678.  
When judging the actual effect of {A = 1} at ! ? 1 
within the transition 2/01 = ABC = 101 ? 5/ = {D = 1}, 
B is assumed to be undetermined. By itself, the occurrence 
{A = 1} does raise the probability of occurrence {D = 1}, 
and thus {A = 1} ?	{D = 1}. Note however, that this 
transition is reducible with é 2/01 ? 5/ = 0, since B 
does not contribute to the causal account. The occurrence 
{C = 1} prevents {B = 0} from having an actual effect (if C 
were 0, D would be 0 and {B = 0} would have an actual 
                                                            
10 One story accompanying this example is that “a prisoner 
dies either if A loads B’s gun and B shoots, or if C loads 
and shoots his gun. … A loads B’s gun, B does not shoot, 
but C does load and shoot his gun, so that the prisoner dies” 
(Hopkins and Pearl, 2003; Halpern, 2016). 
B
A
M
MIP
i M" = 1 = 1
iff			?(ABCD)   ? 3
D
C
!"#$ =	{ABCD	=	1110}	? &"= {M =	1}
g = gH = 0 bits	
- ? 6? WH[\]
{A	=	1}	? {M	=	1}
{B	=	1}	? {M	=	1}
{C	=	1}	? {M	=	1}
{AB	=	11}	? {M	=	1}
{AC	=	11}	? {M	=	1}
{BC	=	11}	? {M	=	1}
{ABC	=	111}	? {M	=	1}
0.678	bits
0.678	bits
0.678	bits
0.585	bits
0.585	bits
0.585	bits
0.415	bits
1.678	bits
- ? 6? WL[\]
{ABC	=	111}? {M	=	1}
 12 
effect). However, if {B = 0} is taken as a fixed background 
condition, the transition 2/01 = AC = 11 ? 5/ = {D =
1} is also reducible, since with {B = 0} fixed, {A = 1} does 
not have an actual effect anymore (Fig. S2A). In this case, 
the background condition {B = 0} prevents {A = 1} from 
having any effect. The only irreducible transition with é =
2.0 is 2/01 = C = 1 ? 5/ = D = 1  with {AB = 10} as 
a fixed background condition, which may thus be 
interpreted as the most parsimonious explanation for what 
happened. 
 
Fig. 7: Disjunction of two conjunctions Æ	 ? Ø ? ?.  
The results from this example extend to the general 
case of disjunctions of conjunctions. In the situation where 
:/ = 1, the actual cause of :/ is the minimum set of 
sufficient elements. If multiple conjunctive conditions are 
satisfied, the actual cause of :/ is thus the smallest such 
condition (see Supplementary Fig S2B for such an example 
of asymmetric overdetermination). At ! ? 1, any 1st–order 
occurrence in state ‘1’, as well as any high-order occurrence 
of such elements that does not overdetermine :/, has an 
actual effect. This includes any occurrence in state all ‘1’ 
that contains only elements from exactly one conjunction, 
as well as any high-order occurrence of elements across 
conjunctions, which do not fully contain any specific 
conjunction.  
If instead :/ = 0, then the actual cause of :/ is an 
occurrence that contains a single element in state ‘0’ from 
each conjunctive condition. At ! ? 1, any occurrence in 
state all ‘0’ that does not overdetermine :/ has an actual 
effect, which is any all ‘0’ occurrence that does not contain 
more than one element from any conjunction.  
These results are formalized by the following theorem.  
 
Theorem 2. For a transition 2/01 ? 5/, where 4/ is a 
single element observation that is a disjunction of k 
conditions, each of which is a conjunction over ?§ elements, 
and ./01 = )§,/01 = )@,§,/01  is the set of all 
? = 	 ?§
±
§I1  inputs. Considering occurrences )/01 with ?§ 
elements from each conjunction, the following holds: 
1. If :/ = 1: 
a. The actual cause of {9/ = 1} is {'/01 = )§,/01}, 
such that min()§,/01) = 1 and 
min )§Ñ,/01 = 1 ? ?§ ? ?§Ñ. 
b. If min )/01 = 1 and ?§ = ?§ ? ?§Ñ = 0, ?	•1 ? •. 
then {9/ = 1}	is the actual effect of {'/01 = )/01}, 
otherwise )/01 has no actual effect, it is reducible. 
2. If :/ = 0: 
a. The actual cause of {9/ = 0} is {'/01 = )/01}, 
such that max )/01 = 0 and ?§ = 1	?	•.	 
b. If max )/01 = 0	and ?§ ? 1,	then {9/ = 0} is the 
actual effect of {'/01 = )/01}, otherwise )/01 is 
reducible and has no actual effect. 
 
Proof: See Supplementary Proof 2. 
3.4 Complicated voting 
As already demonstrated in the examples in Fig. 5C 
and D, the proposed causal analysis is not restricted to 
linear update functions or combinations thereof. Fig. 8 
depicts an example transition featuring a complicated, 
nonlinear update function. This specific example is taken 
from (Halpern, 2015, 2016): If A and B agree, F takes their 
value, if B, C, D, and E agree, F takes A’s value, otherwise 
majority decides. The transition of interest is 
2/01 = ABCDE = 11000 ? 5/ = {F = 1}. 
According to Halpern (Halpern, 2015), intuition 
suggests that  {AB = 11} ? {F = 1}. {AB = 11} is also the 
minimally sufficient occurrence in the transition that 
determines {F = 1}. Indeed, this is the result of the present 
causal analysis of the transition (Fig. 8). In addition, {A = 
1} ? {F = 1}, {B = 1} ?	{F = 1}, {AB = 11} ? {F = 1}, 
and {ACDE = 1000} ? {F = 1} all contribute to the causal 
account. Both {AB = 11} and {ACDE = 1000} completely 
determine that {F = 1} will occur. Nevertheless, according 
to the exclusion principle, {AB = 11} is the actual cause of 
{F = 1} rather than {ACDE = 1000}, as it is the minimal 
occurrence with Üh )/01, :/ = 	Üháàâ :/ = 1.0.  
 
Fig. 8: Complicated voting. 
3.5 Noise and probabilistic elements 
The examples so far involved deterministic update 
functions. Probabilistic accounts of causation are closely 
related to counterfactual accounts (Paul and Hall, 2013). 
Nevertheless, certain problem cases only arise in 
probabilistic settings (e.g. Fig. 9B). The present causal 
analysis can be applied equally to probabilistic as well as 
B
A
D
g = gH = 0 bits
MIP
h n" = 1 = 1
if666?ABo#$   > 1
? Co#$ = 1
C
!"#$ =>{ABC6=6101}6? &"= {D6=61}
0
0
0
1
1
1
1
1
pa
st
6st
at
e6
t–
1
A66B66C6 h n" = 1
- ? 6? WH[\]
{A6=61}6? {D6=61}
{C6=61}6? {D6=61}
0.2636bits
0.6786bits
0.6786bits
- ? 6? WL[\]
{C6=61}? {D6=61}
B
A
F
g = 0.30 bits
MIP
if6:6Ao#$ = Bo#$
! q" = Ao#$
elif:>
(vo#$ = wo#$ = n"#$ = x"#$)
! q" = Ao#$
else:6Majority
C
!"#$ =>{ABCDE6=611000}6? &"= {F6=61}
D
E
- ? 6? WH[\]
{A6=61}6? {F6=61}
{B6=61}6? {F6=61}
{AB6=611}6? {F6=61}
{ACDE6=61000}6? {F6=61}
0.706bits
0.466bits
0.306bits
0.306bits
1.06bits
- ? 6? WL[\]
{AB6=611}6? {F6=61}
Check6if6=6Ae6or6Ac
 13 
deterministic systems, as long as the system’s transition 
probabilities satisfy conditional independence (Eqn. 1). No 
separate, probabilistic calculus for actual causation is 
required.  
In the simplest case, where noise is added to a 
deterministic transition 2/01 ? 5/, the noise will generally 
decrease the strength of the causal links in the transition. 
Fig. 9 shows the causal account of the transition 2/01 =
A = 1 ? 5/ = {N = 1}, where N is the slightly noisy 
version of a COPY-gate. In this example, both {A = 1} ? 
{N = 1} and {A = 1} ? {N = 1}. The only difference with 
the equivalent deterministic case is that the causal strength 
Üa
áàâ = Üh
áàâ = 0.848 is lower than in the deterministic 
case where Üaáàâ = Üháàâ = 1. Note that in this 
probabilistic setting, the actual cause {A = 1} by itself is 
not sufficient to determine {N = 1}. Nevertheless, {A = 1} 
makes a positive difference in bringing about {N = 1}, and 
this difference is irreducible, so the causal link exists within 
the transition. 
The transition 2/01 = A = 1 ? 5/ = {N = 0} has no 
counterpart in the deterministic case (where it would be 
impossible by violating the existence principle). The result 
of the causal analysis is that there are no irreducible causal 
links within the given transition, which is therefore 
reducible with é 2/01 ? 5/ = 0. One interpretation is 
that the actual cause of {N = 0} must lie outside of the 
system, such as a missing latent variable. Another 
interpretation is that the actual cause for {N = 0} is genuine 
‘physical noise’. In any case, the proposed account of actual 
causation is sufficiently general to cover both deterministic 
as well as probabilistic systems.  
 
Fig. 9: Probabilistic elements. While the transition shown 
in (A) does have a deterministic equivalent, the transition 
shown in (B) would be impossible in the deterministic case. 
3.6 Simple classifier 
As a final example, we consider a transition with a 
multi-element observation 5/—in particular, 3 elements A, 
B, and C providing input to 3 different classifier elements 
D, S, and L. Element D is a ‘dot-detector’; it outputs ‘1’ if 
exactly one of the 3 inputs is in state ‘1’. Element S is a 
‘segment-detector’: it outputs ‘1’ for input states {ABC = 
110} and {ABC = 011}. Element L detects lines, that is, 
{ABC = 111}.  
Fig. 10 shows the causal account of the specific 
transition 2/01 = ABC = 001 ? 5/ = {DSL = 100}. In 
this case, only a few occurrences )/01 have actual effects, 
but all possible occurrences :/ are irreducible with their 
own actual cause. The occurrence {C = 1} by itself, for 
example, has no actual effect. This may be initially 
surprising since D is a dot detector and {C = 1} is 
supposedly a dot. However, the specific configuration of 
the entire input set is necessary to determine {D = 1} (a dot 
is only a dot if the other inputs are ‘0’). Consequently, 
{ABC = 001} ? {D = 1} and also {ABC = 001} ? {D = 
1}. By contrast, the occurrence {A = 0} is sufficient to 
determine {L = 0} and raises the probability of {D = 1}; the 
occurrence {B = 0} is sufficient to determine {S = 0} and 
{F = 0} and also raises the probability of {D = 1}. We thus 
get the following causal links from single element 
occurrences in the system: {A = 0} ? {DL = 10}, {A = 0} 
? {L = 0}, {B = 0} ? {DSL = 100} and {B = 0} ? {S = 
0} (note that {B = 0} would be an equivalent actual cause 
of {L = 0} with the same Üháàâ).  
In addition, all high-order occurrences :/  are 
irreducible, each having their own actual cause above those 
of their parts. The actual cause identified for these high-
order occurrences can be interpreted as the ‘strongest’ 
shared cause of the elements in the occurrence, for example 
{B = 0} ? {DS = 10}. While only the occurrence {ABC = 
001} is sufficient to determine {DS = 10}, this candidate 
causal link is reducible, because {DS = 10} does not 
constrain the state of ABC any more than {D = 1} by itself. 
In fact, the occurrence {S = 0} does not constrain the state 
of AC at all. Thus {ABC = 001} and all other candidate 
causes of {DS = 10} that include these elements are either 
reducible (because their causal link can be partitioned with 
Üh
áàâ = 0) or excluded (because there is a smaller set of 
elements whose causal strength is at least as high). In this 
example, {B = 0} is the only irreducible shared cause of {D 
= 1} and {S = 0}, and thus also the actual cause of {DS = 
10}.  
 
Fig. 10: Simple classifier. Element D is a ‘dot-detector’, S 
a ‘segment-detector’, and L a ‘line-detector’ (see text). 
A N
!"#$ =>{A6=61}6? &"= {N6=61}
g> = gH +gL = 1.696 bits6
MIP
B
A
N
!"#$ =>{AB6=610}6? &"= {N6=60}
MIP
- ? 6?
-? ? 6
WH[\](-"#$ = B = 0 , 6"? = {N = 0}) = 0.737
g =gH = 0
WL[\](-"#$? = B = 0 , 6" = {N = 0}) = 0.737
0.01
0.09
pa
st
6st
at
e6
t–
1 A h N" = 1
0.1
0.9
0.9
0.9pa
st
6st
at
e6
t–
1
A66B h AND" = 1
A
C
A N
!"#$ =>{A6=61}6? &"= {N6=60}
no6causal6links
MIP
g = gH = gL = 0 bits
B
- ? 6? WH[\]
{A6=61}6? {N6=61} 0.8486bits
0.8486bits
- ? 6? WL[\]
{A6=61}? {N6=61}
B
A
L
g> = 0.903 bits
MIP
C
!"#$ =>{ABC6=6001}6? &"= {DSL6=6100}
D
S
0 0 0
1 0 0
1 0 0
0 1 0
1 0 0
0 0 0
0 1 0
0 0 1
A66B66C6
pa
st
6st
at
e6
t–
1
- ? 6? WH[\]
{A6=60}6? {DL6=610}
{B6=60}6? {DLS6=6100}
{ABC6=6001}6? {D6=61}
0.6086bits
1.026bits
1.06bits
1.4156bits
0.4156bits
0.1936bits
0.2636bits
0.1266bits
0.1266bits
0.0746bits
- ? 6? WL[\]
{ABC6=6001}? {D6=61}
{B6=60}6? {S6=60}
{A6=60}6? {L6=60}6
{B6=60}6? {DS6=610}
{A6=60}6? {DL6=610}
{B6=60}6? {SL6=600}
{B6=60}6? {DSL6=6100}
 14 
4. Discussion 
In this article, we presented a principled, 
comprehensive formalism to assess actual causation given a 
transition between two subsequent observations in a 
discrete dynamical system. The proposed framework 
provides a complete causal account of all causal links 
between occurrences at	! ? 1 and !, based on five principles 
– existence, composition, information, integration, and 
exclusion. In what follows, we review specific features and 
limitations of our approach, discuss how the results relate to 
intuitive notions about actual causation and causal 
explanation, and highlight some of the main differences 
with previous proposals aimed at operationalizing the 
notion of actual causation. Specifically, our framework 
considers all possible counterfactual states and makes it 
possible to assess the actual strength of causal links. 
Second, it distinguishes between actual causes and actual 
effects, which are considered separately. Third, it allows for 
causal composition, in the sense that first- and high-order 
occurrences can have their own causes and effects within 
the same transition, as long as they are irreducible. As 
demonstrated in the results section, the proposed formalism 
is generally applicable to a vast range of mechanistic 
relations. By enforcing the assessment of actual causation 
in fully specified model systems, this framework also 
avoids several paradoxes that can arise from narrative 
examples loosely tied to a causal model.  
4.1 Testing all counterfactuals with equal 
probability 
From a single transition 2/01 ? 5/, i.e. an observed 
sequence of two states, it is neither possible to infer causal 
relations between the elements involved, nor to identify the 
mechanisms underlying such causal interactions (see Fig. 
5). Moreover, observing the same transition over and over 
again does not automatically reveal a causal link between  
2/01 and 5/. In order to infer a causal dependency between 
2/01 and 5/, one has to assess alternative states 
(counterfactuals) of both sets of elements ./01 and 4/. 
The structural modeling framework of system 
interventions introduced by Pearl (Pearl, 2000) provides a 
basis for formalizing the notion of counterfactual inference. 
In the simplest case, counterfactual approaches to actual 
causation are based on the “but-for” test (Halpern, 2016): ? 
is a cause of ® if ¬? implies ¬® (“but for ?, ® would not 
have happened”). In larger causal models in which this 
condition is typically context dependent (on variables ?), 
more sophisticated counterfactual approaches differ 
depending on the various contingencies (? = V) under 
which the “but-for” test is applied. In general, if one such 
counterfactual state (¬?, V) exists, ? is identified as a cause 
of ® (Pearl, 2000; Halpern, 2016). 
By contrast, the present framework requires that 
interventions set a discrete dynamical system 6 into all its 
possible states, including those that may not spontaneously 
occur during the system’s dynamical evolution. This is 
because a discrete dynamical system 6 and the mechanisms 
of its elements are only specified fully by the transition 
probabilities of all possible system states (Eqn. 1) (Ay and 
Polani, 2008; Oizumi et al., 2014). For example, element E 
in Fig. 5C, an XNOR, can only be distinguished from the 
AND-gate D in Fig. 5B, and F in Fig. 5D, if all its transition 
probabilities (input-output relations) from every possible 
state to the next state are given. Once the underlying 
mechanisms are specified based on all possible transition 
probabilities, causal relations can be quantified in 
probabilistic terms (Ay and Polani, 2008; Oizumi et al., 
2014) even within a single transition 2/01 ? 5/, i.e. in the 
context of actual causation (Glennan, 2011; Korb et al., 
2011). In this way, the present approach to actual causation 
is applicable to both deterministic and probabilistic systems 
and explicitly captures the notion of causal specificity 
within the scope of the transition of interest (Woodward, 
2010; Hoel et al., 2013; Oizumi et al., 2014).  
Accordingly, actual causes and effects of occurrences 
within a transition 2/01 ? 5/, which reflect mechanistic 
dependencies between system variables, are identified 
based on interventional conditional probability distributions 
derived from the set of all possible transition probabilities 
(Eqn. 1) (Ay and Polani, 2008; Balduzzi and Tononi, 2008; 
Oizumi et al., 2014). To correctly capture the strength of 
the causal constraints )/01 exerts over :/ and vice versa, 
unconfounded by constraints due to other endogenous or 
external variables and previous time steps, we explicitly 
require that the distribution over states of ./01 be 
maximum entropy. In other words, the probability of an 
observation )/01 should have no bearing on the question 
whether )/01 had an effect on or was a cause of :/, once 
)/01 actually happened11. Related measures of information 
flow in causal networks (Ay and Polani, 2008) and causal 
information (Korb et al., 2011), by contrast, remain 
agnostic about the assumed distribution over states of ./01. 
4.2 Distinguishing actual effects and actual 
causes 
An implicit assumption commonly made about (actual) 
causation is that the relation between cause and effect is 
bidirectional: if occurrence ª = ? had an effect on 
occurrence	º = ®, then	? is assumed to be a cause of ®. 
Thus, most approaches aimed at identifying the actual cause 
of an occurrence ® ask whether a manipulation of the 
candidate cause in a given context changes the state of ®, or 
                                                            
11 Likewise, which other occurrences in (2/))/01 happened 
and how likely they were to happen should not matter for 
determining the strength with which )/01 constrains :/. 
This is why we use the product distributions (Eqn. 2 and 3), 
assuming independent, maximum entropy inputs to each 
element in 4/.  
 15 
its probability (e.g., Woodward, 2004; Korb et al., 2011; 
Halpern, 2016). In probabilistic accounts this view is 
manifested in the notion that causes should raise the 
probability of their effects, such as the requirement that 
+(®|$% ª = ? ) > +(®) (Good, 1961; Suppes, 1970; Eells 
and Sober, 1983; Pearl, 2009)12. In the present framework, 
+(®|$% ª = ? > +(®) is captured by the effect ratio 
`a()/01, 	:/) (Eqn. 4), which also takes into account all 
possible counterfactuals (all 2 ? ?P). However, even in 
deterministic systems, the fact that an occurrence ?, by 
itself, raises the probability of an occurrence ®, does not 
necessarily determine that º = ® will actually occur (Paul 
and Hall, 2013). Consider, for example, the transition 
shown in Fig. 7: by itself, the occurrence {A = 1} raises the 
probability of {D = 1} (`a )/01, 	:/ = Üa )/01, 	:/ > 0). 
Nevertheless, in the example transition, {A = 1} is neither 
necessary nor sufficient for {D = 1}: if not for {C = 1}, D 
would be in state ‘0’ instead of ‘1’. Accordingly, it has been 
argued that {A = 1} should be not be considered a cause of 
{D = 1} (Halpern, 2015). Our approach clarifies the causal 
account by explicitly distinguishing effects from causes and 
considering both separately. Actual effects are identified 
from the perspective of occurrences at ! ? 1, whereas 
actual causes are identified from the perspective of 
occurrences at !13. In Fig. 7, while both {A = 1} and {C = 
1} have an actual effect on {D = 1}, {C = 1} is the 
maximally irreducible and thus actual cause of {D = 1} 
(following the exclusion principle). In general, the 
existence of an actual effect )/01 ? :/ does not imply that 
the complementary actual cause )/01 ? :/ also exists and 
vice versa.  
Here we propose that a comprehensive causal 
understanding of a given transition is provided by its 
complete causal account å (Definition 2.3), including both 
actual effects and actual causes, i.e. all maximally 
irreducible causal links of direction )/01 ? :/ and )/01 ?
                                                            
12 Initially, the objective of probabilistic accounts of 
causation was to infer causation, or at least causal 
relevance, from observed distributions. As demonstrated in 
(Pearl, 2000, 2009), however, + º ª > +(º) is neither 
necessary nor sufficient to establish a causal relation 
between E and C, highlighting the crucial role of 
interventions, i.e. the do() operator.  
13 Note that Judea Pearl initially proposed maximizing the 
posterior probability	+(?|®) as a means of identifying the 
best (‘most probable’) explanation for an occurrence ® 
(Pearl, 1988; Chapter 5). This approach has later been 
criticized, among others, by Pearl himself (Pearl, 2000; 
Chapter 7), as it had been formalized in purely probabilistic 
terms, lacking the notion of system interventions. 
Moreover, without a notion of irreducibility, as applied in 
the present framework, explanations based on +(?|®) tend 
to include irrelevant variables (Shimony, 1991; Chajewska 
and Halpern, 1997). 
:/. In this way, the causal account provides a 
comprehensive explanation of “what happened” in terms of 
“what caused what”, which acknowledges causal exclusion 
and still provides all causally relevant details. As an 
example, Fig. 5 illustrates how the causal account describes 
and distinguishes four dynamically identical transitions. 
Including both actual causes and effects into the causal 
account can moreover resolve ambiguities in cases of 
redundant causation, such as overdetermination, 
preemption, and prevention (Hitchcock, 2007; Paul and 
Hall, 2013). As an example, consider again the system 
shown in Fig. 7, but now transitioning from 2/01 = {ABC 
= 111} ? 5/ = {D = 1} (Fig. S2B). In this case of 
asymmetric overdetermination, from the perspective of ! ?
1, the occurrences {A = 1}, {B = 1}, {AB = 11}, and {C = 
1} all have actual effects on {D = 1}. However, neither 
occurrence is necessary and at least {AB = 1} and {C = 1} 
are both sufficient for {D = 1}. From the perspective of !, 
{C = 1} is identified as the actual, maximally irreducible, 
cause of {D = 1}. Any framework that disregards this 
distinction between causes and effects by falsely equating 
the two would either violate causal exclusion, or provide an 
incomplete causal description of the transition.  
4.3 High-order occurrences and composition 
That an occurrence can affect several elements (high-
order effect), and that the cause of an occurrence can 
involve several elements (high-order cause) is 
uncontroversial (Woodward, 2010),14 and also applies to the 
framework presented here: given a particular occurrence 
)/01 in the transition 2/01 ? 5/, we consider the whole 
power set of 4/ as candidate effects of )/01, and the whole 
power set of ./01 as candidate causes of a particular 
occurrence :/. This means, for example, that in all cases 
shown in Fig. 5, either )/01 = {A = 1}, {B = 1}, or {AB =
11} could be an actual cause of :/ = {C/D/E/F = 1}. 
However, across the set of candidate causes/effects the 
causal exclusion principle applies: each occurrence has at 
most one actual cause or effect within a transition, whatever 
its order (the number of elements involved). The causal 
principles employed here then single out the actual cause as 
the minimal set of elements whose actual state is most 
irreducibly determined by the occurrence, measured by 
Üáàâ.  
While the causal exclusion principle applies to the 
actual cause and effect of a particular occurrence, the causal 
composition principle applies to the power set of 
occurrences within the transition. The proposed framework 
explicitly acknowledges that there may be high-order 
occurrences, which have actual causes or actual effects that 
                                                            
14 Nevertheless, as illustrated in the case of a simple 
conjunction (Fig. 5B), some regard each individual element 
A and B as a separate cause by itself, a “contributory 
cause”, or “part of a cause” (Halpern, 2015). 
 16 
are irreducible to those of their parts. This notion goes 
against widespread reductionist views that exclusively 
assign causal powers to first-order occurrences that involve 
the elementary constituents of physical systems (here 
individual system elements), typically without providing a 
formal notion of causal influence. A simple example of a 
high-order occurrence with an actual effect is provided in 
Fig. 5C: in this case, the individual elements A and B do 
not fulfill the most basic criterion for having an effect on 
the XNOR element {E = 1} as `a )/01, :/ = 0, whereas 
the 2nd-order occurrence {AB = 11} has the actual effect {E 
= 1}.  
Moreover, as illustrated in the conjunction example 
(Fig. 5B), the existence of causal links specified by first-
order occurrences, in this case {A = 1} ? {D = 1} and {B = 
1} ? {D = 1}, does not exclude the 2nd-order occurrence 
{AB = 11} from having an additional, irreducible effect on 
{D = 1}. Specifically, while {A = 1} and {B = 1} both 
constrain the AND-gate D in the same way, the occurrence 
{AB = 11} further raises the probability of {D = 1} 
compared to the effect of each individual input. Note that, if 
instead of taking a causal approach, one takes a strictly 
informational approach focused on predicting :/ from )/01 
(in an unperturbed system), one might be tempted to 
disregard such compositional occurrences and their actual 
effects, since they do not add predictive power. Thus, the 
actual effect of {AB = 11} in the conjunction example of 
Fig. 5B is informationally redundant, since {D = 1} can be 
inferred (predicted) from {A = 1} and {B = 1} alone. 
However, it is the irreducible mechanistic constraints, and 
not predictive powers, that are relevant for distinguishing 
“what caused what” in the various transitions shown in Fig. 
5 and that provide an explanation for “what happened”.  
The XNOR example can be extended to a “double-
biconditional” in transition 2/01 = {ABC = 111} ? 5/ = 
{DE = 11} to illustrate the importance of the composition 
of occurrences with respect to both actual causes and actual 
effects (Fig. 11). In the figure, both D and E are XNOR 
elements that share one of their inputs (element B), and 
{AB = 11} ? {D = 1} and {BC = 11} ? {E = 1}. As 
illustrated by the cause-repertoires shown in Fig. 11B, and 
in accordance with D’s and E’s logic function (mechanism), 
the actual cause of {D = 1} can be described as the fact that 
A and B were the same, and the actual cause of {E = 1} as 
the fact that B and C were the same. In addition to these 1st-
order occurrences, also the 2nd-order occurrence {DE = 11} 
has an actual cause {ABC = 111}, which can be described 
as the fact that all three elements A, B, and C were in the 
same state. Crucially, this fact is not captured by either the 
actual cause of {D = 1}, or by the actual cause of {E = 1}, 
but only by the irreducible constraints of the 2nd-order 
occurrence {DE = 11}. On the other hand, the causal link 
{ABC = 111} ? {DE = 11} cannot capture the fact that 
{AB = 11} was the actual cause of {D = 1} and {BC = 11} 
was the actual cause of {E = 1}. Of note, in this example 
the same reasoning applies to the composition of high-order 
occurrences at ! ? 1 and their actual effects.  
In sum, irreducible high-order occurrences capture 
actual, joint mechanistic dependencies between the 
occurrence’s elements that are not revealed by the actual 
causes and effects of their parts. Moreover, a high-order 
occurrence does not exclude lower-order occurrences over 
their parts, which specify their own actual causes and 
effects. In this way, high-order and 1st-order occurrences all 
contribute to the explanatory power of the causal account 
by the composition principle. 
 
Fig. 11: High-order occurrences and composition. 
(A) Double-Biconditional: Transition and causal account. 
(B) Cause repertoires corresponding to the two 1st–order 
and one 2nd–order occurrences with actual causes (see 
main text). 
4.4 Irreducibility 
In the present framework, having a positive effect ratio 
`a )/01, 	:/ > 0 is necessary but not sufficient for )/01 ?
:/, and the same for a positive cause ratio `h )/01, :/ >
0.15 As discussed above, high-order occurrences can have 
actual causes and effects, but only if they are irreducible to 
their parts. This is illustrated in Fig. 12, in which a 
transition equivalent to our initial example in Fig. 4 (Fig. 
                                                            
15 Occurrences ? that lower the probability of a subsequent 
occurrence ® have been termed “preventive causes” (Korb 
et al., 2011). Rather than counting a negative effect ratio 
`a ?, ® < 0 as indicating a possible “preventive effect”, 
we take the stance that such occurrences ? have no effect at 
all within the transition of interest, since they actually 
predict other occurrences º = ¬® that did not happen. By 
the same logic, a negative cause ratio `h ?, ® < 0 means 
that ? is no cause of ® within the transition. Nevertheless, 
the current framework can in principle quantify the strength 
of possible “preventive” causes and effects. 
B
A
MC
D
P
!"#$ =>{ABCD6=61110}6? &"= {P,M6=610}
g = gH = gL = 0 bits
- ? 6? WH[\]
{A/B/C6=61}6? {M6=61}
{AB/AC/BC6=611}6? {M6=61}
{ABC6=6111}6? {M6=61}
{ABCD6=61110}6? {P6=61}
0.6786bits
0.5856bits
0.4156bits
1.06bits
1.06bits
1.6786bits
2.3226bits
- ? 6? WL[\]
{ABCD6=6111}6? {¬P =61}
{ABC6=6111}6? {M6=61}
{ABCD6=61111}6? {¬PM6=611}
MIP
B
A
D
!"#$ =>{AB6=6111}6? &"= {DE6=611}
g = gH = gL = 4.06bits
pa
st
6st
at
e6
t–
1 - ? 6? WH[\]
{AB6=611}6? {D6=61}
{BC6=611}6? {E6=61}
{ABC6=6111}6? {DE6=611}
1.06bits
1.06bits
1.06bits
- ? 6? WL[\]
DoubleZBiconditional
C
E {AB6=611}6? {D6=61}
{BC6=611}? {E6=61}
{ABC6=6111}6? {DE6=611}
1.06bits
1.06bits
1.06bits
MIP 1 1
0 1
0 0
1 0
1 0
0 0
0 1
1 1
A66B66C6
A
B
-" =6{AB6=611}
B
A
X| !<#=,>&< = ?@AB C.FC.BF = 1.0*bits
-" =6{BC6=611}
C
B
-" =6{ABC6=6111}
C
B
A
{AB6=611}6? {D6=61} {BC6=611}6? {E6=61} {ABC6=6111}6? {DE6=611}
 17 
12A) is compared against a similar, but reducible transition 
(Fig. 12C). The two situations differ mechanistically: the 
OR and AND gate in Fig. 12A receive common inputs from 
the same two elements, while the OR and AND in Fig. 12C 
have independent sets of inputs. Nevertheless, the actual 
causes and effects of all single element occurrences are 
identical in the two cases. In both transitions, {OR = 1} is 
caused by its one input element in state ‘1’, and {AND = 0} 
is caused by its one input element in state ‘0’. What 
distinguishes the two causal accounts is the additional 
causal link in Fig. 12A, between the 2nd–order occurrence 
{OR, AND = 10} and its actual cause {AB = 10}. While 
{OR, AND = 10} raises the probability of both {AB = 10} 
(Fig. 12A) and {AD = 10} (Fig. 12C) above chance (S = 
0.25, `h )/01, :/ > 0), but only {AB = 10} ? {OR, AND 
= 10} in Fig. 12A is irreducible to its parts. This is shown 
by partitioning across the MIP (S = 0.5 vs.	Síìî = 0.44) 
with Üh()/01, :/) = 0.17. This 2nd-order occurrence thus 
specifies that the OR and AND gate in Fig. 12A receive 
common inputs from the same elements.  
 
Fig. 12: Irreducible vs. Reducible Transition. (A) 
Transition and causal account of Fig. 4. (B) The cause 
repertoire of the 2nd–order occurrence {OR, AND = 10} 
with actual cause {AB = 10} is irreducible to the cause-
repertoire under the MIP. (C) Reducible transition with 
equivalent 1st–order causal links, but missing the 2nd–order 
causal link present in (A). (D) The cause repertoire of the 
2nd–order occurrence {OR, AND = 10} here is the same, 
and thus reducible, to the cause-repertoire under the MIP.   
The measure é 2/01 ? 5/  moreover quantifies the 
extent to which the causal account å of transition 2/01 ? 5/ 
is irreducible. Thus, é 2/01 ? 5/ = 0 indicates that 
2/01 ? 5/ can either be decomposed into multiple 
transitions without causal links between them (Fig. 12C,D), 
or includes irrelevant elements that should be regarded as 
background conditions (see Fig. 6 and 8) in a more 
parsimonious causal account (Fig. S1).  
4.5 Limitations and possible extensions of 
actual causation in physical causal models 
The literature on actual causation is often rooted in 
examples involving situational narratives, such as “Billy 
and Suzy throw rocks at a bottle” (Pearl, 2000; Halpern, 
2016), which are then compressed into a causal model to be 
investigated. Such narratives can serve as intuition pumps, 
but can also lead to confusion if important aspects of the 
story are omitted in the causal model fit to the example 
(Hitchcock, 2007; Paul and Hall, 2013). Issues regarding 
causation by omission, for instance, are hard to resolve 
using binary causal models in which the ‘0’ and ‘1’ states 
are merely labels that could be replaced by ‘a’ and ‘b’. 
Similarly, examples of preemption (temporal 
overdetermination) are often represented by causal models 
that do not explicitly take time into account. In such cases, 
a helpful strategy is to add abstract variables to the 
underlying causal model in order to disambiguate a 
previously inconclusive or unintuitive result (Halpern, 
2015). 
The present approach aims not so much at capturing 
common intuitions about causal judgments (Halpern, 2016), 
but rather at providing a rigorous framework for assessing 
irreducible causal strength and actual causation (“what 
caused what”) given a fully specified (complete) model of a 
physical systems of interacting elements. This puts the 
explanatory burden on the formal framework of actual 
causation, rather than on the adequacy of the model. Time 
is explicitly taken into account, as system elements are 
synchronously updated at every time step. The framework 
is applicable to a wide range of systems, deterministic and 
probabilistic, with feedforward and recurrent architectures, 
as long as they can be represented by a set of transition 
probabilities as specified in Eqn. 1. The explanatory power 
of the proposed framework was illustrated in several 
examples, which included paradigmatic problem cases 
involving overdetermination and prevention.  
The examples examined in this study have been 
purposely limited to direct transitions 2/01 ? 5/ across a 
single system update, involving binary variables. Abstract 
examples that raise issues of “causation by omission” or 
“preemption” have no direct equivalent in these basic types 
of physical causal models, and may even be physically 
impossible. Nevertheless, the present framework can be 
extended in principle to any discrete dynamical system and 
across multiple time steps by specifying the appropriate 
B
A
OR
D
C
AND
!"#$ =>{ABCD6=61010}6? &"= {OR,AND6=610}
g = gH = gL = 0 bits
MIP
C
A
-" =6{AB6=610}
B
A
X| !<#=,>&< = ?@AB C.FC.YY = 0.=E bits
B
A
AND
OR
MIP
-" =6{AD6=610}
D
A
X| !<#=,>&< = ?@AB C.YYC.YY = 0 bits
C
B
AND
OR
MIP
A
D
- ? 6? WH[\]
{A6=61}6? {OR6=61}
{D6=60}6? {AND6=60}
0.415bits
0.4156bits
0.4156bits
0.4156bits
- ? 6? WL[\]
{A6=61}? {OR6=61}
{D6=60}6? {AND6=60}
!"#$ =>{AB6=610}6? &"= {OR,AND6=610}
g = gL = 0.17>bits
- ? 6? WH[\]
{A6=61}6? {OR6=61}
{B6=60}6? {AND6=60}
0.415bits
0.4156bits
0.4156bits
0.4156bits
0.170*bits
- ? 6? WL[\]
{A6=61}? {OR6=61}
{B6=60}6? {AND6=60}
{AB*=*10}*? {OR,AND*=*10}
B
A
AND
OR
MIP
D
B
{AD6=610}6? {OR,AND6=610}
{AB6=610}6? {OR,AND6=610}
 18 
(macro) transition probabilities in Eqn. 1 (Marshall et al., 
2016a). An important area in the causation literature 
concerns causal judgments, which are often tied to notions 
of causal responsibility and blame assignment. The 
irreducible effect ratio Üa permits the comparison of the 
relative effects of different occurrences, and the irreducible 
cause ratio Üh can measure the relative contributions of 
different sets of elements to the actual cause of an 
occurrence. However, addressing the question “who caused 
what?”, as opposed to “what caused what”, implies 
modeling an agent with intrinsic causal power and intention 
(Datta et al., 2015). Future work will therefore combine the 
present mechanistic framework for actual causation with a 
mechanistic account of what it means to be an autonomous, 
causal agent, based on the same set of principles (Marshall 
et al., in press; Oizumi et al., 2014).  
 
Acknowledgements 
We thank Matteo Mainetti for early discussions 
concerning the extension of IIT to actual causation. This 
work has been supported by the Templeton World Charities 
Foundation (Grant #TWCF0067/AB41). L.A. receives 
funding from the Templeton World Charities Foundation 
(Grant #TWCF0196). 
_______________________________________ 
 
References: 
Albantakis L, Tononi G (2015) The Intrinsic Cause-Effect 
Power of Discrete Dynamical Systems—From 
Elementary Cellular Automata to Adapting Animats. 
Entropy 17:5472–5502. 
Ay N, Polani D (2008) Information Flows in Causal 
Networks. Adv Complex Syst 11:17–41. 
Balduzzi D, Tononi G (2008) Integrated information in 
discrete dynamical systems: motivation and 
theoretical framework. PLoS Comput Biol 
4:e1000091. 
Chajewska U, Halpern J (1997) Defining Explanation in 
Probabilistic Systems. In: Uncertainty in Artificial 
Intelligence 13 (Geiger D, Shenoy P, eds), pp 62–71. 
San Francisco, CA: Morgan Kaufmann. 
Datta A, Garg D, Kaynar D, Sharma D, Sinha A (2015) 
Program Actions as Actual Causes: A Building Block 
for Accountability. In: Proceedings of the Computer 
Security Foundations Workshop, pp 261–275. IEEE. 
Eells E, Sober E (1983) Probabilistic causality and the 
question of transitivity. Philos Sci 50:35–57. 
Glennan S (2011) Singular and General Causal Relations: A 
Mechanist Perspective. Causality Sci:789. 
Good IJI (1961) A Causal Calculus I. Br J Philos Sci 
11:305–318. 
Halpern JY (2015) A Modification of the Halpern-Pearl 
Definition of Causality. arXiv 1505.00162. 
Halpern JY (2016) Actual Causality. Cambridge, MA: MIT 
Press. 
Halpern JYJ, Pearl J (2005) Causes and explanations: A 
structural-model approach. Part I: Causes. Br J Philos 
Sci 56:843–887. 
Hitchcock C (2007) Prevention, Preemption, and the 
Principle of Sufficient Reason. Philos Rev 116:495–
532. 
Hoel EP, Albantakis L, Tononi G (2013) Quantifying 
causal emergence shows that macro can beat micro. 
PNAS 110:19790–19795. 
Hopkins M, Pearl J (2003) Clarifying the Usage of 
Structural Models for Commonsense Causal 
Reasoning. In: Proceedings of the AAAI Spring 
Symposium on Logical Formalizations of 
Commonsense Reasoning, pp 83–89. Menlo Park, 
CA: AAAI Press. 
Illari M, Phyllis FR, Williamson J eds. (2011) Causality in 
the Sciences. Oxford: Oxford University Press. 
Janzing D, Balduzzi D, Grosse-Wentrup M, Schölkopf B 
(2013) Quantifying causal influences. Ann Stat 
41:2324–2358. 
Korb KB, Nyberg EP, Hope L (2011) A new causal power 
theory. In: Causality in the Sciences. Oxford: Oxford 
University Press. 
Lewis D (1973) Counterfactuals. Cambridge, MA: Harvard 
University Press. 
Lewis D (1986) Philosophical Papers, volume II. Oxford 
University Press, Oxford. 
Marshall W, Albantakis L, Tononi G (2016a) Black-boxing 
and cause-effect power. arXiv 1608.03461. 
Marshall W, Gomez-Ramirez J, Tononi G (2016b) 
Integrated Information and State Differentiation. 
Front Psychol 7:926. 
Marshall W, Kim H, Walker SI, Tononi G, Albantakis L (in 
press) How causal analysis can reveal autonomy in 
models of biological systems. Philos Trans R Soc A. 
Oizumi M, Albantakis L, Tononi G (2014) From the 
Phenomenology to the Mechanisms of 
Consciousness: Integrated Information Theory 3.0. 
PLoS Comput Biol 10:e1003588. 
Paul LA, Hall EJ (2013) Causation: A user’s guide. Oxford, 
UK: Oxford University Press. 
Pearl J (1988) Probabilistic Reasoning in Intelligent 
Systems: Networks of Plausible Inference. Morgan 
Kaufmann Publishers. 
Pearl J (2000) Causality: models, reasoning and inference. 
Cambridge Univ Press. 
Pearl J (2009) The Structural Theory of Causations. In: 
Causality in the Sciences. Oxford: Oxford University 
 19 
Press. 
Shimony SE (1991) Explanation, irrelevance and statistical 
independence. In: Proceedings of the ninth National 
conference on Artificial intelligence-Volume 1, pp 
482–487. AAAI Press. 
Suppes P (1970) A Probabilistic Theory of Causality. 
Amsterdam: North-Holland Pub. Co. 
Tononi G (2015) Integrated information theory. 
Scholarpedia 10:4164. 
Tononi G, Boly M, Massimini M, Koch C (2016) Integrated 
information theory: from consciousness to its physical 
substrate. Nat Rev Neurosci 17:450–461. 
Tononi G, Sporns O, Edelman GM (1999) Measures of 
degeneracy and redundancy in biological networks. 
Proc Natl Acad Sci U S A 96:3257–3262. 
Woodward J (2004) Counterfactuals and causal 
explanation. Int Stud Philos Sci 18:41–72. 
Woodward J (2010) Causation in biology: stability, 
specificity, and the choice of levels of explanation. 
Biol Philos 25:287–318. 
Yablo S (2004) Advertisement for a Sketch of an Outline of 
a Prototheory of Causation. In: Causation and 
Counterfactuals (Hall N, Paul LA, Collins J, eds), pp 
119–137. Cambridge, MA: MIT Press. 
 
 1 
Supplementary Material 
 
Supplementary Fig. S1: 
 
Fig. S1: Linear threshold unit with background condition. Transition and causal account of the linear threshold unit of Fig. 
5 (main text) with four inputs and threshold k = 3, now with {D = 0} as a fixed background condition. In this case, the causal 
account is irreducible with higher Ütæ_ values. 
 
 
Supplementary Fig. S2:  
 
Fig. S2: Disjunction of two conjunctions Æ	 ? Ø ? ?, extended. (A) Background condition. Transition and causal account 
of the example in Fig. 6 with {B = 0} as a fixed background condition. The causal account of this transition is still reducible. 
This is because, given {B = 0}, {A = 1} does not have an actual effect on {D = 1} (and, as in Fig. 6, is also not its actual 
cause). (B) Asymmetric overdetermination. Shown is the causal account of the same system as shown in Fig. 6, but for 
transition {ABC = 111} ? {D = 1}. The actual cause of {D = 1} is {C = 1}, the smallest sufficient condition. Note also that 
under this transition the causal account is irreducible. 
B
A
M
MIP
i M" = 1 = 1
iff			?(ABC)   ? 3
D
C
g = 5.0 bits
!"#$ =	{ABC	=	111}	? &"= {M =	1}
background
- ? 6? WH[\]
{A	=	1}	? {M	=	1}
{B	=	1}	? {M	=	1}
{C	=	1}	? {M	=	1}
{AB	=	11}	? {M	=	1}
{AC	=	11}	? {M	=	1}
{BC	=	11}	? {M	=	1}
{ABC	=	111}	? {M	=	1}
1.0	bits
1.0	bits
1.0	bits
1.0	bits
1.0	bits
1.0	bits
1.0	bits
3.0	bits
- ? 6? WL[\]
{ABC	=	111}	? {M	=	1}
background
B
A
D
C
MIP
0
0
1
1p
as
t6s
ta
te
6t–
1
A66C6i q" = 1
B
A
DMIP
i q" = 1 = 1
if666?ABr#$   > 1
? Cr#$ = 1
C
0
0
0
1
1
1
1
1
pa
st
6st
at
e6
t–
1
A66B66C6 i q" = 1
A B
!"#$ =>{AC6=611}6? &"= {D6=61}
g = gH = 0 bits
g = gH = gL>= 0.678 bits
!"#$ =>{ABC6=6111}6? &"= {D6=61}
- ? 6? WH[\]
{C6=61}6? {D6=61} 1.06bits
1.06bits
- ? 6? WL[\]
{C6=61}? {D6=61}
- ? 6? WH[\]
{A6=61}6? {D6=61}
{B6=61}6? {D6=61}
{C6=61}6? {D6=61}
{AB6=61}6? {D6=61}
0.2636bits
0.2636bits
0.6786bits
0.4156bits
0.6786bits
- ? 6? WL[\]
{C6=61}? {D6=61}
Supplementary Proof 1
The first theorem describes the actual causes and effects for an observation of a linear threshold unit (LTU) Y
t
with
n inputs and threshold k, and its inputs X
t 1. First, a series of lemmas are demonstrated based on the transition
probabilities q
c,j
: if X
t 1 = xt 1 such that the size of the occurrence is |xt 1| = c and j of the c elements in Xt 1
are in the ‘ON’ state (
P
x2xt 1 x = j), then
q
c,j
= p(Y
t
= 1|X
t 1 = xt 1) =
8
>>><
>>>:
n cP
i=k j
1
2
n c
 
n c
i
 
if j ? k and j > k   (n  c)
1 if j   k
0 if j < k   (n  c)
First we demonstrate that the probabilities q
c,j
are non-decreasing as the number of ‘ON’ inputs j increases for a
fixed size of occurrence c, and that there is a specific range of values of j and c such that the probabilities are strictly
increasing.
Lemma 1.1. q
c,j
  q
c,j 1 with qc,j = qc,j 1 iff j > k or j < k   (n  c).
Proof. If j > k then
q
c,j
= q
c,j 1 = 1.
If j < k   (n  c) then
q
c,j
= q
c,j 1 = 0.
If k   (n  c) ? j ? k then
q
c,j
=
1
2n c
n cX
i=k j
?
n  c
i
?
=
1
2n c
n cX
i=k (j 1)
?
n  c
i
?
  1
2n c
= q
c,j 1 +
1
2n c
> q
c,j 1
Next we demonstrate two results relating the transition probabilities between occurrences of different sizes,
Lemma 1.2. q
c,j
= 1
2
(q
c+1,j
+ q
c+1,j+1
) for 1 ? c < n and 0 ? j ? c.
Proof. If j   k then
q
c,j
= q
c+1,j
= q
c+1,j+1
= 1,
so
q
c,j
=
1
2
(q
c+1,j
+ q
c+1,j+1
) = 1.
If j = k   1 then
q
c+1,j+1
= 1,
and
q
c,j
= q
c,k 1 =
1
2n c
n cX
i=1
?
n  c
i
?
=
1
2n c
0
@1 +
n (c+1)X
i=1
?
n  (c+ 1)
i  1
?
+
?
n  (c+ 1)
i
?1
A
=
1
2
0
@ 1
2n (c+1)
n (c+1)X
i=1
?
n  (c+ 1)
i
?
+
1
2n (c+1)
n (c+1)X
i=0
?
n  (c+ 1)
i
?1
A
=
1
2
(q
c+1,j
+ 1)
=
1
2
(q
c+1,j
+ q
c+1,j+1
)
1
If j < k   (n  c) then
q
c,j
= q
c+1,j
= q
c+1,j+1
= 0,
so
q
c,j
=
1
2
(q
c+1,j
+ q
c+1,j+1
) = 0.
If j = k   (n  c) then
q
c+1,j
= 0,
and
q
c,j
= q
c,k (n c) =
1
2n c
n cX
i=n c
?
n  c
i
?
=
1
2n c
n (c+1)X
i=n (c+1)
?
n  (c+ 1)
i
?
=
1
2
q
c+1,j+1
=
1
2
(q
c+1,j+1
+ q
c+1,j
).
Finally, if k   (n  c) + 1 < j < k   1 then
q
c,j
=
1
2n c
n cX
i=k j
?
n  c
i
?
=
1
2n c
0
@1 +
n (c+1)X
i=k j
?
n  c
i
?1
A
=
1
2n c
0
@1 +
n (c+1)X
i=k j
?
n  (c+ 1)
i
?
+
n (c+1)X
i=k j
?
n  (c+ 1)
i  1
?1
A
=
1
2n c
0
@
n (c+1)X
i=k j
?
n  (c+ 1)
i
?
+
0
@1 +
n (c+1)X
i=k j
?
n  (c+ 1)
i  1
?1
A
1
A
=
1
2n c
0
@
n (c+1)X
i=k j
?
n  (c+ 1)
i
?
+
n (c+1)X
i=k (j+1)
?
n  (c+ 1)
i
?1
A
=
1
2
(q
c+1,j
+ q
c+1,j+1
)
Lemma 1.3. If c < k then q
c,c
< q
c+1,c+1
Proof.
q
c,c
=
1
2
(q
c+1,c
+ q
c+1,c+1
) (Lemma 1.2)
<
1
2
(q
c+1,c+1
+ q
c+1,c+1
) (Lemma 1.1)
= q
c+1,c+1
Finally, we consider a quantity Q(c), the sum of q over all possible states for an occurrence of size c. The value
Q(c) acts as a normalization term when calculating the cause repertoire of occurrence {Y
t
= 1}. Here we demonstrate
a relationship between these normalization terms across occurrences of different sizes,
Lemma 1.4. Let Q(c) =
cP
j=0
 
c
j
 
q
c,j
then Q(c) = 1
2
Q(c+ 1)
2
Proof.
Q(c) =
cX
j=0
?
c
j
?
q
c,j
=
1
2
cX
j=0
?
c
j
?
(q
c+1,j
+ q
c+1,j+1
)
=
1
2
0
@
cX
j=1
?
c
j   1
?
q
c+1,j
+
cX
j=0
?
c
j
?
q
c+1,j
1
A
=
1
2
0
@
q
c+1,c+1
+ q
c+1,0
+
cX
j=1
?
c+ 1
j
?
q
c+1,j
1
A
=
1
2
0
@
c+1X
j=0
?
c+ 1
j
?
q
c+1,j
1
A
=
1
2
Q(c+ 1)
Using the above lemmas, we are now in a position to prove the actual causes and actual effects in the causal
account of a single LTU in the ‘ON’ state. The causal account for a LTU in the ‘OFF’ state follows by symmetry.
Theorem 1.5. For a transition x
t 1 ! yt, where Yt is a single LTU with n inputs and threshold k, and Xt 1 are
its n inputs, the following holds:
1. The actual cause of {Y
t
= 1} is {X
t 1 = xt 1} where |xt 1| = k and min(xt 1) = 1. Furthermore, the causal
strength of the link is
?
max
c
(y
t
) = k   log
2
0
@
kX
j=0
q
k,j
1
A
2. If min(x
t 1) = 1 and |xt 1| ? k then the actual effect of {Xt 1 = xt 1} is {Yt = 1} with causal strength
?
e
(x
t 1, yt) = log
2
?
q
c,c
q
c 1,c 1
?
> 0,
otherwise {X
t 1 = xt 1} is reducible.
Proof. Part 1: Consider an occurrence {X
t 1 = xt 1} such that |xt 1| = c ? n and
P
x2xt 1
x = j, the probability of
x
t 1 in the cause-repertoire of yt is
?(x
t 1|yt) =
q
c,j
Q(c)
.
Since Y
t
is a first-order occurrence, there is only one possible partition, and the causal strength of a potential link is
thus
?
c
(x
t 1, yt) = ?c(xt 1, yt) = log
2
?
?(x
t 1|yt)
?(x
t 1)
?
= log
2
?
2cq
c,j
Q(c)
?
.
For a fixed value of c, the maximum value of causal strength occurs at j = c (since adding ‘ON’ elements can only
increase q(c, j), Lemma 1.1),
3
max
|xt 1|=c
?
c
(x
t 1, yt) = max
j
log
2
?
2cq
c,j
Q(c)
?
= log
2
?
2cq
c,c
Q(c)
?
Applying Lemma 1.3 and Lemma 1.4, we see that across different values of c, this maximum is increasing for
0 < c < k,
max
|xt 1|=c+1
?
c
(x
t 1, yt)  max
|xt 1|=c
?
c
(x
t 1, yt) = log
2
?
2c+1q
c+1,c+1
Q(c+ 1)
?
  log
2
?
2cq
c,c
Q(c)
?
= log
2
?
2c+1q
c+1,c+1
Q(c)
2cq
c,c
Q(c+ 1)
?
= log
2
?
q
c+1,c+1
q
c,c
?
> 0,
and that for k ? c the causal strength is constant,
max
|xt 1|=c+1
?
c
(x
t 1, yt)  max
|xt 1|=c
?
c
(x
t 1, yt) = log
2
?
2c+1q
c+1,c+1
Q(c+ 1)
?
  log
2
?
2cq
c,c
Q(c)
?
= log
2
?
q
c+1,c+1
q
c,c
?
= log
2
?
1
1
?
= 0.
Therefore, the maximum causal strength (?max
c
(y
t
)) can only occur if c   k. We then find the actual cause of
{Y
t
= 1} by minimizing |x
t 1| = c subject to ?c(xt 1, yt) = ?max
c
(y
t
). Since c   k, we have j = c = k, the actual
cause is any set of c = k elements X
t 1 such that |xt 1| = c and
P
x2xt 1
x = c (an occurrence of size c where each
element is ‘ON’). Furthermore, when c = j = k,
?
max
c
(y
t
) = log
2
?
2kq
k,k
Q(k)
?
= k   log
2
0
@
kX
j=0
q
k,j
1
A
> 0.
Part 2: Again, consider occurrences X
t 1 = xt 1 with |xt 1| = c and
P
x2xt 1
x = j. The probability of y
t
in the
effect repertoire of such an occurrence is
?(y
t
|x
t 1) = qc,j =
8
>>><
>>>:
n cP
i=k j
1
2
n c
 
n c
i
 
if j ? k and j > k   (n  c)
1 if j   k
0 if j < k   (n  c)
First, if j < c, then 9 x = 0 2 x
t 1 and we can define a partition P = {{(xt 1   x), yt}, {x,?}} such that
?
P
(y
t
|x
t 1) = ?(yt|(xt 1   x))? ?(?|x) = ?(yt|(xt 1   x)) = qc 1,j
and
?
e
(x
t 1, yt) ? log
2
?
?(y
t
|x
t 1)
?
P
(y
t
|x
t 1)
?
= log
2
?
q
c,j
q
c 1,j
?
? 0 (Lemma1.1/1.2),
so x
t 1 is reducible. Next we consider the case where j = c but c > k. In this case we define a partition
P = {{(x
t 1   x), yt}, {x,?}} (where x 2 xt 1 is any element), such that
4
?
P
(y
t
|x
t 1) = ?(yt|(xt 1   x))? ?(?|x) = ?(yt|(xt 1   x)) = qc 1,c 1,
and since c > k,
?
e
(x
t 1, yt) ? log
2
?
?(y
t
|x
t 1)
?
P
(y
t
|x
t 1)
?
= log
2
?
q
c,c
q
c 1,c 1
?
= log
2
?
1
1
?
= 0,
so x
t 1 is again reducible. Lastly, we show that for j = c and c ? k, that xt 1 is irreducible with actual effect
{Y
t
= 1}. All possible partitions of the pair of occurrences can be formulated as P = {{(x
t 1   x), yt}, {x,?}}
(where x ? x
t 1 with |x| = d > 0), such that
?
P
(y
t
|x
t 1) = ?(yt|(xt 1   x))? ?(?|x) = ?(yt|(xt 1   x)) = qc d,c d,
and
?
e
(x
t 1, yt) = min
P
log
2
?
?(y
t
|x
t 1)
?
P
(y
t
|x
t 1)
?
= min
d
log
2
?
q
c,c
q
c d,c d
?
.
The minimum information partition occurs when d = 1 (Lemma 1.3) and thus {X
t 1 = xt 1} is irreducible with
actual effect {Y
t
= 1} and causal strength
?
e
(x
t 1, yt) = log
2
?
q
c,c
q
c 1,c 1
?
.
Supplementary Proof 2
The second theorem describes the actual causes and effects for an observation of a disjunction of conjunctions (DOC)
Y
t
that is a disjunction of k conjunctions, each over n
j
elements, and its inputs X
t 1. The total number of inputs to
the DOC element is n =
P
k
j=1
n
j
. We consider occurrences x
t 1 that contain cj ? nj elements from each of the k
conjunctions, and the total number of elements is |x
t 1| = c =
P
k
j=1
c
j
. To simplify notation, we further define
x?
j,t 1 = {xi,j,t 1}
nj
i=1
, an occurrence with c
j
= n
j
and c
j
0 = 0 if j0 6= j. In other words, x?
j,t 1 is the set of elements
that make up the jth conjunction. First, a series of lemmas are demonstrated based on the transition probabilities
q(s):
q(s) = p(Y
t
= 1|X
t 1 = s)
To isolate the specific conjunctions, we define s
j
? x
t 1 to be the state of Xt 1 within the jth conjunction, and
s?
j
= [j
i=1
s
i
? x
t 1 be the state Xt 1 within the first j conjunctions. For a DOC with k conjunctions, we consider
occurrences with c
j
elements from each conjunction, X
t 1 = {{xi,j,t 1}
cj
i=1
}k
j=1
. In the specific case of a disjunction
of two conjunctions,
q(s
1
, s
2
) =
8
>>><
>>>:
0 if min(s
1
) = min(s
2
) = 0
1
2
n1 c1 if min(s1) = 1, min(s2) = 0
1
2
n2 c2 if min(s1) = 0, min(s2) = 1
2
n1 c1
+2
n2 c2 1
2
n1+n2 c1 c2 if min(s1) = min(s2) = 1,
and in the case of k > 2 conjunctions, we define the probability recursively
q(s?
k 1, sk) =
(
q(s?
k 1) if min(sk) = 0
q(s?
k 1) +
(1 q(s?k 1))
2
nk ck if min(sk) = 1
The first two lemmas demonstrate the effect of adding an additional element to an occurrence. Adding an ‘ON’
input to an occurrence x
t 1 can never decrease the probability of {Yt = 1}, while adding an ‘OFF’ input to an
occurrence x
t 1 can never increase the probability of {Yt = 1}.
5
Lemma 2.1. If {x
t 1 = s} = {x0
t 1 = s
0
, x
i,j,t 1 = 1}, then q(s0) ? q(s).
Proof. The proof is given by induction. We first consider the case where k = 2. Assume w.l.g. that the additional
element x
i,j,t 1 is from the first conjunction (c1 = c0
1
+ 1, c
2
= c0
2
). If min(s0
1
) = 0 then q(s0) = q(s). If min(s0
2
) = 0
and min(s0
1
) = 1 then
q(s0)
q(s)
=
2n1 (c
0
1+1)
2n1 c
0
1
=
1
2
< 1,
so q(s0) < q(s). Lastly, if min(s0
1
) = min(s0
2
) = 1 then
q(s0)
q(s)
=
2n1+n2 (c
0
1+1) c
0
2(2n1 c
0
1 + 2n2 c
0
2   1)
2n1+n2 c
0
1 c02(2n1 (c
0
1+1) + 2n2 c
0
2   1)
=
2n1 c
0
1 + 2n2 c
0
2   1
2n1 c
0
1 + 2(2n2 c
0
2   1)
< 1.
Therefore, when k = 2 we have that q(s0) ? q(s). Next, we assume the result holds for k   1, q(s?0
k 1) ? q(s?k 1) and
demonstrate the result for general k. Again, assume the additional element is from the first conjunction
(c
1
= c0
1
+ 1, c
j
= c0
j
for j > 1). If min(s
k
) = 0 then
q(s?0
k
)
q(s?
k
)
=
q(s?0
k 1)
q(s?
k 1)
? 1,
and if min(s
k
) = 1 then
q(s?0
k
)
q(s?
k
)
=
q(s?0
k 1) + (1  q(s?0k 1))/2nk ck
q(s?
k 1) + (1  q(s?k 1))/2nk ck
=
(2nk ck   1)q(s?0
k 1) + 1
(2nk ck   1)q(s?
k 1) + 1
? 1.
Lemma 2.2. If {x
t 1 = s} = {x0
t 1 = s
0
, x
i,j,t 1 = 0}, then q(s0)   q(s).
Proof. The proof is given by induction. We first consider the case where k = 2. Assume w.l.g. that the additional
element is from the first conjunction (c
1
= c0
1
+ 1, c
2
= c0
2
). If min(s0
1
) = 0 then q(s0) = q(s). If min(s0
2
) = 0 and
min(s0
1
) = 1 then
q(s0) =
1
2n1 c
0
1
> 0 = q(s)
Lastly, if min(s0
1
) = min(s0
2
) = 1 then
q(s0)
q(s)
=
2n2 c
0
2(2n1 c
0
1 + 2n2 c
0
2   1)
2n1+n2 c
0
1 c02
=
2n1 c
0
1 + 2n2 c
0
2   1
2n1 c
0
1
  1
Therefore, when k = 2 we have that q(s0)   q(s). Next, we assume the result holds for k   1, q(s?0
k 1)   q(s?k 1) and
demonstrate the result for general k. Again, assume the additional element is from the first conjunction
(c
1
= c0
1
+ 1, c
j
= c0
j
for j > 1). If min(s
k
) = 0 then
q(s?0
k
)
q(s?
k
)
=
q(s?0
k 1)
q(s?
k 1)
? 1,
and if min(s
k
) = 1 then
q(s?0
k
)
q(s?
k
)
=
q(s?0
k 1) + (1  q(s?0k 1))/2nk ck
q(s?
k 1) + (1  q(s?k 1))/2nk ck
=
(2nk ck   1)q(s?0
k 1) + 1
(2nk ck   1)q(s?
k 1) + 1
  1.
6
Next, we again consider a normalization term Q(c), which is the sum of q(s) over all states of the occurrence.
Here we demonstrate the effect on Q(c) of adding an additional element to an occurrence.
Lemma 2.3. For an occurrence {X
t 1 = xt 1} with |xt 1| = c > 0, define Q(c) =
P
s
q(s). Now consider adding a
single element to an occurrence, x0
t 1 = {xt 1, xi,j1,t 1}, (xi,j1,t 1 /2 xt 1) such that c0
j1
= c
j1 + 1 and c
0
j
= c
j
for
j 6= j
1
, so that c0 = c+ 1. Then Q(c
0
)
Q(c)
= 2.
Proof. The proof is again given by induction. We first consider the case where k = 2,
Q(c) =
X
s
q(s)
=
2c1   1
2n2 c2
+
2c2   1
2n1 c1
+
2n1 c1 + 2n2 c2   1
2n1+n2 c1 c2
=
2n1 + 2n2   1
2n1+n2 c1 c2
Assume w.l.g. that the additional element to the first conjunction (c0
1
= c
1
+ 1). Then we have that
Q(c0)
Q(c)
=
2n1+n2 c1 c2(2n1 + 2n2   1)
2n1+n2 c
0
1 c02(2n1 + 2n2   1)
=
2n1+n2 c1 c2
2n1+n2 (c1+1) c2
= 2
Therefore, when k = 2 we have that Q(c
0
)
Q(c)
= 2. Next, we assume the result holds for k   1 and demonstrate the result
for general k. Using the recursive relationship for q, we get
Q
k
(c) =
X
s?k
q(s?
k
)
=
X
sk
X
s?k 1
q(s?
k 1, sk)
= (2ck   1)
X
s?k 1
q(s?
k 1) +
X
s?k 1
?
q(s?
k 1) +
(1  q(s?
k 1))
2nk ck
?
=
(2nk   1)Q
k 1(c  ck) + 2c ck
2nk ck
,
Again, assume the additional element is from the first conjunction c0
1
= c
1
+ 1, for the ratio we have
Q
k
(c0)
Q
k
(c)
=
(2nk   1)Q
k 1(c0   c0
k
) + 2c
0 c0k
(2nk   1)Q
k 1(c  ck) + 2c ck
=
(2nk   1)2Q
k 1(c  ck) + 2(c ck)+1
(2nk   1)Q
k 1(c  ck) + 2c ck
= 2
?
(2nk   1)Q
k 1(x0
t 1) + 2
c ck
(2nk   1)Q
k 1(x0
t 1) + 2
c ck
?
= 2
The final two Lemmas demonstrate conditions under which the probability of {Y
t
= 1} is either strictly increasing
or strictly decreasing.
7
Lemma 2.4. If min(x
t 1) = 1 , cj < nj 8 j and x0
t 1 ? xt 1 then q(s0) < q(s).
Proof. The proof is given by induction. We first consider the case where k = 2. Assume w.l.g. that x
t 1 has an
additional element in the first conjunction relative to x0
t 1 (c1 = c
0
1
+ 1, c
2
= c0
2
). The result can be applied
recursively for differences of more than one element.
q(s)
q(s0)
=
?
2n1 c1 + 2n2 c2   1
2n1 c
0
1 + 2n2 c
0
2   1
? 
2n1+n2 c
0
1 c
0
2
2n1+n2 c1 c2
!
= 2
?
2n1 c1 + 2n2 c2   1
2n1 c1+1 + 2n2 c2   1
?
> 1 (since c
2
< n
2
)
Therefore, when k = 2 we have that q(s0) < q(s). Next, we assume the result holds for k   1, q(s?0
k 1) < q(s?k 1) and
demonstrate the result for general k. Again, assume that x
t 1 and x0
t 1 differ by a single element in the first
conjunction (c
1
= c0
1
+ 1, c
j
= c0
j
for j > 1). Since min(s
k
) = 1,
q(s?
k
)
q(s?0
k
)
=
q(s?0
k 1) + (1  q(s?0k 1))/2nk ck
q(s?
k 1) + (1  q(s?k 1))/2nk ck
=
(2nk ck   1)q(s?0
k 1) + 1
(2nk ck   1)q(s?
k 1) + 1
> 1.
Lemma 2.5. If max(x
t 1) = 0 , cj ? 1 8 j and x0
t 1 ? xt 1 then q(s) < q(s0).
Proof. The proof is given by induction. We first consider the case where k = 2. Assume w.l.g. that x
t 1 has an
additional element in the first conjunction relative to x0
t 1 (c1 = c
0
1
+ 1 = 1, c
2
= c0
2
). The result can be applied
recursively for differences of more than one element. First, consider the case where c
2
= 1. Then we have
q(s0) =
1
2n2 c2
> 0 = q(s).
Next consider the case where c
2
= 0:
q(s0) =
2n1 + 2n2   1
2n1+n2
=
1
2n2
?
2n1 + 2n2   1
2n1
?
= q(s)
?
2n1 + 2n2   1
2n1
?
> q(s).
Therefore, when k = 2 we have that q(s) < q(s0). Next, we assume the result holds for k   1, q(s?
k 1) < q(s?0
k 1), and
demonstrate the result for general k. Again, assume that x
t 1 and x0
t 1 differ by a single element in the first
conjunction (c
1
= c0
1
+ 1, c
j
= c0
j
for j > 1). Since min(s
k
) = 0,
q(s?
k
)
q(s?0
k
)
=
q(s?
k 1)
q(s?0
k 1)
< 1.
Using the above Lemmas, we are now in a position to prove the actual causes and actual effects in the causal
account of a single DOC and its inputs. We separately consider the case where the DOC is in the ‘ON’ and the ‘OFF’
state.
Theorem 2.6. For a transition x
t 1 ! yt, where Yt is a single DOC element that is a disjunction of k conditions,
each of which is a conjunction of n
j
inputs, and X
t 1 = {x?j,t 1}k
j=1
= {{x
i,j,t 1}
nj
i=1
}k
j=1
is the set of its n =
P
j
n
j
inputs, the following holds:
8
1. If y
t
= 1,
(a) The actual cause of {Y
t
= 1} is {X
t 1 = xt 1} where xt 1 = {x?j,t 1} ? xt 1 such that min(x?j,t 1) = 1
and if min(x?
j
0
,t 1) = 1 then nj ? nj0 .
(b) The actual effect of {X
t 1 = xt 1} is {Yt = 1} if min(xt 1) = 1 and cj = nj ) cj0 = 0, j 6= j0, otherwise
x
t 1 is reducible.
2. If y
t
= 0,
(a) The actual cause of {Y
t
= 0} is x
t 1 ? xt 1 such that max(xt 1) = 0 and cj = 1 8 j.
(b) If max(x
t 1) = 0 and cj ? 1 8 j then the actual effect of {Xt 1 = xt 1} is {Yt = 0}; otherwise xt 1 is
reducible.
Proof. Part 1a: The actual cause of {Y
t
= 1}. For an occurrence {X
t 1 = xt 1}, the probability of xt 1 in the
cause repertoire of y
t
is
?(x
t 1 | yt) =
q(s)
Q(c)
.
Since Y
t
is a first-order occurrence, there is only one possible partition, and the causal strength of a potential link is
thus
?
c
(x
t 1, yt) = log
2
?
?(x
t 1 | yt)
?(x
t 1)
?
= log
2
?
2cq(s)
Q(c)
?
= log
2
(Q
1
q(s)) ,
where Q
1
= 2
c
Q(c)
8 c (Lemma 2.3). If we then consider adding a single element to the occurrence
x
0
t 1 = {xt 1, x0
i,j,t 1} (x0i,j,t 1 /2 xt 1) then the difference in causal strength is
?
c
(x
t 1, yt)  ?c(x0
t 1, yt) = log2
?
Q
1
q(s)
Q
1
q(s0)
?
= log
2
?
q(s)
q(s0)
?
Combining the above with Lemma 2.2, adding an element x
i,j,t 1 = 0 to an occurrence cannot increase the causal
strength, and thus occurrences that include elements in state ‘OFF’ cannot be the actual cause of y
t
. By Lemma 2.1,
adding an element x
i,j,t 1 = 1 to an occurrence cannot decrease the causal strength. Furthermore, if cj = nj and
min(x?
j,t 1) = 1, then q(s) = 1 and
?
c
(y
t
, x
t 1) = log
2
(Q
1
q(s)) = log
2
(Q
1
),
independent of the number of elements in the occurrence from other conjunctions c
j
0 and their states s
j
0 (j0 6= j).
Since the value Q
1
does not depend on the specific value of j, it must be the case that this is the maximum value of
causal strength, ?max(y
t
). Furthermore, if c
j
< n
j
8 j then
?
c
(y
t
, x
t 1) = log
2
(Q
1
q(s)) < log
2
(Q
1
) .
Therefore, the maximum value of causal strength is
log
2
(Q
1
) ,
and this value occurs if and only if there exists j such that c
j
= n
j
and min(x?
j,t 1 = 1), i.e. the occurrence includes
a conjunction whose elements are all ‘ON’. Finally, minimizing |X
t 1| subject to this constraint gives Xt 1 = x?j,t 1
such that c
j
= n
j
, min(x?
j,t 1) = 1, and min(x?j0,t 1) = 1 ) cj ? cj0
Part 1b: Again, consider occurrences X
t 1 = xt 1 with cj elements from each of the k conjunctions. The effect
repertoire of a DOC with k conjunctions over such occurrences is
?(y
t
| x
t 1 = s) = q(s)
9
First, if there exists x 2 x
t 1 with x = 0 then we can define x0
t 1 such that xt 1 = {x0t 1, x}, and a partition
P =
 
{x0
t 1, yt}, {x,?}
 
, i.e. cutting away x, such that
?(y
t
| x
t 1)P = ?(yt | x0
t 1)? ?(? | x) = ?(yt | x0t 1) = q(s0),
By Lemma 2.2, q(s0)   q(s), and thus
?
e
(x
t 1, yt) ? log
2
?
?(y
t
| x
t 1)
?(y
t
| x
t 1)P
?
= log
2
?
q(s)
q(s0)
?
? 0,
so x
t 1 is reducible. Next we consider the case where min(xt 1) = 1, but there exists j1, j2 such that cj1 = nj1 and
c
j2 > 0. We define x
0
t 1 = x?j 1,t 1 and a partition P =
 
{x0
t 1, yt}, (xt 1 \ x0t 1),?}
 
, such that
?(y
t
| x
t 1)P = ?(yt | x0
t 1)? ?(? | (xt 1   x0t 1)) = ?(yt | x0t 1) = q(s0),
and thus
?
e
(x
t 1, yt) ? log
2
?
?(y
t
| x
t 1)
?(y
t
| x
t 1)P
?
= log
2
?
q(s)
q(s0)
?
= log
2
?
1
1
?
= 0,
so x
t 1 is again reducible. We now split the irreducible occurrences into two cases. First, we consider min(xt 1) = 1
and all c
j
< n
j
. All possible partitions of the pair of occurrences can be formulated as
P =
 
x
0
t 1, yt}, {(xt 1 \ x0t 1),?}
 
(where x0
t 1 ? xt 1), such that
?(y
t
| x
t 1)P = ?(yt | x0
t 1))? ?(? | (xt 1   x0t 1) = ?(yt | x0t 1) = q(s0),
and by Lemma 2.4,
?
e
(x
t 1, yt) = min
P
?
log
2
?
?(y
t
| x
t 1)
?(y
t
| x
t 1)P
??
= min
P
?
log
2
?
q(s)
q(s0)
??
> 0.
So x
t 1 is irreducible, and its actual effect is {Y1 = 1}. Next we consider occurrences such that min(xt 1) = 1 and
all c
j1 = nj1 and cj = 0 for j 6= j1 (i.e. xt 1 = x?j1,t 1). All possible partitions of the pair of occurrences can be
formulated as P =
 
{x0
t 1, yt}, {(xt 1   x0t 1,?}
 
(where x0
t 1 ? xt 1), such that
?(y
t
| x
t 1)P = ?(yt | x0
t 1)? ?(? | (xt 1   x0t 1) = ?(yt | x0t 1) = q(s0),
?
e
(x
t 1, yt) ? log
2
?
?(y
t
| x
t 1)
?(y
t
| x
t 1)P
?
= log
2
?
q(s)
q(s0)
?
= log
2
?
1
q(s0)
?
> 0,
and x
t 1 is again irreducible with actual effect {Yt = 1}.
Part 2a: The actual cause of {Y
t
= 0}. For an occurrence {X
t 1 = xt 1} the cause repertoire of yt is
?(x
t 1 | yt) =
1  q(s)
2c  Q(c) .
Since Y
t
is a first-order occurrence, there is only one possible partition, and the causal strength of a potential link is
thus
?
c
(x
t 1, yt) = log
2
?
?(x
t 1 | yt)
?(x
t 1)
?
= log
2
?
2c(1  q(s))
2c  Q(c)
?
= log
2
(Q
0
q(s)) ,
where Q
0
= 2
c
2
c Q(c) 8 c (Lemma 2.3). If we then consider adding a single element to the occurrence
x
0
t 1 = {xt 1, x0
i,j,t 1} (x0i,j,t 1 /2 xt 1), then the difference in causal strength is
?
c
(x
t 1, yt)  ?c(x0
t 1, yt) = log2
?
Q
0
(1  q(s))
Q
0
(1  q(s0))
?
= log
2
?
1  q(s)
1  q(s0)
?
10
By Lemma 2.2, adding an element x = 1 to an occurrence cannot increase the causal strength, and thus occurrences
that include elements in state ‘ON’ cannot be the actual cause of y
t
. By Lemma 2.1, adding an element x = 0 to an
occurrence cannot decrease the causal strength. Furthermore, if c
j
> 0 8 j and max(x
t 1) = 0, then
?
c
(y
t
, x
t 1) = log
2
(Q
0
(1  q(s))) = log
2
(Q
0
) ,
independent of the actual values of c
j
. Since this holds for any set of c
j
that satisfies the conditions, it must be the
case that this value is ?max(y
t
). Furthermore, if there exists j such that c
j
= 0 then
?
c
(y
t
, x
t 1) = log
2
(Q
0
(1  q(s))) < log
2
(Q
0
) .
Therefore, the maximum value of causal strength is
log
2
(Q
0
) ,
and this value occurs if and only if c
j
> 0 8 j and max(x
t 1 = 0), i.e. the occurrence contains elements from every
conjunction, and only elements whose state is ‘OFF’. Finally, minimizing |X
t 1| subject to this constraint gives any
occurrence |X
t 1| = k such that cj = 1 8 j and max(xt 1) = 0.
Part 2b: Again, consider occurrences X
t 1 = xt 1 with cj elements from each of k conjunctions. The probability
of y
t
in the effect repertoire of x
t 1 is
?(y
t
| x
t 1 = s) = 1  q(s).
First, if there exists x
i,j,t 1 2 xt 1 such that xi,j,t 1 = 1 then we can define x0
t 1 such that xt 1 = {x0t 1, xi,j,t 1}
and a partition P =
 
{x0
t 1, yt}, {xi,j,t 1,?}
 
such that
?(y
t
| x
t 1)P = ?(yt | x0
t 1)? ?(? | xi, j, t  1) = ?(yt | x0t 1) = 1  q(s0).
By Lemma 2.1, we have 1  q(s) ? 1  q(s0), and thus
?
e
(x
t 1, yt) ? log
2
?
?(y
t
| x
t 1)
?(y
t
| x
t 1)P
?
= log
2
?
1  q(s)
1  q(s0)
?
? 0,
so x
t 1 is reducible. Next we consider the case where max(xt 1) = 0, but there exists j such that cj > 1. We define
x
0
t 1 such that xt 1 = {x0 1, xi,j,t 1}, and a partition P =
 
{x0
t 1, yt}, {xi,j,t 1,?}
 
, such that
?(y
t
| x
t 1)P = ?(yt | x0
t 1)? ?(? | xi,j,t 1) = ?(yt | x0t 1) = q(s0) = 1
and
?
e
(x
t 1, yt) ? log
2
?
?(y
t
| x
t 1)
?(y
t
| x
t 1)P
?
= log
2
?
1  q(s)
1  q(s0)
?
= log
2
?
1
1
?
= 0,
so x
t 1 is again reducible. Finally, we show that occurrences xt 1 are irreducible if max(xt 1) = 0 and all cj ? 1. All
possible partitions of the pair of occurrences can be formulated as P =
 
{x0
t 1, yt}, {(xt 1 \ x0t 1),?}
 
(where
x
0
t 1 ? xt 1), such that c0
j
? c
j
8 j, and c0 < c. Then
?(y
t
| x
t 1)P = ?(yt | x0
t 1)? ?(? | (xt 1   x0t 1)) = ?(yt | x0t 1) = 1  q(s0),
and by Lemma 2.5,
?
e
(x
t 1, yt) = min
P
?
log
2
?
?(y
t
| x
t 1)
?(y
t
| x
t 1)P
??
= min
P
?
log
2
?
1  q(s)
1  q(s0)
??
> 0.
So {X
t 1 = xt 1} is irreducible, and its actual effect is {Y1 = 1}.
11
